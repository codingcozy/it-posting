---
title: "순차 데이터에 대한 딥러닝 입문서"
description: ""
coverImage: "/assets/img/2024-07-14-AnIntroductiontoDeepLearningforSequentialData_0.png"
date: 2024-07-14 01:57
ogImage:
  url: /assets/img/2024-07-14-AnIntroductiontoDeepLearningforSequentialData_0.png
tag: Tech
originalTitle: "An Introduction to Deep Learning for Sequential Data"
link: "https://medium.com/towards-data-science/an-introduction-to-deep-learning-for-sequential-data-ac966b9b9b67"
isUpdated: true
---

![Sequential data](/assets/img/2024-07-14-AnIntroductiontoDeepLearningforSequentialData_0.png)

시계열 및 자연 언어와 같은 순차 데이터는 순서 및 맥락을 포착할 수 있는 모델이 필요합니다. 시계열 분석은 시간 패턴을 기반으로 예측에 중점을 두는 반면, 자연 언어 처리는 단어 시퀀스에서 의미론적 의미를 추출하려고 합니다.

두 데이터 유형은 서로 다른 작업이지만, 먼 요소가 예측에 영향을 미치는 장거리 종속성이 있습니다. 딥 러닝이 발전함에 따라 하나의 도메인을 위해 초발되었던 모델 아키텍처가 다른 도메인으로 적응되었습니다.

<div class="content-ad"></div>

시계열 데이터와 자연어 모두 관측치의 위치가 매우 중요한 연속 구조를 가지고 있어요.

시계열은 시간에 따른 순서가 지정된 관측치의 집합으로, 일정한 시간 간격으로 샘플링된 순서대로 정렬돼 있어요. 예시로는 다음과 같은 것들이 있죠:

- 매일의 주식 가격
- 매 시간의 서버 메트릭
- 매 초의 온도 측정

시계열 데이터의 주요 속성은 관측치의 순서가 의미가 있다는 것이에요. 시간에 가까운 값들은 일반적으로 상호 의존성이 높습니다 — 최근 값들을 알면 다음 값을 예측하는 데 도움이 돼요. 시계열 분석은 이러한 시간적 의존성을 모델링하여 패턴을 이해하고 예측을 하려는 것이 목적입니다.

<div class="content-ad"></div>

텍스트 데이터도 순차적입니다. 단어의 순서가 의미와 문맥을 전달합니다. 예를 들어:

- 존이 공을 던졌다.
- 공이 존을 던졌다.

이 두 문장은 같은 단어를 포함하지만, 단어 순서에 따라 의미가 전혀 달라집니다. 이러한 시간적 관계는 언어 모델에 나타나며 번역 및 요약과 같은 자연어 작업의 핵심입니다.

시계열 데이터와 텍스트는 먼 거리의 의존성을 가지고 있습니다. 시퀀스에서 떨어져 있는 값들도 서로 영향을 줍니다. 또한, 지역적인 패턴들이 서로 다른 위치에서 반복됩니다.

<div class="content-ad"></div>

## Time series 및 신경망에서의 텍스트 표현

텍스트 데이터는 기계가 읽을 수 있도록 임베딩으로 변환되어야 합니다.

큰 데이터셋에서 학습된 임베딩이라고 불리는 벡터 표현은 단어 또는 데이터 포인트 간의 의미론적 의미와 관계를 포착하기 위해 사용됩니다. 임베딩 벡터는 각 요소에서 서로 다른 의미 속성을 인코딩하여 기계 학습 모델을 위해 단어/데이터를 밀집하고 저차원 방식으로 표현합니다. 임베딩은 큰 말뭉치에서 사전 학습될 수 있으며, 이후 특정 작업에 대해 세밀하게 조정할 수 있습니다.

![Image](/assets/img/2024-07-14-AnIntroductiontoDeepLearningforSequentialData_1.png)

<div class="content-ad"></div>

시계열을 분석할 때 고려해야 할 사항은 추세와 계절성과 같은 것이 더 있습니다. 그러나 신경망에서 이러한 데이터가 어떻게 표현되는지에 대한 차이는 결국 시계열이 값들의 순서이고 텍스트는 벡터의 순서인 사실에서 비롯됩니다.

## 순차 데이터 작업

순차 데이터를 검토할 때 가장 직관적인 다음 단계는 순서에서 그 다음에 올 값 예측하는 것이 될 겁니다.

시계열 예측에서는 과거 데이터를 기반으로 내일의 주가나 다음 주의 온도와 같은 연속적인 값을 예측하려고 합니다. 모델은 예측값과 실제 값 사이의 차이를 최소화하도록 학습되는데, 이는 회귀 작업의 일반적인 특성입니다.

<div class="content-ad"></div>

텍스트 생성 또는 보다 적절히 말하자면, 다음 토큰 예측은 이전 토큰이 주어졌을 때 다음 토큰을 예측하도록 모델을 훈련시키는 것을 의미합니다. 자기 회귀 언어 모델링은 다중 클래스 분류 문제로 볼 수 있으며, 각 가능한 토큰을 별도의 클래스로 생각할 수 있습니다. 결과는 어휘 내 모든 가능한 토큰에 대한 확률 분포입니다.

다른 작업에는 문장 분류 - 문장이나 문서를 미리 정의된 클래스로 분류하는 것과 시계열 분류가 포함됩니다.
감성 분석이라는 예가 있으며, 이것은 각 텍스트가 긍정적 및 부정적 클래스로 분류되는 작업입니다. 시계열도 분류될 수 있으며, 예를 들어 심박수가 건강하거나 질병이 있다고 분류할 수 있어 비정상을 감지할 수 있습니다.

![Deep Learning for Sequential Data](/assets/img/2024-07-14-AnIntroductiontoDeepLearningforSequentialData_2.png)

여기서 모델은 텍스트 또는 시계열 특성을 범주형 레이블로 매핑하는 방법을 배우기 위해 수동으로 주석이 달린 예제 데이터 세트로 훈련이 필요합니다.

<div class="content-ad"></div>

## 연속 데이터 모델링

오늘날의 강력한 신경망이 시계열 예측 및 자연어 처리를 위해 만들어지기 전에는 일반적으로 이러한 작업에 다른 모델이 사용되었습니다.

2010년대 이전에는 ARIMA (자기회귀누적이동평균)와 지수 평활 모델과 같은 통계적 방법이 시계열 예측에 인기를 끌었습니다. 이러한 방법들은 시계열의 과거 값 사이의 수학적 관계에 의존하여 미래 값을 예측합니다. 특정 데이터에 효과적이지만, 복잡한 실제 시계열 데이터에서 성능을 제한하는 엄격한 가정을 가지고 있습니다.

자연어 처리에서는 언어 번역과 음성 인식과 같은 작업이 전통적으로 규칙 기반 시스템을 사용하여 처리되었습니다. 이들은 인간이 만든 규칙과 문법을 인코딩하며 광범위한 수작업 노력이 필요하며 실제 인간 언어의 미묘함과 변화에 어려움을 겪었습니다. 반면, 나이브 베이즈, 로지스틱 회귀 및 기타 고전적인 기계 학습 모델이 가끔 사용되기도 하지만 텍스트 데이터의 장기적 맥락과 의존성을 효과적으로 포착할 수 없었습니다.

<div class="content-ad"></div>

RNN 및 LSTM 네트워크의 소개로 인해 시계열 예측 및 NLP에 대한 문맥적 학습이 가능해졌습니다. 강철 같은 통계적 가정이나 단순한 입력-출력 매핑에 의존하는 대신, RNN은 순차적 데이터로부터 장거리 종속성을 학습할 수 있습니다. 이 혁신은 RNN이 언어 모델링, 감성 분석 및 비선형 예측과 같은 문제에서 뛰어난 성과를 낼 수 있도록 했으며, 고전적인 방법론이 작동하지 않는 문제에 대해 실용적이게 됐습니다. 1980년대에 소개된 이 모델은 계산 능력이 크게 향상된 마지막 10년 사이에 비로소 실용적으로 사용되기 시작했습니다. Google은 2015년 Google Voice에서 LSTM을 사용하기 시작했습니다.

## 순환 신경망

RNN에는 정보가 타임스텝을 걸쳐 지속되도록 하는 재귀적 연결이 포함되어 있습니다.

예측 작업을 수행할 때 RNN은 시계열에서 지난 관측치로 훈련될 수 있어 시간적 패턴을 학습할 수 있습니다. RNN은 각 타임 스텝에서 현재 입력과 이전 숨겨진 상태에 기반하여 그 순서를 처리하고 갱신합니다.

<div class="content-ad"></div>

다음 토큰 예측을 위해 RNN은 단어로 구성된 문장과 같은 텍스트 시퀀스로 훈련됩니다. RNN은 이전 단어를 기반으로 다음 단어를 예측하는 방법을 학습합니다. 은닉 상태는 이전 단어의 맥락을 유지하여 다음 예측을 정하는 데 도움을 줍니다. 각 단계에서 RNN은 다음 토큰에 대한 확률 분포를 출력합니다.

RNN의 과거 맥락을 기억하는 능력은 NLP 및 시계열 분석의 순차 작업에 혁명을 가져왔습니다. 그러나 소멸 및 폭발 그래디언트와 같은 문제로 인해 장기 의존성에 어려움을 겪을 수 있습니다. 이 문제는 LSTMs와 같은 구조적 개선에 영감을 주어 많은 타임 스텝을 횡단하는 기울기 흐름을 개선하고 어텐션 기반 모델을 통해 더욱 향상되었습니다.

## 트랜스포머

어텐션 메커니즘은 우리가 오늘 알고있는 모든 멋진 LLMs를 가능하게 만들었습니다. 이들은 초기에 RNN을 보완하기 위해 도입되었으며, 예측을 할 때 모델이 입력 시퀀스의 관련 부분에 집중할 수 있게 했습니다. 어텐션 함수는 각 타임 스텝의 중요성을 점수화하고 이러한 가중치를 사용하여 관련 컨텍스트를 추출합니다.

<div class="content-ad"></div>

주의는 NLP 및 시계열 모델링에서 연속 작업에 있어서 중요한 구성 요소가 되었습니다. 관련된 입력에 집중함으로써 모델의 정확성과 해석 가능성을 향상시킵니다.

![이미지](/assets/img/2024-07-14-AnIntroductiontoDeepLearningforSequentialData_3.png)

완전히 자기 주의에 의존하는 Transformer 구조는 NLP 및 시계열 모델링에서 Durchbruch 결과를 이끌어냈습니다. 자기 주의 레이어는 시퀀스 요소 간의 거리와 관계없이 의존성을 모델링할 수 있습니다. 시퀀스가 문맥의 길이에 맞다면 가능합니다.

Transformer는 순차 데이터에 대해 최첨단 기술이 되었으며, 해당 구조는 NLP로 BERT 및 시계열로 Temporal Fusion Transformer로 적응되었습니다.

<div class="content-ad"></div>

# 시계열 데이터에 대한 기초 모델로

기초 모델은 방대한 양의 데이터에서 훈련될 수 있는 대형 기계 학습 모델로, 다양한 작업에 적응시킬 수 있습니다. 기초 모델은 일반적인 기계 학습 모델과 다릅니다. 일반적으로 특정 작업을 수행하는 대신 더 일반적이고 유연하며 더 특화된 애플리케이션을 개발하기 위한 출발점으로 사용될 수 있습니다. 처음부터 비싼 훈련을 피함으로써 새로운 응용 프로그램을 구축하는 데 드는 시간과 비용을 크게 줄일 수 있습니다.

자연어 처리(NLP)에서 대형 언어 모델은 문맥에서 학습하는 것을 가능하게 합니다. 이러한 혁신적인 능력 덕분에 ChatGPT 등의 대형 언어 모델은 다양한 작업에 일반화될 수 있어 힘을 발휘합니다.

대부분의 현재의 예측 접근 방식은 각 새 데이터셋에 개별적으로 맞춰져야 합니다. 이 과정은 시간이 많이 소요되며 도메인 전문 지식이 필요합니다. 이 문제를 해결하기 위해 최근에는 기초 모델의 개념이 시계열 데이터에 적용되었습니다.

<div class="content-ad"></div>

TimeGPT은 다양한 경제, 기상, 교통 및 소매 판매 등 다양한 분야를 아우르는 1000억 개 이상의 시계열 데이터 포인트로 사전 훈련된 Transformer 기반 신경망입니다. 주요 혁신은 GPT-3와 마찬가지로 TimeGPT가 새로운 시계열 데이터에 대한 정확한 예측을 일반화할 수 있으며 각 새 데이터셋에 대해 재훈련할 필요가 없습니다. 이 zero-shot 기능은 기존의 예측 파이프라인과 비교하여 엄청난 시간과 자원을 절약해줍니다. 기초 모델은 예측을 단일 모델로 단순화하여 몇 줄의 코드로 모든 시계열에 적용할 수 있습니다.

## 핵심 내용

딥 러닝을 할 때, 상상력을 발휘해보세요. 데이터와 모델은 보다 많은 공통점을 가지고 있습니다 - 모든 것이 서로 연결되어 있습니다. 시계열 분석과 NLP는 빠르게 혁신하며 아이디어를 공유하고 있습니다.

시계열과 NLP는 순차적 데이터 유형으로 많은 유사성을 공유합니다. 우리는 RNN, LSTM 및 Transformers와 같은 구조를 사용하여 두 가지를 모두 모델링합니다. 딥 러닝이 발전함에 따라 이러한 도메인 사이를 계속 넘나들 것으로 기대됩니다.

<div class="content-ad"></div>

2010년대는 통계 모델에 의해 지배되던 영역을 신경망이 정복하는 것이라고 할 수 있는 논다. 2020년대는 트랜스포머가 자신의 지배력을 공고히 하는 것이라고 예상되며, 연구자들은 이 강력한 모델의 한계를 계속해서 허무는 중입니다.

이 기사를 즐겼다면? 매주 데이터 과학 인터뷰 질문을 받아보세요. [The Data Interview](데이터 인터뷰) 뉴스레터를 구독하여 이메일로 받아보세요.

제 LinkedIn에서 저를 찾을 수도 있습니다.

## 참고 자료

<div class="content-ad"></div>

[1] 오래된 단기 기억 - 위키백과

[2][1706.03762] 주의는 필요한 모든 것이다 (arxiv.org)

[3] [2310.03589] 시간GPT-1 (arxiv.org)
