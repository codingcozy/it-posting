---
title: "Word2Vec, GloVe, FastText 단어 임베딩 기법 완벽 설명"
description: ""
coverImage: "/assets/img/2024-07-14-Word2VecGloVeandFastTextExplained_0.png"
date: 2024-07-14 02:20
ogImage:
  url: /assets/img/2024-07-14-Word2VecGloVeandFastTextExplained_0.png
tag: Tech
originalTitle: "Word2Vec, GloVe, and FastText, Explained"
link: "https://medium.com/towards-data-science/word2vec-glove-and-fasttext-explained-215a5cd4c06f"
isUpdated: true
---

![이미지](/assets/img/2024-07-14-Word2VecGloVeandFastTextExplained_0.png)

컴퓨터는 우리처럼 단어를 이해하지 못합니다. 그들은 숫자로 작업하는 것을 선호합니다. 그러므로, 컴퓨터가 단어와 그 의미를 이해하는 데 도움이 되기 위해, 우리는 임베딩이라는 것을 사용합니다. 이러한 임베딩은 단어들을 수학적 벡터로 숫자로 표현합니다.

이 임베딩의 멋진 점은 제대로 학습하면, 비슷한 의미를 가진 단어는 유사한 숫자 값을 가지게 된다는 것입니다. 다시 말해, 그들의 숫자는 서로에 가까울 것입니다. 이는 컴퓨터가 숫자적으로 표현된 것에 기반하여 서로 다른 단어 사이의 연결과 유사성을 파악할 수 있도록 합니다.

단어 임베딩을 학습하는 한 가지 탁월한 방법은 Word2Vec입니다. 이 글에서는 Word2Vec의 복잡성을 탐구하고 그 다양한 구조와 변형을 살펴볼 것입니다.

<div class="content-ad"></div>

# 워드투벡

![Link](/assets/img/2024-07-14-Word2VecGloVeandFastTextExplained_1.png)

예전에는 문장을 n-gram 벡터로 표현했습니다. 이러한 벡터는 단어 시퀀스를 고려하여 문장의 본질을 잡으려고 했습니다. 그러나 이러한 방식에는 제한 사항이 있었습니다. n-gram 벡터는 종종 크고 희소하여 계산적으로 어렵게 만들었습니다. 이것은 차원의 저주로 알려진 문제를 발생시켰습니다. 본질적으로 이는 고차원 공간에서 단어를 표현하는 벡터가 너무 멀리 떨어져 있어 실제로 유사한 단어들을 결정하기가 어렵다는 것을 의미했습니다.

그 후 2003년에는 감탄할 만한 발전이 있었습니다. 신경 확률 언어 모델이 등장했고 이 모델은 어떻게 단어를 나타내는지를 완전히 바꿔 놓았습니다. 이 모델은 연속적이고 밀집된 벡터를 사용하여 단어를 표현했습니다. n-gram 벡터와는 달리, 이 밀집된 벡터는 연속적인 표현을 제공했습니다. 이러한 벡터에 작은 변화가 가해져도 의미 있는 표현이 생성되었으며 이들은 특정한 영어 단어와 직접적으로 대응되지 않을 수도 있습니다.

<div class="content-ad"></div>

그 놀라운 진전을 토대로 2013년에 Word2Vec 프레임워크가 등장했습니다. 이 프레임워크는 단어 의미를 연속적이고 밀집된 벡터로 인코딩하는 강력한 방법을 제시했습니다. Word2Vec 내에서는 Continuous Bag of Words (CBoW)와 Skip-gram이라는 두 가지 주요 아키텍처가 소개되었습니다.

이러한 아키텍처들은 고품질의 단어 임베딩을 생성할 수 있는 효과적인 모델 학습을 가능하게 했습니다. 방대한 양의 텍스트 데이터를 활용함으로써 Word2Vec은 숫자 세상에서 단어들을 살아있게 만들었습니다. 이를 통해 컴퓨터가 단어 간의 문맥적 의미와 관계를 이해하고 자연어 처리에 혁신적인 접근 방식을 제공했습니다.

# Continuous Bag-of-Words (CBoW)

![CBoW](/assets/img/2024-07-14-Word2VecGloVeandFastTextExplained_2.png)

<div class="content-ad"></div>

이번 섹션과 다음 섹션에서는 다섯 단어의 작은 어휘를 사용하여 CBoW 및 스킵-그램 모델이 어떻게 훈련되는지 이해해봅시다: biggest, ever, lie, told 그리고 the. 그리고 우리가 예시 문장 "The biggest lie ever told"를 가지고 있습니다. 이것을 CBoW 아키텍처로 어떻게 전달할까요? 이것은 위의 그림 2에서 보여졌지만, 우리는 이 과정을 묘사할 것입니다.

우리는 컨텍스트 윈도우 크기를 2로 설정했다고 가정해봅시다. "The," "biggest," "ever," 그리고 "told" 단어를 가져와서 5x1 원핫 벡터로 변환합니다.

이러한 벡터들은 그런 다음 모델의 입력으로 전달되고 프로젝션 레이어에 매핑됩니다. 이 프로젝션 레이어의 크기를 3으로 가정해봅시다. 각 단어의 벡터는 5x3 가중 행렬과 곱해지며(입력 간에 공유됨), 결과적으로 네 개의 3x1 벡터가 생성됩니다. 이러한 벡터들의 평균을 내면 하나의 3x1 벡터가 됩니다. 이 벡터는 다시 다른 3x5 가중 행렬을 사용하여 5x1 벡터로 투영됩니다.

이 최종 벡터는 중간 단어 "lie"를 대표합니다. 진정한 원핫 벡터와 실제 출력 벡터를 계산함으로써, 우리는 네트워크의 가중치를 업데이트하기 위해 역전파를 통해 사용되는 손실을 얻을 수 있습니다.

<div class="content-ad"></div>

이 프로세스를 반복하여 문맥 창을 슬라이드하고 수천 개의 문장에 적용합니다. 훈련이 완료되면, 모델의 첫 번째 레이어는 차원이 5x3 (어휘 크기 x 투영 크기)인 학습된 매개변수를 포함합니다. 이러한 매개변수는 각 단어를 해당하는 벡터 표현으로 매핑하는 룩업 테이블로 사용됩니다.

# Skip-gram

![Skip-gram](/assets/img/2024-07-14-Word2VecGloVeandFastTextExplained_3.png)

스킵-그램 모델에서는 연속 가방 단어 (CBOW) 케이스와 유사한 아키텍처를 사용합니다. 그러나, 주변 단어를 기반으로 대상 단어를 예측하는 대신, 그림 3에 나와 있는 것처럼 시나리오를 뒤집습니다. 이제 단어 "lie"가 입력이 되고, 우리는 해당 문맥 단어를 예측하는 것을 목표로 합니다. "스킵-그램"이라는 이름은 이 접근을 반영하는데, 몇 개의 단어를 "건너띌" 수있는 문맥 단어를 예측합니다.

<div class="content-ad"></div>

이를 설명하기 위해 몇 가지 예를 살펴보겠습니다:

- 입력 단어 "거짓말"은 출력 단어 "그"와 짝지어집니다.
- 입력 단어 "거짓말"은 출력 단어 "가장 큰"과 짝지어집니다.
- 입력 단어 "거짓말"은 출력 단어 "지금까지"와 짝지어집니다.
- 입력 단어 "거짓말"은 출력 단어 "하였습니다."와 짝지어집니다.

우리는 모든 단어에 대해 이 과정을 반복합니다. 훈련이 완료되면, 첫 번째 층의 매개변수는 어휘 크기 x 프로젝션 크기의 차원을 가지며, 입력 단어와 해당 벡터 표현 사이의 관계를 포착합니다. 이러한 학습된 매개변수를 사용하면 skip-gram 모델에서 입력 단어를 해당 벡터 표현으로 매핑할 수 있습니다.

# 장점

<div class="content-ad"></div>

- 단순함으로 차원의 저주를 극복: Word2Vec은 단어를 밀집 벡터로 표현함으로써 차원의 저주와 관련된 희소성과 계산 복잡성을 감소시켜줍니다.
- 의미가 유사한 단어가 가까운 벡터 값을 갖도록 벡터를 생성: Word2Vec의 임베딩은 유사한 의미를 가진 단어가 숫자 값이 가까운 벡터로 표현되는 가치 있는 특성을 나타냅니다. 이는 의미론적 관계를 포착하고 단어 유사성 및 유추 감지와 같은 작업을 수행하는 데 도움이 됩니다.
- 다양한 NLP 응용 프로그램을 위한 사전 훈련된 임베딩: Word2Vec의 사전 훈련된 임베딩은 널리 사용 가능하며 감정 분석, 개체명 인식, 기계 번역 등과 같은 자연어 처리 (NLP) 응용 프로그램에 활용될 수 있습니다. 이러한 임베딩은 대규모 말뭉치에서 훈련되어 감정 분석, 기계 번역 등과 같은 작업에 유용한 자원을 제공합니다.
- 데이터 증강 및 훈련을 위한 자기 감독 프레임워크: Word2Vec은 기존 데이터를 활용하여 단어 표현을 학습하는 자기 감독 방식으로 작동합니다. 이는 더 많은 데이터를 수집하고 모델을 훈련하기 쉽게 만들어 줍니다. 이 프레임워크는 레이블이 지정된 데이터 집합을 필요로 하지 않으므로 대규모의 레이블이 지정되지 않은 텍스트에 적용될 수 있어 훈련 과정을 향상시킵니다.

# 단점

- 전역 정보의 제한적 보존: Word2Vec의 임베딩은 주로 지역 문맥 정보를 포착하는 데 중점을 두므로 단어 간의 전역적인 관계를 유지하지 못할 수 있습니다. 이 제한은 문헌 분류나 문헌 수준에서의 감정 분석과 같이 텍스트에 대한 더 넓은 이해력이 필요한 작업에 영향을 줄 수 있습니다.
- 형태론적으로 풍부한 언어에 덜 적합: 형태론적으로 풍부한 언어는 복잡한 단어 형태와 변형을 가지며, 이는 Word2Vec에 어려움을 야기할 수 있습니다. Word2Vec은 각 단어를 원자 단위로 취급하므로 이러한 언어에서 나타나는 풍부한 형태론과 의미론적 세세 사이를 포착하는 데 어려움을 겪을 수 있습니다.
- 넓은 문맥 인식의 부족: Word2Vec 모델은 훈련 중 대상 단어 주변의 한정된 단어 창문만을 고려합니다. 이 한정된 문맥 인식은 특정 맥락에서의 단어 의미를 부분적으로 이해할 수 있게 하는데, 이는 특정 언어 현상에서 나타나는 장거리 종속성과 복잡한 의미론적 관계를 포착하는 데 어려움을 겪을 수 있습니다.

다음 섹션에서는 이러한 단점을 해결하는 몇 가지 단어 임베딩 아키텍처를 살펴볼 것입니다.

<div class="content-ad"></div>

# GloVe: Global Vectors

워드투벡 메소드는 어느 정도 지역적 맥락을 포착하는 데 성공했지만, 말뭉치에서 사용 가능한 전반적 맥락을 충분히 활용하지 못합니다. 전역적 맥락이란 말뭉치 전체의 여러 문장을 사용하여 정보를 수집하는 것을 의미합니다. 이것이 GloVe가 등장하는 곳이며, 이는 단어 임베딩을 학습하는 데 단어-단어 동시발생을 활용합니다.

단어-단어 동시발생 행렬의 개념이 GloVe에 중요합니다. 이는 말뭉치의 모든 다른 단어들과의 맥락에서 각 단어의 발생을 포착한 행렬입니다. 행렬의 각 셀은 한 단어가 다른 단어의 맥락에서 발생하는 횟수를 나타냅니다.

![이미지](/assets/img/2024-07-14-Word2VecGloVeandFastTextExplained_4.png)

<div class="content-ad"></div>

Word2Vec이 확률과 함께 작동하는 반면, Glove는 공동발생 확률의 비율부터 시작합니다. Figure 4의 맥락에서 P(k | ice)는 단어 "ice"의 맥락에서 단어 k가 발생할 확률을 나타내며, P(k | steam)은 단어 "steam"의 맥락에서 단어 k가 발생할 확률을 나타냅니다. P(k | ice) / P(k | steam) 비율을 비교함으로써 단어 k와 얼음 또는 증기의 관련성을 결정할 수 있습니다. 비율이 1보다 훨씬 크면, 얼음과 더 강한 관련성이 있음을 나타냅니다. 반면, 0에 가까울수록 증기와 더 강한 관련성을 시사합니다. 1에 가까운 비율은 어느 쪽과도 명확한 관련이 없음을 시사합니다.

예를 들어, k = "solid"일 때, 확률 비율이 1보다 훨씬 크므로, 얼음과 강한 연관성을 나타냅니다. 반면 k = "gas"일 때, 확률 비율은 0에 더 가까워져서 증기와 더 강한 관련성을 시사합니다. "water"와 "fashion" 단어는 얼음이나 증기와 명확한 관련이 나타나지 않습니다.

확률 비율에 기반한 이 단어들의 관련성은 우리가 달성하고자 하는 목표입니다. 이는 GloVe로 임베딩을 학습할 때 최적화됩니다.

# FastText

<div class="content-ad"></div>

전통적인 word2vec 아키텍처는 전역 정보 활용 부족 외에 형태론적으로 풍부한 언어를 효과적으로 다루지 못합니다.

그래서 언어가 형태론적으로 풍부하다는 것이 무엇을 의미할까요? 이러한 언어에서는 단어가 사용된 맥락에 따라 형태가 변할 수 있습니다. 남인도어로 불리는 칸나다어를 예로 들어보겠습니다.

칸나다어에서 "집"이라는 단어는 ಮನೆ (mane)로 쓰입니다. 그러나 "집 안에서"라고 할 때는 ಮನೆಯಲ್ಲಿ (maneyalli)가 되며, "집으로부터"라고 할 때는 ಮನೆಯಿಂದ (maneyinda)로 바뀝니다. 보시다시피 전치사만 변경되지만 번역된 단어는 다른 형태를 가집니다. 영어로는 그냥 "house"일 뿐입니다. 그래서 전통적인 word2vec 아키텍처는 이러한 변형을 전부 동일한 벡터로 매핑합니다. 그러나 형태론적으로 풍부한 칸나다어에 대한 word2vec 모델을 만든다면, 이 세 가지 경우마다 다른 벡터가 할당될 것입니다. 게다가, 칸나다어에서 "집"은 이 세 예만이 아니라 더 많은 형태를 취할 수 있습니다. 우리의 말뭉치에 이러한 변형이 모두 포함되지 않을 수 있기 때문에 전통적인 word2vec 학습은 다양한 단어 표현을 모두 잡아내지 못할 수도 있습니다.

이 문제를 해결하기 위해 FastText는 단어 벡터 생성 시 하위 단어 정보를 고려하는 솔루션을 제공합니다. 단어를 전체로 처리하는 대신 FastText는 단어를 문자 n-그램으로 분해합니다. 이러한 n-그램은 삼음음절부터 육음음절까지 변화하며, 이들은 벡터로 매핑되어 이후 전체 단어를 나타내는 데 집계됩니다. 그러한 집계된 벡터는 skip-gram 아키텍처로 전달됩니다.

<div class="content-ad"></div>

이 방식은 언어 내 다양한 단어 형태 사이의 공통 특성을 인식할 수 있게 해줍니다. 말뭉치(corpus)에서 단어의 모든 형태를 보지 않았더라도 학습된 벡터는 이러한 형태들 사이의 공통점과 유사성을 포착합니다. 아랍어, 터키어, 핀란드어, 그리고 다양한 인도어들과 같이 형태론적으로 풍부한 언어들은 FastText의 다양한 형태와 변형을 고려한 단어 벡터를 생성할 수 있는 능력에서 혜택을 받을 수 있습니다.

# 문맥 인식

앞서 말한 word2vec 아키텍처들의 장점에도 불구하고 한 가지 제한이 있습니다: 주어진 단어에 대해 동일한 벡터 표현을 생성하며, 문맥에 관계없이 동일한 벡터를 제공합니다.

이 점을 설명하기 위해 다음 두 문장을 살펴보겠습니다:

<div class="content-ad"></div>

"그 드래그 퀸은 멋있어요."
"그녀는 완벽한 패에 ace와 퀸을 가지고 있어요."

이 문장들에서, '퀸'이라는 단어는 다른 의미를 갖고 있어요. 하지만 word2vec 아키텍처에서는 '퀸'의 벡터가 두 경우 모두 같을 겁니다. 이는 우리가 단어 벡터가 그들의 맥락에 따라 다른 의미를 포착하고 나타내기를 원하는데 이상적이지 않아요.

이 문제를 해결하기 위해, LSTM 셀과 같은 보다 고급 아키텍처가 도입되었어요. 이러한 아키텍처는 단어 표현에 맥락 정보를 통합하도록 설계되었어요. 시간이 지나 BERT와 GPT와 같은 트랜스포머 기반 모델이 등장함으로써, 우리가 오늘날 보게 된 대규모 언어 모델의 개발로 이어졌어요. 이러한 모델은 맥락을 고려하며 주변 단어와 문장들에 민감한 단어 표현을 생성하는 데 뛰어나다고 해요.

이러한 고급 아키텍처들은 맥락을 고려함으로써 더 섬세하고 의미 있는 단어 벡터의 생성을 가능케 하며, 동일한 단어가 특정 맥락에 따라 다른 벡터 표현을 갖도록 보장해요.

<div class="content-ad"></div>

# 결론

이 블로그 포스트를 통해 word2vec 아키텍쳐에 대한 통찰력을 제공했습니다. 이 아키텍쳐는 연속적이고 밀집된 벡터를 사용하여 단어를 표현하는 능력을 가지고 있습니다. 이후에 나온 GloVe와 같은 구현체들은 전역적 맥락을 활용했고, FastText는 아랍어, 핀란드어 및 다양한 인도어 언어와 같이 형태론적으로 풍부한 언어들에 대한 벡터 학습을 효율적으로 가능케 했습니다. 그러나 이러한 방법들 사이의 공통된 단점은 추론 중에 단어에 대해 맥락에 관계없이 동일한 벡터를 할당한다는 것이며, 이는 다의성을 가진 단어들을 정확하게 나타내는 데 제약을 줄 수 있습니다.

이 한계를 해결하기 위해 NLP 분야의 후속 발전은 LSTM 셀과 트랜스포머 아키텍처를 소개했는데, 이들은 특정 맥락을 포착하는 데 뛰어나며 현재의 대규모 언어 모델의 기초가 되었습니다. 이러한 모델들은 주변 맥락에 따라 다양한 단어 표현을 이해하고 생성하는 능력을 가지고 있어, 서로 다른 시나리오에서 단어의 뉘앙스를 수용할 수 있습니다.

그럼에도 불구하고, word2vec 프레임워크는 여전히 중요하다는 점을 인정해야 합니다. 이 프레임워크는 자연어 처리 분야에서 다양한 응용 프로그램을 지속적으로 지원하고 있습니다. 그 간단함과 유의미한 단어 임베딩을 생성하는 능력은 맥락 변화에 따른 단어 의미의 변형이라는 어려움에도 불구하고 가치가 증명되었습니다.

<div class="content-ad"></div>

언어 모델에 대한 더 많은 정보는 이 YouTube 재생 목록을 확인해보세요.

즐거운 학습 되세요!
