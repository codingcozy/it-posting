---
title: "최신 언어 모델들이 Mixture of Experts MoE 아키텍처를 사용하는 이유"
description: ""
coverImage: "/assets/img/2024-07-09-WhytheNewestLLMsuseaMoEMixtureofExpertsArchitecture_0.png"
date: 2024-07-09 10:23
ogImage:
  url: /assets/img/2024-07-09-WhytheNewestLLMsuseaMoEMixtureofExpertsArchitecture_0.png
tag: Tech
originalTitle: "Why the Newest LLMs use a MoE (Mixture of Experts) Architecture"
link: "https://medium.com/datadriveninvestor/why-the-newest-llms-use-a-moe-mixture-of-experts-architecture-1023238ea145"
isUpdated: true
---

![Specialization Made Necessary](/assets/img/2024-07-09-WhytheNewestLLMsuseaMoEMixtureofExpertsArchitecture_0.png)

# Specialization Made Necessary

한 병원은 전문가와 의사들로 넘친다. 각자가 고유한 전문 분야를 가지고 독특한 문제들을 해결하며 환자들에게 필요한 치료를 제공하기 위해 손을 맞잡는 것을 생각해보세요. 외과의, 심장 전문의, 소아과 의사들 — 모든 종류의 전문가들이 환자들에게 필요한 치료를 제공하기 위해 종종 협력합니다. 우리는 인공지능에서도 같은 것을 할 수 있습니다.

인공지능의 MoE(전문가의 혼합) 구조는 서로 협력하여 복잡한 데이터 입력에 대응하기 위해 다양한 "전문가" 모델을 혼합하거나 섞은 것으로 정의됩니다. 인공지능에서 MoE 모델의 각 전문가는 그들의 의료 분야별 전문 지식처럼 훨씬 더 큰 문제에 전문화되어 있습니다. 이는 효율성을 향상시키고 시스템 효과성과 정확성을 높입니다.

<div class="content-ad"></div>

Mistral AI는 오픈 소스의 기반이 되는 LLM을 제공하는데, 이는 OpenAI의 것과 견줄 만합니다. 그들은 Mixtral 8x7B 모델에서 MoE 아키텍처의 사용에 대해 형식적으로 논의했습니다. 이 혁신적인 대형 언어 모델 (LLM) 형태의 획기적인 발전에 대해 알아보겠습니다. 또한 Mistral AI의 Mixtral이 다른 기본 LLM들 중에서 빛을 발하며, 현재 LLM들이 속도, 크기 및 정확도를 강조하기 위해 MoE 아키텍처를 채택하는 이유에 대해 깊이 파고들 것입니다.

# 대형 언어 모델 (LLM)을 업그레이드하는 일반적인 방법들

MoE 아키텍처가 어떻게 LLM을 개선하는지 더 잘 이해하기 위해, LLM 효율성을 향상하는 일반적인 방법들에 대해 알아보겠습니다. AI 전문가와 개발자들은 모델을 향상시키기 위해 매개 변수를 늘리거나 아키텍처를 조정하거나 세밀 조정하는 방법으로 모델을 향상시킵니다.

- 매개 변수 증가: 더 많은 정보를 공급하고 해석함으로써, 모델은 복잡한 패턴을 학습하고 표현하는 능력이 증가합니다. 그러나 이로 인해 오버피팅과 환각이 발생할 수 있으며, 이는 인간의 피드백을 통한 철저한 강화 학습이 필요할 수 있습니다.
- 아키텍처 조정: 새로운 레이어나 모듈을 도입함으로써 매개 변수 수를 늘릴 수 있으며, 특정 작업에 대한 성능을 향상시킬 수 있습니다. 그러나 기본 아키텍처를 변경하는 것은 구현하기 어려울 수 있습니다.
- 세밀 조정: 사전 훈련된 모델을 특정 데이터나 전이 학습을 통해 세밀하게 조정하면, 기존 LLM이 새로운 작업이나 도메인을 처리할 수 있게 되며, 처음부터 다시 시작할 필요가 없습니다. 이것은 가장 쉬운 방법이며, 모델에 상당한 변경이 필요하지 않습니다.

<div class="content-ad"></div>

# MoE 아키텍처란 무엇인가요?

Mixture of Experts (MoE) 아키텍처는 각 입력에 대해 전문화된 네트워크 하위 집합인 전문가들을 동적으로 활성화하여 효율성과 성능을 향상시키는 신경망 설계입니다. 게이팅 네트워크가 어떤 전문가를 활성화할지 결정하며, 희소한 활성화와 계산 비용을 줄이게 됩니다. MoE 아키텍처는 게이팅 네트워크와 전문가라는 두 가지 중요 구성 요소로 이루어져 있습니다. 이를 자세히 살펴보겠습니다:

MoE 아키텍처는 핵심적으로 효율적인 교통 시스템처럼 작동하여 실시간 상황과 원하는 목적지에 따라 각 차량(또는 이 경우 데이터)을 최적의 경로로 안내합니다. 각 작업은 그 작업을 처리하기에 특화된 가장 적합한 전문가 또는 서브 모델로 라우팅됩니다. 이 동적 라우팅은 각 작업에 가장 능숙한 자원이 활용되어 전반적인 모델의 효율성과 효과성을 향상시킵니다. MoE 아키텍처는 모델의 충실도를 향상시키는 세 가지 방법을 모두 활용합니다.

- 여러 전문가를 도입함으로써, MoE는 모델의 매개 변수 크기를 증가시킴으로써
- 각 전문가당 매개변수를 추가합니다.
- MoE는 지정된 작업에 사용될 전문가를 결정하기 위한 게이트된 네트워크를 통합하는 고전적인 신경망 아키텍처를 변경합니다.
- 모든 AI 모델은 어느 정도의 세밀한 조정을 갖고 있으며, MoE의 모든 전문가는 추가로 의도대로 작동하도록 조정되어 전통적인 모델이 활용하지 못하는 조정의 추가층을 제공합니다.

<div class="content-ad"></div>

# MoE 게이팅 네트워크

게이팅 네트워크는 MoE 모델 내에서 의사 결정자이자 컨트롤러 역할을 합니다. 이는 들어오는 작업들을 평가하고 어떤 전문가가 처리할 수 있는지 결정합니다. 이 결정은 일반적으로 학습된 가중치를 기반으로 하며, 이러한 가중치는 훈련을 통해 시간이 지남에 따라 조정되어 작업과 전문가를 매칭하는 능력을 더욱 향상시킵니다. 게이팅 네트워크는 여러 전략을 활용할 수 있으며, 확률론적 방법에서는 여러 전문가에게 소프트 할당을 하거나, 결정론적 방법에서는 각 작업을 단일 전문가로 라우팅하는 등 다양한 전략을 채택할 수 있습니다.

# MoE 전문가들

MoE 모델 내의 각 전문가는 특정 문제 도메인 하위 집합에 최적화된 작은 신경망, 기계 학습 모델 또는 LLM을 대표합니다. 예를 들어, Mistral에서 다양한 전문가들은 특정 언어, 방언 또는 쿼리 유형을 이해하는 데 특화될 수 있습니다. 이러한 전문화는 각 전문가가 자신의 전문 분야에 능통함을 보장하며, 다른 전문가들의 기여와 결합되었을 때 다양한 작업에 뛰어난 성능을 나타낼 수 있도록 합니다.

<div class="content-ad"></div>

# MoE 손실 함수

MoE 아키텍처의 주요 구성 요소로 간주되지는 않지만, 손실 함수는 모델의 미래 성능에 중추적인 역할을 합니다. 개별 전문가 및 게이팅 네트워크를 최적화하기 위해 설계되었기 때문에 중요합니다.

일반적으로 손실 함수는 각 전문가에 대해 계산된 손실을 게이팅 네트워크에 의해 할당된 확률 또는 중요성에 의해 가중치가 부여됩니다. 이를 통해 각 전문가를 특정 작업에 맞게 세밀하게 조정하면서 게이팅 네트워크를 조정하여 경로 지정 정확도를 향상할 수 있습니다.

![Image](/assets/img/2024-07-09-WhytheNewestLLMsuseaMoEMixtureofExpertsArchitecture_1.png)

<div class="content-ad"></div>

# MoE 프로세스 전반적으로

이제 전 과정을 정리해보겠습니다. 좀 더 자세한 내용을 추가해보겠습니다.

다음은 라우팅 프로세스가 시작부터 끝까지 어떻게 작동하는지에 대한 간략한 설명입니다:

- 입력 처리: 수신 데이터의 초기 처리. LLMs의 경우 Prompt가 주로 사용됩니다.
- 피처 추출: 분석을 위해 원시 입력을 변환합니다.
- 게이팅 네트워크 평가: 전문가 적합성을 확률이나 가중치를 통해 평가합니다.
- 가중 라우팅: 계산된 가중치를 기반으로 입력을 할당합니다. 이 과정에서 가장 적합한 LLM을 선택하는 작업이 완료됩니다. 경우에 따라 하나의 입력에 대해 여러 LLM이 선택되기도 합니다.
- 작업 실행: 각 전문가가 할당된 입력을 처리합니다.
- 전문가 결과 통합: 개별 전문가 결과를 최종 출력으로 결합합니다.
- 피드백 및 적응: 성능 피드백을 사용하여 모델을 개선합니다.
- 반복적 최적화: 라우팅 및 모델 매개변수의 계속적인 개선입니다.

<div class="content-ad"></div>

# **MoE 구조를 활용한 인기 있는 모델들**

- OpenAI의 GPT-4와 GPT-4o: GPT-4와 GPT-4o는 ChatGPT의 프리미엄 버전을 구동합니다. 이 다중 모달 모델은 이미지, 텍스트, 음성과 같은 다양한 소스 형식을 수용할 수 있도록 MoE를 활용합니다. GPT-4는 각각 2200억개의 매개변수를 가진 8명의 전문가를 보유하고 있어 전체 모델의 총 매개변수 수가 1.7조개를 넘는다는 소문과 약간의 확인이 있습니다.
- Mistral AI의 Mixtral 8x7b: Mistral AI는 강력한 AI 모델을 오픈 소스로 제공하며, Mixtral 모델이 작은 용량으로 제공되는 sMoE 모델 또는 sparse Mixture of Experts 모델이라고 밝혔습니다. Mixtral 8x7b는 총 467억개의 매개변수를 가지고 있지만, 토큰당 12.9B 매개변수만을 사용하여 해당 비용으로 입력과 출력을 처리합니다. 그들의 MoE 모델은 Llama2 (70B)와 GPT-3.5 (175B)를 합친 것보다 운영 비용이 적게 들면서도 일관되게 성능이 우수합니다.

# **MoE의 이점 및 선호되는 구조인 이유**

최종적으로, MoE 구조의 주요 목표는 복잡한 기계 학습 작업에 접근하는 방법에 혁명을 가져다주는 것입니다. 이는 전통적인 모델보다 독특한 이점을 제공하며 여러 가지 방법으로 전통적인 모델에 우월성을 보여줍니다.

<div class="content-ad"></div>

에러 없이 잘 작동했습니다. 감사합니다!

<div class="content-ad"></div>

Specialization and Accuracy:

- An MoE system allows each expert to focus on specific aspects of a problem, leading to enhanced expertise and precision in those particular areas.
- This kind of specialization proves beneficial in fields such as medical imaging or financial forecasting, where accuracy is crucial.
- MoE excels in producing superior outcomes within specific domains, thanks to its nuanced comprehension, in-depth knowledge, and its ability to surpass generalist models in specialized tasks.

![MoE Architecture](/assets/img/2024-07-09-WhytheNewestLLMsuseaMoEMixtureofExpertsArchitecture_2.png)

## Challenges of the MoE Architecture

<div class="content-ad"></div>

MoE 구조는 많은 이점을 제공하지만 도입과 효과에 영향을 줄 수 있는 도전 과제들도 가지고 있어요.

- 모델 복잡성: 여러 신경망 전문가와 트래픽을 제어하는 게이팅 네트워크를 관리하는 것은 MoE 개발 및 운영 비용을 어렵게 만들어요.
- 훈련 안정성: 게이팅 네트워크와 전문가들 사이의 상호작용으로 일관된 학습 속도를 달성하기 어렵게 만들고 많은 하이퍼파라미터 튜닝을 요구하는 예측할 수 없는 동력을 도입해요.
- 불균형: 전문가들을 활용하지 않고 방치하는 것은 MoE 모델의 최적화에 좋지 않아요. 사용하지 않는 전문가에 자원을 낭비하거나 특정 전문가에 지나치게 의존하는 것은 문제에요. 작업 부하 분배를 균형있게 조절하고 효과적인 게이트를 조정하는 것이 MoE 인공지능의 성능을 높이는 데 중요해요.

상기 단점들은 대개 MoE 구조가 개선됨에 따라 시간이 지나면서 점차 사라지는 경향이 있어요.

# 전문화에 의해 모양잡히는 미래

<div class="content-ad"></div>

MoE 방법론을 고찰해보면 인간과의 유사성을 발견할 수 있습니다. 전문화된 팀이 일반화된 직장보다 더 많은 성과를 거둬내는 것처럼, 전문화된 모델은 AI 모델 중에선 단일체 모델들을 능가합니다. 다양성과 전문성을 우선시하는 것은 대규모 문제의 복잡성을 전문가들이 효과적으로 다룰 수 있는 관리 가능한 세그먼트로 바꿀 수 있습니다.

미래를 내다보며 전문화된 시스템이 다른 기술 분야의 발전에 미치는 폭넓은 영향을 고려해봅시다. MoE의 원칙은 의료, 금융, 자율 시스템과 같은 분야에서 발전에 영향을 미치며, 보다 효율적이고 정확한 솔루션을 장려합니다.

MoE의 여정은 시작에 불과하며, 계속되는 진화는 AI를 넘어 다른 혁신을 동력으로 삼을 것입니다. 고성능 하드웨어가 계속 발전함에 따라, 전문가 AI들의 이 혼합체는 스마트폰에서도 머무를 수 있게 될 것입니다. 하지만 먼저, 누군가가 하나를 훈련시켜야 할 것입니다.
