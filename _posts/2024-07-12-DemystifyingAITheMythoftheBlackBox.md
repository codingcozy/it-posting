---
title: "인공지능 해부 블랙 박스 신화의 진실"
description: ""
coverImage: "/assets/img/2024-07-12-DemystifyingAITheMythoftheBlackBox_0.png"
date: 2024-07-12 23:57
ogImage:
  url: /assets/img/2024-07-12-DemystifyingAITheMythoftheBlackBox_0.png
tag: Tech
originalTitle: "Demystifying AI: The Myth of the Black Box"
link: "https://medium.com/@tristwolff/demystifying-ai-the-myth-of-the-black-box-0e44fc657e2a"
isUpdated: true
---

이미지를 Markdown 형식으로 바꿔볼까요?

![2024-07-12-DemystifyingAITheMythoftheBlackBox_0](/assets/img/2024-07-12-DemystifyingAITheMythoftheBlackBox_0.png)

## AI의 “블랙 박스” 오해

AI에서 “블랙 박스”라는 용어는 신경망의 불투명한 성격을 설명하는 데 사용되었습니다. 여기에서의 아이디어는 다음과 같습니다: 우리가 신경망에 무엇을 주입했는지(“입력”)를 알고, 신경망에서 무엇이 나왔는지(“출력”)를 관찰하지만, 이 둘 사이에서 무슨 일이 벌어지는지 알 수 없다는 것입니다. 구체적으로 말하자면, 입력이 어떤 규칙에 따라 출력으로 변환되는지 관찰할 수 없는 것입니다.

![2024-07-12-DemystifyingAITheMythoftheBlackBox_1](/assets/img/2024-07-12-DemystifyingAITheMythoftheBlackBox_1.png)

<div class="content-ad"></div>

이 투명도는 항상 그랬던 것은 아닙니다. AI의 초기에는 연구자들이 그들의 의사결정 과정을 쉽게 추적할 수 있을 만큼 시스템이 단순했습니다. 그러나 사랑하는 독자들도 알다시피, 신경망은 수천 개부터 수백만 개, 그리고 수십억 개의 매개변수(매개변수란 신경망 내부의 데이터 처리의 가장 작은 단위인 "노드", "뉴런", 또는 "가중치"를 부르는 용어 중 하나입니다. 인간 두뇌에서 영감을 받은 것으로 유명합니다.)로 커져서 복잡성은 급격히 증가했고, 이에 신경망이 작동하는 방식을 해석하는 것이 더욱 어려워졌습니다.

상황을 더욱 악화시킨 것은: 전통적인 컴퓨터 프로그램처럼 각 코드 줄마다 명확한 목적이 있는 것과는 달리 신경망에서는 각 "개념"이 수많은 뉴런에 퍼져 있고, 각 뉴런이 여러 "개념"에 기여한다는 점입니다(여기서 "개념"이란 신경망이 학습할 수 있는 언어, 소리 또는 이미지와 같은 문맥 내의 의미 단위를 가리키는 용어를 사용합니다).

그러나 최근 Anthropc(클로드 소네에 대한 혁신적인 작업으로)와 OpenAI(그들의 GPT-4 연구로)가 이룬 최근의 발전은 현대 제작 등급 AI 시스템 내부의 첫 번째 자세한 통찰력을 제공했습니다. 이 연구소에서 연구하는 연구자들이 성취한 것은 AI "사고"를 위한 개념 사전을 만들어, AI가 학습한 각 개별적인 아이디어나 개념을 나타내는 수백만 개의 특징을 추출한 것입니다. 요컨대, AI가 블랙 박스라는 개념은 구식입니다.

## AI 해석을 위한 도구와 기법

<div class="content-ad"></div>

기본적으로, 우리는 인조 뇌를 위한 MRI 기계를 개발한 것과 같은 것입니다. 뇌 이미징 기술을 연상시키는 시각화 방법을 통해 연구자들은 신경망의 다양한 부분이 다양한 입력에 대해 어떻게 반응하는지를 "보는" 것이 가능해졌습니다.

예를 들어, OpenAI는 GPT-4에서 1600만 개의 특징을 추출하는 데 성공했는데, 각 특징은 모델 내에서 고유한 개념이나 사고 패턴을 나타낼 수 있습니다. 이 첨단 기술은 우리에게 AI가 정보를 처리하는 방식을 시각화할 수 있는 가능성을 열어주었습니다. "인간의 불완전성"과 같은 추상적인 개념부터 "대수학적 링"과 같은 구체적인 주제까지를 모델이 어떻게 다루는지를 시각적으로 확인할 수 있게 되었습니다. Anthropic과 OpenAI의 연구는 두 모델이 학습한 다양한 개념의 풍부한 실마리를 발견하게 해 주었으며, 이는 우리가 AI의 내부 언어를 해독하기 시작할 수 있게 해주었습니다.

인공지능 시스템이 인간의 삶에 영향을 미치는 결정을 점점 더 내리는 가운데, 설명 가능한 인공지능에 대한 필요성은 기술적 호기심에서 윤리적 요구사항으로 변모했습니다. 의료, 금융, 그리고 법 진술과 같은 분야에서는 AI가 결론에 도달하는 방식을 이해하는 것이 공정함과 책임감을 보장하기 위해 중요합니다.

인터프리터빌리티의 잠재적 이점은 윤리를 넘어 확장됩니다. 모델이 정보를 표현하고 처리하는 방식을 이해함으로써, 연구자들은 편향과 안전 위험을 식별하고 완화할 수 있습니다. 예를 들어, Anthropic과 OpenAI의 연구원들은 잠재적으로 문제가 될 수 있는 행동이나 개념에 해당하는 특징을 발견하여 더 목표적인 안전 조치의 문을 열었습니다. 그리고 더 발전한 AI를 사용하여 보다 간단한 AI 시스템을 해석하는 흥미로운 가능성도 있습니다.

<div class="content-ad"></div>

하지만 OpenAI 연구원인 Leo Gao가 언급한 대로, 발견된 기능 중 많은 부분은 여전히 해석하기 어려운 것들이며, 몇 가지는 명확하게 부호화된 개념과 관련이 없는 패턴으로 활성화됩니다. 게다가 현재 기술들은 원래 모델의 모든 행동을 포착하지 못합니다. 해석된 버전과 원본 간의 성능 차이가 여전히 존재합니다.

우리가 새로운 시대의 경계에 서있는 지금, 블랙 박스는 천천히지만 확실하게 투명해지고 있습니다. 발견의 여정은 단지 시작에 불과하며, 앞으로 예상되는 발견들은 변화적이면서도 밝혀지는 만큼 혁신적일 것입니다. Anthropic과 OpenAI 모두 연구 및 시각화를 포함한 코드를 오픈소스로 공개하여 해당 분야에서의 더 깊은 탐구를 촉진하고 있습니다. 이 협업적 접근은 해석력 연구의 중요성을 강조하며, 더 안전하고 믿을 수 있는 AI 시스템의 구축에 기여합니다. 우리가 AI 능력의 경계를 더욱 넓혀 나감에 따라, 해석력의 병행적인 발전은 이러한 강력한 도구들이 블랙 박스로 남지 않고, 이해 가능하고, 제어 가능하며, 인간의 가치와 일치하게 되도록 보장합니다.
