---
title: "PyTorchë¡œ GPT-2 êµ¬ì¶•í•˜ê¸° 2ë¶€"
description: ""
coverImage: "/assets/img/2024-07-13-BuildingGPT-2withPyTorchPart2_0.png"
date: 2024-07-13 23:18
ogImage:
  url: /assets/img/2024-07-13-BuildingGPT-2withPyTorchPart2_0.png
tag: Tech
originalTitle: "Building GPT-2 with PyTorch (Part 2)"
link: "https://medium.com/towards-artificial-intelligence/heres-how-you-can-build-and-train-gpt-2-from-scratch-using-pytorch-part-2-9b41d15baf62"
isUpdated: true
---

ì´ê²ƒì€ GPT-2ë¥¼ ì²˜ìŒë¶€í„° ë§Œë“œëŠ” í”„ë¡œì íŠ¸ì˜ ë‘ ë²ˆì§¸ ë¶€ë¶„ì…ë‹ˆë‹¤. ì•„ì§ ì²« ë²ˆì§¸ ë¶€ë¶„ì„ ì½ì§€ ì•Šìœ¼ì…¨ë‹¤ë©´, ê³„ì†í•˜ê¸° ì „ì— ì–¸ì–´ ëª¨ë¸ì˜ ê¸°ë³¸ ì‚¬í•­ì— ìµìˆ™í•´ì§€ëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤.

- GPT-2 êµ¬ì¶• ë° í›ˆë ¨ (1ë¶€)

ìµœì¢… ë¡œìŠ¤:

![ì´ë¯¸ì§€](/assets/img/2024-07-13-BuildingGPT-2withPyTorchPart2_0.png)

<div class="content-ad"></div>

# 4. GPT-2 ì•„í‚¤í…ì²˜ êµ¬í˜„í•˜ê¸°

ì´ë²ˆ ì„¹ì…˜ì—ì„œëŠ” GPT-2ì˜ ë¶€ë¶„ë“¤ì„ í•˜ë‚˜ì”© ì¶”ê°€í•˜ê³  ê° ë‹¨ê³„ì—ì„œ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í›ˆë ¨í•˜ê³  í‰ê°€í•´ë³¼ ê²ƒì…ë‹ˆë‹¤. ë‹¤ìŒì€ ì§„í–‰ ë°©ì‹ì…ë‹ˆë‹¤:

a. ìœ„ì¹˜ ì¸ì½”ë”© + ì™„ì „ ì—°ê²°ì¸µ (NN)

b. (ê°€ë¦¬í‚¨) ì…€í”„ ì–´í…ì…˜ + ì •ê·œí™”

<div class="content-ad"></div>

c. (ê°€ë ¤ì§„) ë©€í‹° í—¤ë“œ ì–´í…ì…˜

d. ë‹¤ì¤‘ GPT ë””ì½”ë” ë¸”ë¡

e. í† í¬ë‚˜ì´ì € ê°œì„ 

f. ìµœì¢… GPT-2 í›ˆë ¨

<div class="content-ad"></div>

ì´ì „ ë¶€ë¶„ì„ ê¸°ì–µí•´ ë³´ê² ìŠµë‹ˆë‹¤. ëª¨ë¸ì€ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤:

![ì´ë¯¸ì§€](/assets/img/2024-07-13-BuildingGPT-2withPyTorchPart2_1.png)

ì½”ë“œ:

```python
import torch.nn as nn
import torch.nn.functional as F

# ì„ë² ë”© í¬ê¸° ì •ì˜ì— ì‚¬ìš©ë¨
d_model = vocab_size

class GPT(nn.Module):
    def __init__(self, vocab_size, d_model):
        super().__init__()
        self.wte = nn.Embedding(vocab_size, d_model)  # ë‹¨ì–´ í† í° ì„ë² ë”©

    def forward(self, inputs, targets=None):
        logits = self.wte(inputs)  # ì°¨ì› -> ë°°ì¹˜ í¬ê¸°, ì‹œí€€ìŠ¤ ê¸¸ì´, d_model
        loss = None
        if targets is not None:
            batch_size, sequence_length, d_model = logits.shape
            # ë°°ì¹˜ ë‚´ ëª¨ë“  í† í° ì„ë² ë”©ì— ëŒ€í•œ ì†ì‹¤ ê³„ì‚°
            logits = logits.view(batch_size * sequence_length, d_model)
            targets = targets.view(batch_size * sequence_length)
            loss = F.cross_entropy(logits, targets)
        return logits, loss

    def generate(self, inputs, max_new_tokens):
        # ì´ê³³ì— ëª¨ë¸ ì¶œë ¥ì„ ì´ˆê¸° ì…ë ¥ ì‹œí€€ìŠ¤ì™€ í•¨ê»˜ ì €ì¥í•  ê²ƒì…ë‹ˆë‹¤
        # ëª¨ë¸ê³¼ ê°„ì„­í•˜ì§€ ì•Šë„ë¡ ë³µì‚¬ë³¸ì„ ìƒì„±í•©ë‹ˆë‹¤
        for _ in range(max_new_tokens):
            # ì†ì‹¤ ê³„ì‚°ì„ ìœ„í•´ êµìœ¡ ì¤‘ì—ë§Œ ëŒ€ìƒì„ ì „ë‹¬í•©ë‹ˆë‹¤
            logits, _ = self(inputs)
            # ëª¨ë“  ë°°ì¹˜ì— ëŒ€í•´, ë§ˆì§€ë§‰ìœ¼ë¡œ ì˜ˆì¸¡í•œ ì‹œí€€ìŠ¤ì˜ ì„ë² ë”©ì„ ê°€ì ¸ì˜µë‹ˆë‹¤
            logits = logits[:, -1, :]
            probs = F.softmax(logits, dim=1)
            # ì…ë ¥ í™•ë¥ ì— ê¸°ë°˜í•˜ì—¬ ê°€ëŠ¥ì„± ìˆëŠ” í† í°ì„ ê°€ì ¸ì˜µë‹ˆë‹¤
            idx_next = torch.multinomial(probs, num_samples=1)

            inputs = torch.cat([inputs, idx_next], dim=1)
        # ì´ˆê¸° ì…ë ¥ + ëª¨ë¸ ì¶œë ¥ì´ ëª¨ë‘ í¬í•¨ëœ inputsë¥¼ ìµœì¢… ì¶œë ¥ìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤
        return [decode(out.tolist()) for out in inputs]

m = GPT(vocab_size=vocab_size, d_model=d_model).to(device)
```

<div class="content-ad"></div>

## a. ìœ„ì¹˜ ë¶€í˜¸ ì¸ì½”ë”© (PE) + ì‹ ê²½ë§ (NN) ì¶”ê°€

ì´ì œ ëª¨ë¸ì— ìœ„ì¹˜ ë¶€í˜¸ ì¸ì½”ë”©ì„ ì¶”ê°€í•´ ë´…ì‹œë‹¤:

![image](/assets/img/2024-07-13-BuildingGPT-2withPyTorchPart2_2.png)

ì¶œë ¥ ì„ë² ë”©(ìš°ë¦¬ì˜ ê²½ìš° ì…ë ¥ ì„ë² ë”©ì¸ wte)ì— ìœ„ì¹˜ ë¶€í˜¸ ì¸ì½”ë”©ì„ ì¶”ê°€í•˜ê³  ë‚˜ì„œ ë” ë‚˜ì•„ê°€ëŠ” ì‹ ê²½ë§ìœ¼ë¡œ ì „ë‹¬ë©ë‹ˆë‹¤. PEê°€ ë¬´ì—‡ì¸ì§€ ì´í•´í•˜ë ¤ë©´ í† í° ì„ë² ë”©ì„ ìƒê¸°í•´ ë´…ì‹œë‹¤. í† í° ì„ë² ë”©ì€ ìš°ë¦¬ ì–´íœ˜ì—ì„œ ê° ë¬¸ìì— ëŒ€í•´ d_model ì°¨ì›ì˜ ë²¡í„°ë¥¼ ì €ì¥í•©ë‹ˆë‹¤. ì´ëŠ” ë¬¸ìì˜ ë‹¤ë¥¸ íŠ¹ì„±ì„ í›ˆë ¨ ì‹œ ì–´ë–»ê²Œ ë‚˜íƒ€ë‚¬ëŠ”ì§€ì™€ ì–´ë””ì— ë‚˜íƒ€ë‚¬ëŠ”ì§€ì— ê¸°ë°˜í•˜ì—¬ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.

<div class="content-ad"></div>

ì´ì™€ ìœ ì‚¬í•˜ê²Œ ìœ„ì¹˜ ì¸ì½”ë”©ì€ `context_length`ì—ì„œ ê° ë¬¸ìì˜ ìˆœì„œ/ìœ„ì¹˜ ì‹ í˜¸ë¥¼ ì €ì¥í•©ë‹ˆë‹¤. ì½”ì‚¬ì¸ ë° ì‚¬ì¸ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ í•œ ë²ˆë§Œ ê³„ì‚°ë˜ë©° í›ˆë ¨ì´ í•„ìš”í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì´ëŠ” ì‹œí€€ìŠ¤ì˜ ê° ë¬¸ìì˜ ìœ„ì¹˜ ë²¡í„°ê°€ êµìœ¡ ì„¸íŠ¸ì˜ ëª¨ë“  ë°ì´í„°ì— ëŒ€í•´ ë™ì¼í•  ê²ƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤.

ë”°ë¼ì„œ ë‘ ê°€ì§€ë¥¼ í•¨ê»˜ ì¶”ê°€í•˜ë©´ ëª¨ë¸ì´ ë” ì˜ í•™ìŠµí•˜ë„ë¡ ë„ì™€ì£¼ëŠ” ì†ì„± + ì‹œí€€ìŠ¤ ë‚´ ë¬¸ìì˜ ìœ„ì¹˜ë¥¼ ì–»ê²Œ ë©ë‹ˆë‹¤.

ëª¨ë¸ì— PE(Positional Encoding)ë¥¼ ì¶”ê°€í•˜ëŠ” ë°©ë²•ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

```python
# PE í´ë˜ìŠ¤ ì •ì˜
class PositionalEncoding(nn.Module):
    def __init__(self, context_length, d_model) -> None:
        super().__init__()
        # positional encodingsì„ ì €ì¥í•  (context_length, d_model) ëª¨ì–‘ì˜ í–‰ë ¬ ìƒì„±
        pe = torch.zeros(context_length, d_model)

        # (context_length, 1) ëª¨ì–‘ì˜ ìœ„ì¹˜ [0, 1, 2, ..., context_length-1]ì„ ê°€ì§„ ë²¡í„° ìƒì„±
        position = torch.arange(0, context_length, dtype=torch.float).unsqueeze(1)

        # ì°¨ì›ì— ë”°ë¼ ê¸°ë³¸ ì ê¸°ì¤€ ë²¡í„° ìƒì„±
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))

        # ì‚¼ê°í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ positional encodings ê³„ì‚°
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)

        pe = pe.unsqueeze(0)  # ëª¨ì–‘: (1, context_length, d_model)

        # peë¥¼ ë²„í¼ë¡œ ë“±ë¡í•˜ì—¬ íŒŒë¼ë¯¸í„°ê°€ ì•„ë‹Œ ëª¨ë“ˆ ìƒíƒœì˜ ì¼ë¶€ë¡œ ì·¨ê¸‰
        self.register_buffer('pe', pe)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # ì…ë ¥ ì„ë² ë”©ì— positional encodings ì¶”ê°€
        return x + self.pe[:,:x.size(1), :]

class GPT(nn.Module):
    def __init__(self, vocab_size, d_model):
        ...
       # positional encodings ì´ˆê¸°í™”
       self.wpe = PositionalEncoding(context_length, d_model)

    def forward(self, inputs, targets = None):
        logits = self.wte(inputs)
        # ë¡œì§“ì„ PEì— ì „ë‹¬
        logits = self.wpe(logits)
        ...
        return logits, loss
```

<div class="content-ad"></div>

Now, if you try to train the model and generate a sequence, you might encounter an error similar to the one below:

![Error Message](/assets/img/2024-07-13-BuildingGPT-2withPyTorchPart2_3.png)

This error indicates that we attempted to generate 1000 tokens one by one, passing the previous _n_ tokens to the model to predict the next token. However, with the addition of the PositionalEmbedding layer, it now only accepts tokens of a size less than or equal to the context_length, which is set at 256 in our case.

Let's adjust our generate function to accommodate the context_length:

<div class="content-ad"></div>

```python
def generate(self, inputs, max_new_tokens):
Â Â Â # ì´ ì €ì¥ì†ŒëŠ” ì´ˆê¸° ì…ë ¥ ì‹œí€€ìŠ¤ì™€ í•¨ê»˜ ëª¨ë¸ ì¶œë ¥ì„ ì €ì¥í•  ê²ƒì…ë‹ˆë‹¤.
Â Â Â # ëª¨ë¸ê³¼ ê°„ì„­í•˜ì§€ ì•Šë„ë¡ ë³µì‚¬ë³¸ì„ ë§Œë“­ë‹ˆë‹¤.
Â Â Â output = inputs.clone()
Â Â Â for _ in range(max_new_tokens):
Â Â Â Â Â Â Â current_seq_length = inputs.size(1)
Â Â Â Â Â Â Â # context_lengthë¥¼ ì´ˆê³¼í•˜ë©´ ì…ë ¥ì„ ìë¦…ë‹ˆë‹¤.
Â Â Â Â Â Â Â if current_seq_length > context_length:
Â Â Â Â Â Â Â Â Â Â Â inputs = inputs[:, -context_length:]
Â Â Â Â Â Â Â ...
Â Â Â Â Â Â Â output = torch.cat([output, idx_next], dim=1)
Â Â Â return [decode(out.tolist()) for out in output]
```

ëª¨ë¸ì„ ì´ë¯¸ í›ˆë ¨ì‹œí‚¤ê³  í–¥ìƒì„ ê´€ì°°í•  ìˆ˜ ìˆì§€ë§Œ, ê·¸ ì „ì— í•œ ë‹¨ê³„ ë” ì¶”ê°€í•´ ë³´ê² ìŠµë‹ˆë‹¤. í˜„ì¬ ë¬¸ìì˜ ë‹¤ë¥¸ í‘œí˜„ì„ ì–»ê³  ëª¨ë¸ì— í”¼ë“œí•˜ê³  ìˆëŠ” ë°©ì‹ì„ ìƒê¸°í•´ë³´ì„¸ìš”. ë§Œì•½ ì´ ì •ë³´ë¥¼ ê²°í•©í•˜ê³  ë” ë³µì¡í•œ ì„ë² ë”© í‘œí˜„ì„ í•™ìŠµí•˜ëŠ” ë° ì¶”ê°€ ë„¤íŠ¸ì›Œí¬ê°€ ìˆë‹¤ë©´ ì–¼ë§ˆë‚˜ ìœ ìµí• ê¹Œìš”?

ì´ê²ƒì´ Fully Connected Networksì…ë‹ˆë‹¤. PyTorchì˜ Linear Layerë¥¼ ëª¨ë¸ì— ì¶”ê°€í•´ ë³´ê² ìŠµë‹ˆë‹¤:

![2024-07-13-BuildingGPT-2withPyTorchPart2_4](/assets/img/2024-07-13-BuildingGPT-2withPyTorchPart2_4.png)

<div class="content-ad"></div>

ì½”ë“œ:

```js
class GPT(nn.Module):
    def __init__(self, vocab_size, d_model):
        ...
        self.fcn = nn.Sequential(
            nn.Linear(d_model, 4 * d_model),
            nn.GELU(),
            nn.Linear(4 * d_model, d_model)
        )

    def forward(self, inputs, targets = None):
        ...
        logits = self.fcn(logits)
        ...
        return logits, loss
```

ê·¸ê²Œ ë‹¤ì•¼! ì •ë§ ê°„ë‹¨í•˜ì£ !! ì´ì œ ëª¨ë¸ì„ í›ˆë ¨í•˜ê³  ì„±ëŠ¥ì„ í‰ê°€í•´ë³¼ê²Œìš”. ì´ë²ˆ ì‹¤í–‰ì—ì„œëŠ” ì—í¬í¬ë¥¼ 5000ìœ¼ë¡œ, í•™ìŠµë¥ ì„ 1e-3ìœ¼ë¡œ ì„¤ì •í–ˆì–´ìš”.

![ì´ë¯¸ì§€](/assets/img/2024-07-13-BuildingGPT-2withPyTorchPart2_5.png)

<div class="content-ad"></div>

ì—¬ê¸° ì‚¬ì§„ì€ b. (ê°€ë ¤ì§„) ì…€í”„ ì–´í…ì…˜ + ì •ê·œí™”ì˜ ê³¼ì •ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ì´ ë¶€ë¶„ë„ ì—´ì‹¬íˆ ê³µë¶€í•˜ì‹œëŠ”êµ°ìš”! ê³„ì†í•´ì„œ í•¨ê»˜ ì•Œì•„ë³´ë„ë¡ í•˜ì£ .

\![ì´ë¯¸ì§€](/assets/img/2024-07-13-BuildingGPT-2withPyTorchPart2_6.png)

ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥

<div class="content-ad"></div>

íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ì—ì„œ ê°€ì¥ í¥ë¯¸ë¡œìš´ ë¶€ë¶„ì€ Self-Attentionì…ë‹ˆë‹¤. ì´ ê°œë…ì„ ë” ì˜ ì´í•´í•˜ê¸° ìœ„í•´ Jay Alammarì˜ ì‹œê° ìë£Œë¥¼ ì°¸ì¡°í•˜ì„¸ìš”. ê°„ë‹¨íˆ ë§í•´ì„œ, Self-Attentionì€ í˜„ì¬ ë° ì´ì „ n í† í°ì´ ì£¼ì–´ì¡Œì„ ë•Œ ëª¨ë¸ì´ ë‹¤ìŒ í† í°ì— ë” ë§ì€ ê´€ì‹¬ì„ ê¸°ìš¸ì–´ì•¼ í•˜ëŠ”ì§€ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.

ì´ë¥¼ ìœ„í•´ ê° ë¬¸ì(ìš°ë¦¬ì˜ ê²½ìš°)ì˜ ì„ë² ë”©ì— ì ìˆ˜ë¥¼ í• ë‹¹í•˜ê³  ì´ë¥¼ ì¿¼ë¦¬, í‚¤, ë°¸ë¥˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹¤ì–‘í•œ ë§¥ë½ì— ë”°ë¼ ê²°í•©í•©ë‹ˆë‹¤.

ì´ë¡ ì´ ì¶©ë¶„í•˜ë‹¤ë©´, ì½”ë”©ìœ¼ë¡œ ë„˜ì–´ê°€ ë´…ì‹œë‹¤:

ë‹¤ìŒì€ ëª¨ë¸ì— Self-Attentionì„ ì¶”ê°€í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤:

<div class="content-ad"></div>

```python
class SelfAttention(nn.Module):
    def __init__(self, d_model: int):
        super().__init__()

        self.query = nn.Linear(d_model, d_model)
        self.key = nn.Linear(d_model, d_model)
        self.value = nn.Linear(d_model, d_model)
        self.fc_out = nn.Linear(d_model, d_model)
        self.dropout = nn.Dropout(0.2)

    def forward(self, inputs: torch.Tensor):
        B, seq_length, d_model = inputs.shape

        # Project the input embeddings into Q, K, and V
        Q = self.query(inputs)
        K = self.key(inputs)
        V = self.value(inputs)

        # Compute attention scores
        attention_scores = torch.matmul(Q, K.transpose(-2, -1))

        # Apply mask to prevent attention to future tokens
        mask = torch.triu(torch.ones(seq_length, seq_length), diagonal=1).bool().to(inputs.device)
        attention_scores = attention_scores.masked_fill(mask, float('-inf'))

        attention_weights = torch.softmax(attention_scores, dim=-1)
        # Compute the weighted sum of the values
        attention_output = torch.matmul(attention_weights, V)

        # Apply the final linear transformation
        out = self.fc_out(attention_output)

        return out

class GPT(nn.Module):
    def __init__(self, vocab_size, d_model):
        ...
        self.att = SelfAttention(d_model)

    def forward(self, inputs, targets = None):
        ...
        logits = self.att(logits)
        logits = self.fcn(logits)
        ...
        return logits, loss
```

Just like that, we have our code ready. Let's embark on the training journey and witness the results:

![GPT-2 model](/assets/img/2024-07-13-BuildingGPT-2withPyTorchPart2_7.png)

ğŸ˜Oh, wow! Do you feel the same way as I do? It seems like the model is grasping the essence of word representations and their harmonious arrangement in a piece of music, doesn't it? Truly remarkable! Imagine the magic when this layer evolves into Multi-Head.

<div class="content-ad"></div>

ì •ê·œí™”(Normalization)

í•¨ê»˜ ëª¨ë¸ì„ í›ˆë ¨ ì¤‘ì´ë¼ë©´ ì£¼ëª©í•  ì  ì¤‘ í•˜ë‚˜ëŠ” ì†ì‹¤ì´ ë§¤ìš° ë¹ ë¥´ê²Œ ê°ì†Œí•˜ëŠ” ë™ì•ˆ ëª¨ë¸ì´ ë°ì´í„°ì— ê³¼ì í•©ë˜ê¸° ì‹œì‘í•œë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤. ì´ëŠ” ëª¨ë¸ì´ ìš°ë¦¬ê°€ ê°€ì§„ ì œí•œëœ í›ˆë ¨ ë°ì´í„°ì— ë¹„í•´ ë„ˆë¬´ ì»¤ì§€ëŠ” ê²½ìš°ì— ì¼ì–´ë‚  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ì´ë¥¼ ì™„í™”í•˜ê¸° ìœ„í•´ LayerNormê³¼ Dropout ë ˆì´ì–´ë¥¼ ì¶”ê°€í•˜ì—¬ í•™ìŠµì„ ê· í˜•ìˆê²Œ ë§ì¶”ì–´ ë³´ê² ìŠµë‹ˆë‹¤.

![image](/assets/img/2024-07-13-BuildingGPT-2withPyTorchPart2_8.png)

<div class="content-ad"></div>

ì½”ë“œ:

```python
class GPT(nn.Module):
    def __init__(self, vocab_size, d_model):
        ...
        self.ln1 = nn.LayerNorm(d_model)
        self.ln2 = nn.LayerNorm(d_model)
        self.dropout = nn.Dropout(0.2)

    def forward(self, inputs, targets=None):
        ...
        logits = self.wte(inputs)
        logits = self.wpe(logits)
        att_logits = self.att(logits)
        adn_logits = self.ln1(logits + att_logits)
        logits = self.dropout(adn_logits)
        logits = self.fcn(logits)
        logits = self.ln2(logits + adn_logits)
        ...
        return logits, loss

    ...
```

ë°ì´í„°ì…‹ì— ì˜¤ë²„í”¼íŒ… ì—†ì´ ëª¨ë¸ì„ ë” ì˜¤ë«ë™ì•ˆ í•™ìŠµí•  ìˆ˜ ìˆë„ë¡ ë„ì™€ì¤ë‹ˆë‹¤.

ë¹ ë¥¸ ë³€í™”

<div class="content-ad"></div>

ì´ì œ ê·¸ ì‘ì—…ì„ ë§ˆì³¤ìœ¼ë‹ˆ, d_model=vocab_sizeë¡œ ì„¤ì •í•œ ì ì„ ê¸°ì–µí•´ì£¼ì„¸ìš”. ì™œëƒí•˜ë©´ ìš°ë¦¬ëŠ” ë‹¨ í•˜ë‚˜ì˜ ë ˆì´ì–´ì¸ Embedding ë ˆì´ì–´ë§Œ ê°€ì¡Œê¸° ë•Œë¬¸ì´ì£ .

ì, ì´ì œ Linearë¥¼ ì‚¬ìš©í•˜ì—¬ ì ì ˆí•œ ë§¤í•‘ ë ˆì´ì–´ë¥¼ ê°–ê³  ìˆê¸° ë•Œë¬¸ì— ìš°ë¦¬ëŠ” ì›í•˜ëŠ” ìˆ«ìë¡œ Embedding í¬ê¸°ë¥¼ ë³€ê²½í•˜ê³  ë¬¸ìì˜ ë” ë§ì€ í‘œí˜„ì„ í•™ìŠµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. 512ë¡œ ë§Œë“¤ì–´ë³´ì£ .

```python
# ì„ë² ë”© í¬ê¸°ë¥¼ ì •ì˜í•˜ëŠ” ë° ì‚¬ìš©ë¨
d_model = 512

# embedding ì°¨ì›(d_model)ì„ vocab_sizeë¡œ ë³€í™˜í•˜ëŠ” linear ë ˆì´ì–´ë¥¼ ì¶”ê°€í•˜ëŠ” ê²ƒì„ ìŠì§€ ë§ˆì„¸ìš”
class GPT(nn.Module):
    def __init__(self, vocab_size, d_model):
        ...
        self.linear1 = nn.Linear(d_model, vocab_size)

    def forward(self, inputs, targets = None):
        ...
        logits = self.ln2(logits + adn_logits)
        logits = self.linear1(logits)
        ...
        return logits, loss

    ...
```

ë”± ì´ê²ƒë§Œ ë°”ê¾¸ë©´, ìš°ë¦¬ì˜ GPT-2 Transformer ë””ì½”ë” ì•„í‚¤í…ì²˜ì˜ ì „ì²´ ëª¨ë¸ì„ ì™„ì„±í–ˆìŠµë‹ˆë‹¤.

<div class="content-ad"></div>

ì•„ì§ ëë‚˜ì§€ ì•Šì•˜ì–´ìš”. ëª¨ë¸ì„ ê³„ì† í–¥ìƒì‹œì¼œë´…ì‹œë‹¤..

## c. (ê°€ë¦¬í‚¨) ë‹¤ì¤‘ í—¤ë“œ ì–´í…ì…˜

ì´ë¯¸ ì…€í”„-ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ì˜ ê°•ë ¥í•¨ê³¼ ëª¨ë¸ì´ í…ìŠ¤íŠ¸ ë‚´ì˜ ë¬¸ë§¥ì  ê´€ê³„ë¥¼ ì¼ë°˜í™”í•˜ëŠ” ëŠ¥ë ¥ì„ ì•Œê³  ìˆì„ ìˆ˜ë„ ìˆì–´ìš”. í•˜ì§€ë§Œ ë§Œì•½ì— ëª¨ë¸ì´ í…ìŠ¤íŠ¸ ë‚´ì—ì„œ ë‹¨ì–´ë‚˜ ë¬¸ìë“¤ì´ ì–´ë–»ê²Œ ì„œë¡œ ì—°ê²°ë˜ì–´ ìˆê³  ê·¸ë“¤ì˜ ì‹œê°„ì  ì‚¬ìš© ë“±ê³¼ ê°™ì€ ë‹¤ì–‘í•œ ì–¸ì–´ì  íŠ¹ì„±ì„ ì´í•´í•  ìˆ˜ ìˆëŠ” ë°©ë²•ì´ ìˆë‹¤ë©´ ì–´ë–¨ê¹Œìš”? ììŒê³¼ ëª¨ìŒ ì‚¬ì´ì˜ ì°¨ì´, ê·¸ë¦¬ê³  ì–´ë–¤ ìƒí™©ì—ì„œ ì–´ë–»ê²Œ ì‚¬ìš©í•´ì•¼ í•˜ëŠ”ì§€ë¥¼ í•™ìŠµí•˜ëŠ” ëª¨ë¸ì„ ìƒìƒí•´ë³´ì„¸ìš”.

<div class="content-ad"></div>

í¥ë¯¸ë¡œìš´ ë‚´ìš©ì´ì£ ? Self-Attentionì—ì„œ d_modelì— ëŒ€í•œ ì „ì²´ì ì¸ ì‹œí€€ìŠ¤ ë¬¸ë§¥ì„ ë‚˜íƒ€ë‚´ëŠ” ë™ì•ˆ, ì´ì œ d_modelì„ ì—¬ëŸ¬ ê°œì˜ í—¤ë“œë¡œ ë‚˜ëˆŒ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê° í—¤ë“œëŠ” Query, Key ë° Valueë¥¼ ìœ„í•œ ë³„ë„ì˜ í‘œí˜„ì„ ê°–ê³  ìˆì–´ ëª¨ë¸ì´ ì‹œí€€ìŠ¤ ë‚´ì—ì„œ ì—¬ëŸ¬ ë¬¸ë§¥ì  ë‰˜ì•™ìŠ¤ë¥¼ í•™ìŠµí•  ìˆ˜ ìˆê²Œ ë©ë‹ˆë‹¤. ë‹¤ì¤‘ í—¤ë“œë¥¼ í†µí•©í•˜ì—¬ ìš°ë¦¬ì˜ ì–´í…ì…˜ ë ˆì´ì–´ë¥¼ ê°•í™”í•´ ë´…ì‹œë‹¤.

![ì´ë¯¸ì§€](/assets/img/2024-07-13-BuildingGPT-2withPyTorchPart2_10.png)

ì½”ë“œ:

```python
n_heads = 4  # self-attention í—¤ë“œì˜ ìˆ˜. d_modelë¡œ ë‚˜ëˆŒ ìˆ˜ ìˆì–´ì•¼ í•©ë‹ˆë‹¤

class MultiHeadAttention(nn.Module):
    def __init__(self, d_model: int, n_heads: int):
        super().__init__()

        self.n_heads = n_heads
        self.head_dim = d_model // n_heads

        assert (n_heads * self.head_dim == d_model)

        self.query = nn.Linear(d_model, d_model)
        self.key = nn.Linear(d_model, d_model)
        self.value = nn.Linear(d_model, d_model)
        self.fc_out = nn.Linear(d_model, d_model)
        self.dropout = nn.Dropout(0.2)

    def forward(self, inputs: torch.Tensor):
        B, seq_length, d_model = inputs.shape

        # ì…ë ¥ ì„ë² ë”©ì„ Q, K ë° Vë¡œ í”„ë¡œì ì…˜
        Q = self.query(inputs).view(B, seq_length, self.n_heads, self.head_dim).permute(0, 2, 1, 3)
        K = self.key(inputs).view(B, seq_length, self.n_heads, self.head_dim).permute(0, 2, 1, 3)
        V = self.value(inputs).view(B, seq_length, self.n_heads, self.head_dim).permute(0, 2, 1, 3)

        # ì–´í…ì…˜ ìŠ¤ì½”ì–´ ê³„ì‚°
        attention_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)

        # ë¯¸ë˜ í† í°ì— ëŒ€í•œ ì–´í…ì…˜ ë°©ì§€ë¥¼ ìœ„í•œ ë§ˆìŠ¤í¬ ì ìš©
        mask = torch.triu(torch.ones(seq_length, seq_length), diagonal=1).bool().to(inputs.device)
        attention_scores = attention_scores.masked_fill(mask, float('-inf'))

        attention_weights = torch.softmax(attention_scores, dim=-1)
        # ê°’ë“¤ì˜ ê°€ì¤‘í•© ê³„ì‚°
        attention_output = torch.matmul(self.dropout(attention_weights), V)

        # í—¤ë“œë“¤ì„ ì—°ê²°í•˜ê³  ì›ë˜ í˜•íƒœë¡œ ë˜ëŒë¦¼
        attention_output = attention_output.permute(0, 2, 1, 3).contiguous()
        attention_output = attention_output.view(B, seq_length, d_model)

        # ìµœì¢… ì„ í˜• ë³€í™˜ ì ìš©
        out = self.fc_out(attention_output)

        return out

class GPT(nn.Module):
    def __init__(self, vocab_size, d_model, n_heads):
        super().__init__()
        ...
        # selfattention ë ˆì´ì–´ë¥¼ multiheadattentionìœ¼ë¡œ ëŒ€ì²´
        self.att = MultiHeadAttention(d_model, n_heads)
        ...

m = GPT(vocab_size=vocab_size, d_model=d_model, n_heads=n_heads).to(device)
```

<div class="content-ad"></div>

ì´ì œ, í¸ì•ˆíˆ ì•‰ì•„ ê³„ì†í•´ì„œ ëª¨ë¸ì„ í›ˆë ¨ì‹œí‚¤ê³  ë§ˆë²•ì„ ë³´ì„¸ìš”...

![BuildingGPT-2withPyTorchPart2_11.png](/assets/img/2024-07-13-BuildingGPT-2withPyTorchPart2_11.png)

ì´ì œ ëª¨ë¸ì˜ ì„±ëŠ¥ê³¼ ì¶œë ¥ì—ì„œ ìƒë‹¹í•œ í–¥ìƒì„ ë³¼ ìˆ˜ ìˆì–´ì•¼ í•©ë‹ˆë‹¤. ì´ ëª¨ë“  ê²ƒì€ Multi-Head attention ë•ë¶„ì…ë‹ˆë‹¤. ë¨¸ë¦¬ í¬ê¸°ë¥¼ ì¡°ì ˆí•˜ì—¬ ëª¨ë¸ì´ ë” ë‚˜ì€ í‘œí˜„ì„ ë°°ìš°ëŠ”ì§€ í™•ì¸í•´ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## d. GPT ë””ì½”ë” ë¸”ë¡ë“¤

<div class="content-ad"></div>

ë§ˆìŒì— ë“œì…¨ìœ¼ë©´ ì¢‹ê² ë„¤ìš”!ğŸ”®

í”„ë¡œì íŠ¸ë¥¼ í†µí•´ ì œì‹œëœ ëª¨ë¸ ë‹¤ì´ì–´ê·¸ë¨ì„ ì£¼ì˜ ê¹Šê²Œ ì‚´í´ë³´ì‹œë©´, ì§ì‚¬ê°í˜• ë¸”ë¡ ì•ˆì— ëª‡ ê°œì˜ ë ˆì´ì–´ê°€ ì¶”ê°€ëœ ê²ƒì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ê²ƒë“¤ì„ 'ë””ì½”ë” ë¸”ë¡'ì´ë¼ê³  í•©ë‹ˆë‹¤. í•˜ì§€ë§Œ ì—¬ëŸ¬ ê°œì˜ ì„ í˜• ë„¤íŠ¸ì›Œí¬ ë ˆì´ì–´ë¥¼ ì¶”ê°€í•  ìˆ˜ ìˆëŠ” ê²ƒì²˜ëŸ¼, ì´ ê·¸ë£¹ ëª¨ë¸ì˜ ì—¬ëŸ¬ ë¸”ë¡ë„ ì¶”ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì–´ë–»ê²Œ í• ì§€ ì•Œì•„ë³´ê² ìŠµë‹ˆë‹¤:

ë¨¼ì €, ìš°ë¦¬ì˜ ì–´í…ì…˜, ë ˆì´ì–´ ë…¸ë¦„ ë° í”¼ë“œ í¬ì›Œë“œ ë„¤íŠ¸ì›Œí¬ë¥¼ GPTBlockì´ë¼ëŠ” ë³„ë„ ëª¨ë“ˆë¡œ êº¼ë‚´ë³´ê² ìŠµë‹ˆë‹¤.

```python
class GPTBlock(nn.Module):
    def __init__(self, d_model, n_heads):
        super().__init__()
        self.att = MultiHeadAttention(d_model, n_heads)
        self.ln1 = nn.LayerNorm(d_model)
        self.ln2 = nn.LayerNorm(d_model)
        self.dropout = nn.Dropout(0.2)
        self.fcn = nn.Sequential(
            nn.Linear(d_model, 4 * d_model),
            nn.GELU(),
            nn.Linear(4 * d_model, d_model)
        )

    def forward(self, logits):
        att_logits = self.att(logits)
        adn_logits = self.ln1(logits + att_logits)
        logits = self.dropout(adn_logits)
        logits = self.fcn(logits)
        logits = self.ln2(logits + adn_logits)
        return logits
```

<div class="content-ad"></div>

ì´ì œ ì—¬ê¸°ì„œ ë‚´ë¶€ì˜ ëª¨ë“  ë ˆì´ì–´ë“¤ì„ ë¸”ë¡ìœ¼ë¡œ ëŒ€ì²´í•˜ëŠ” GPT í´ë˜ìŠ¤ë¥¼ ìˆ˜ì •í•´ë´…ì‹œë‹¤. ì¶”ê°€ë¡œ ìƒì„±ì ë§¤ê°œë³€ìˆ˜ n_layerë¥¼ ì‚¬ìš©í•˜ì—¬ ë””ì½”ë” ë¸”ë¡/ë ˆì´ì–´ì˜ ê°œìˆ˜ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.

```python
n_layers = 2 # GPT ë¸”ë¡/ë ˆì´ì–´ ìˆ˜

class GPT(nn.Module):
    def __init__(self, vocab_size, d_model, n_heads, n_layers):
        super().__init__()
        self.wte = nn.Embedding(vocab_size, d_model) # ë‹¨ì–´ í† í° ì„ë² ë”©
        self.wpe = PositionalEncoding(context_length, d_model) # ë‹¨ì–´ ìœ„ì¹˜ ì¸ì½”ë”©
        self.blocks = nn.ModuleList([GPTBlock(d_model, n_heads) for _ in range(n_layers)])
        self.linear1 = nn.Linear(d_model, vocab_size)

    def forward(self, inputs, targets=None):
        logits = self.wte(inputs) # ì°¨ì› -> ë°°ì¹˜ í¬ê¸°, ì‹œí€€ìŠ¤ ê¸¸ì´, d_model
        logits = self.wpe(logits)
        for block in self.blocks:
            logits = block(logits)
        logits = self.linear1(logits)
        ...
        return logits, loss
    ...

m = GPT(vocab_size=vocab_size, d_model=d_model, n_heads=n_heads, n_layers=n_layers).to(device)
```

## e. í† í¬ë‚˜ì´ì € ê°œì„ 

ì´ì œ ì½”ë“œì—ì„œ í•œ ê°€ì§€ ë” ìˆ˜ì •ì„ í•´ì•¼í•©ë‹ˆë‹¤. í† í¬ë‚˜ì´ì €ì…ë‹ˆë‹¤. ë„¤, ëª¨ë¸ì„ ìœ ìš©í•œ ì •ë³´ë¥¼ ë§ì´ ì–»ì„ ìˆ˜ ìˆëŠ”ë°ë„ ë¶ˆêµ¬í•˜ê³  ë§ì€ í† í°ìœ¼ë¡œ ëª¨ë¸ì˜ ë¶€í•˜ë¥¼ ì¦ê°€ì‹œí‚¤ê³  ìˆëŠ” ë¬¸ì ë ˆë²¨ í† í¬ë‚˜ì´ì €ë¥¼ ê°œì„ í•´ë³´ê² ìŠµë‹ˆë‹¤. GPT í† í¬ë‚˜ì´ì €ë¥¼ ìœ„í•œ ê³µì‹ íŒŒì´ì¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ì¸ tiktoken ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ í† í¬ë‚˜ì´ì €ë¥¼ ê°œì„ í•´ë´…ì‹œë‹¤.

<div class="content-ad"></div>

ì´ ë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ” Byte Pair Encoding(BPE) ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ì´ ì•Œê³ ë¦¬ì¦˜ì€ í† í¬ë‚˜ì´ì €ë¥¼ í›ˆë ¨í•  ë•Œ ë‹¨ì–´ë‚˜ ë‹¨ì–´ì˜ ì—¬ëŸ¬ ë¶€ë¶„ì´ ì–¼ë§ˆë‚˜ ìì£¼ ë‚˜íƒ€ë‚˜ëŠ”ì§€ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹¨ì–´ë¥¼ ë³‘í•©í•©ë‹ˆë‹¤.

ì„¤ì¹˜:

```js
pip install tiktoken
```

ì½”ë“œ:

<div class="content-ad"></div>

```python
import tiktoken

tokenizer = tiktoken.get_encoding('gpt2')
vocab_size = tokenizer.n_vocab
```

![Image](/assets/img/2024-07-13-BuildingGPT-2withPyTorchPart2_13.png)

We have now expanded our vocabulary size to 50257, allowing the model to encounter a wide range of words and sequences.

Next, we will encode our data using the updated tokenizer. Let's adjust our data initialization as follows:

<div class="content-ad"></div>

```python
import torch

# ì‹œìŠ¤í…œì— ë”°ë¼ CPU ë˜ëŠ” GPU ì‚¬ìš©
device = "cpu"
if torch.cuda.is_available():
    device = "cuda"

data_dir = "data.txt"
text = open(data_dir, 'r').read() # ëª¨ë“  ë°ì´í„°ë¥¼ ë‹¨ìˆœ ë¬¸ìì—´ë¡œ ë¡œë“œ


# í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ í† í°í™”ëœ í…ì„œë¡œ ë³€í™˜
data = torch.tensor(tokenizer.encode(text), dtype=torch.long, device=device)
```

ê·¸ëŸ° ë‹¤ìŒ, ì´ì „ ì¸ì½”ë”©(encode) ë° ë””ì½”ë”©(decode) ê¸°ëŠ¥ í˜¸ì¶œì„ tokenizer.encode() ë° tokenizer.decode()ë¡œ ë³€ê²½í•˜ì„¸ìš”. ì´ ì¡°ì •ìœ¼ë¡œ ìƒˆë¡œìš´ í† í°í™” ë„êµ¬ì™€ í˜¸í™˜ë˜ë„ë¡ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## f. ìµœì¢… GPT-2 í›ˆë ¨

ğŸ¥³ ë§ˆì¹¨ë‚´ í”„ë¡œì íŠ¸ì˜ ëì— ë‹¤ë‹¤ëê³  ìƒˆë¡œìš´ í•™ìŠµ ê²½í—˜ì„ ì–»ì—ˆìŠµë‹ˆë‹¤. ëª‡ ê°€ì§€ ì¡°ì •ì„ í•´ì„œ ëª¨ë¸ì´ ë” ë¹¨ë¦¬ ë” ì˜ í›ˆë ¨ë˜ë„ë¡ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë©´ ëª¨ë“  ì¤€ë¹„ê°€ ëë‚©ë‹ˆë‹¤.

<div class="content-ad"></div>

ä»¥ä¸‹æ˜¯æ‚¨è¯·æ±‚çš„æ›´æ”¹ï¼š

```js
context_length = 512  # ê° ë°°ì¹˜ì—ì„œ ì²˜ë¦¬ë˜ëŠ” í† í° ìˆ˜
d_model = 512
n_layers = 1  # GPT ë¸”ë¡/ë ˆì´ì–´ ìˆ˜

class GPT(nn.Module):
    def __init__(self, vocab_size, d_model, n_heads, n_layers):
        super().__init__()
        self.wte = nn.Embedding(vocab_size, d_model)  # ë‹¨ì–´ í† í° ì„ë² ë”©
        self.wpe = PositionalEncoding(context_length, d_model)  # ë‹¨ì–´ ìœ„ì¹˜ ì¸ì½”ë”©
        self.blocks = nn.ModuleList([GPTBlock(d_model, n_heads) for _ in range(n_layers)])
        self.linear1 = nn.Linear(d_model, vocab_size)

        # ë§¤ê°œë³€ìˆ˜ ê³µìœ 
        self.wte.weight = self.linear1.weight
```

GPT-2ì—ì„œ ë§¤ê°œë³€ìˆ˜ ê³µìœ ì— ëŒ€í•´ ë” ìì„¸íˆ ì•Œì•„ë³´ë ¤ë©´ ì—¬ê¸°ë¥¼ í™•ì¸í•˜ì‹­ì‹œì˜¤.

í˜„ì¬ ëª¨ë¸ êµ¬ì¡°ë¥¼ ì‹œê°í™”í•˜ë ¤ë©´ ëª¨ë¸ ë³€ìˆ˜ ìì²´ë¥¼ ì¶œë ¥í•´ì£¼ì„¸ìš”:

<div class="content-ad"></div>

ê·¸ë¦¬ê³  ì´ë ‡ê²Œ í•´ì„œ ìš°ë¦¬ë§Œì˜ 29M GPT-2 ëª¨ë¸ì„ ë§Œë“¤ì—ˆìŠµë‹ˆë‹¤. ì´ê±´ ìš°ë¦¬ì˜ ì‚¬ìš© ëª©ì ì— ì¶©ë¶„í•  ê±°ì—ìš”.

ëª¨ë¸ì„ í›ˆë ¨í•˜ê¸° ì „ì— torch.compileì„ ì‚¬ìš©í•´ ëª¨ë¸ì„ ì»´íŒŒì¼í•´ì•¼ í•´ìš”. ì´ë ‡ê²Œ í•˜ë©´ ëª¨ë¸ ë‚´ì—ì„œ ë°œìƒí•˜ëŠ” ê±°ì˜ ëª¨ë“  í–‰ë ¬ ê³±ì…ˆê³¼ ë‹¤ë¥¸ ì‘ì—…ë“¤ì´ ë¯¸ë¦¬ ë§¤í•‘ë˜ì–´ìš”. ê°„ë‹¨íˆ ë§í•´ì„œ, ëª¨ë¸ì€ ê° ì—°ì‚°ì„ í•œ ì¤„ì”© ë˜ëŠ” ë ˆì´ì–´ë³„ë¡œ ì§„í–‰í•˜ëŠ” ëŒ€ì‹  ëª¨ë“  ì‘ì—…ì„ í•©ì³ ìµœì¢… ë‹¨ê³„ë¥¼ ì§ì ‘ ê³„ì‚°í•  ìˆ˜ ìˆì–´ìš”.

<div class="content-ad"></div>

ì œê°€ í•™ìŠµë¥ ê³¼ í›ˆë ¨ ë£¨í”„ë¥¼ ì•„ë˜ì™€ ê°™ì´ ìˆ˜ì •í–ˆìŠµë‹ˆë‹¤:

```js
lr = 1e-3;
optim = torch.optim.AdamW(m.parameters(), (lr = lr), (weight_decay = 0.1));
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optim, (T_max = 3000), (eta_min = lr * 0.1));
```

```js
epochs = 3500
eval_steps = 100  # n ìŠ¤í…ë§ˆë‹¤ í‰ê°€ ìˆ˜í–‰

# ì†ì‹¤ê°’ ì €ì¥
train_loss = {}

# ëª¨ë¸ í›ˆë ¨
for e in range(epochs):
    xb, yb = train_loader.get_batch()

    logits, loss = m(xb, yb)
    optim.zero_grad(set_to_none=True)
    loss.backward()

    # ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘
    torch.nn.utils.clip_grad_norm_(m.parameters(), max_norm=1)

    optim.step()
    scheduler.step()
    train_loss[e] = loss.item()

    if e % eval_steps == 0 or e == epochs - 1:
        m.eval()
        with torch.no_grad():
            xvb, yvb = eval_loader.get_batch()
            _, e_loss = m(xvb, yvb)

        print(f"Epoch: {e+ep}\ttrain_loss: {loss:.4f}\teval_loss: {e_loss:.4f}")

        m.train()  # í›ˆë ¨ ëª¨ë“œë¡œ ë³€ê²½
```

ëª¨ë¸ í¬ê¸°ê°€ ë§¤ìš° ì»¤ì ¸ì„œ ì´ì œ GoogleColabì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ í›ˆë ¨í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì´ ë§í¬ë¥¼ í´ë¦­í•˜ì—¬ Colabì—ì„œ ì—´ì–´ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

<div class="content-ad"></div>

After training for approximately 3500 epochs, here is the training loss curve I obtained:

![Training Loss Curve](/assets/img/2024-07-13-BuildingGPT-2withPyTorchPart2_15.png)

And here is a snippet of a song generated by the model ğŸ¶ğŸ¶:

![Song Output](/assets/img/2024-07-13-BuildingGPT-2withPyTorchPart2_16.png)

<div class="content-ad"></div>

ê·¸ê³³ì— ìˆì—ˆìŠµë‹ˆë‹¤. ì‚¬ìš©ì ë°ì´í„°ë¡œ ì‚¬ìš©ì ì •ì˜ GPT-2 ëª¨ë¸ì„ êµ¬ì¶•í•˜ê³  í›ˆë ¨í•˜ëŠ” ë‹¨ê³„ë³„ ì•ˆë‚´ì„œì…ë‹ˆë‹¤. ë°ì´í„° ìš”êµ¬ ì‚¬í•­ì— ë”°ë¼ í•˜ì´í¼íŒŒë¼ë¯¸í„° ë° ë ˆì´ì–´ë¥¼ ìˆ˜ì •í•´ ë³´ì„¸ìš”.

ì´ í”„ë¡œì íŠ¸ëŠ” ì—¬ê¸°ê¹Œì§€ì…ë‹ˆë‹¤. í•˜ì§€ë§Œ ì—¬ê¸°ì„œ ë©ˆì¶”ì§€ ì•Šì„ ê±°ì˜ˆìš”. ëª¨ë¸ ì„±ëŠ¥ í–¥ìƒê³¼ í›ˆë ¨ì— ê´€í•œ ëª‡ ê°€ì§€ ìƒˆ ê¸€ì„ ì¤€ë¹„ ì¤‘ì´ì—ìš”. ê³„ì† ê¸°ëŒ€í•´ ì£¼ì„¸ìš”. ì¦ê±°ìš´ í•™ìŠµë˜ê¸¸ :).
