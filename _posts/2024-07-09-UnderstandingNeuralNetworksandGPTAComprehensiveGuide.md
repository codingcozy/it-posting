---
title: "신경망과 GPT 이해하기 종합 가이드"
description: ""
coverImage: "/assets/img/2024-07-09-UnderstandingNeuralNetworksandGPTAComprehensiveGuide_0.png"
date: 2024-07-09 10:13
ogImage:
  url: /assets/img/2024-07-09-UnderstandingNeuralNetworksandGPTAComprehensiveGuide_0.png
tag: Tech
originalTitle: "Understanding Neural Networks and GPT: A Comprehensive Guide"
link: "https://medium.com/@sergeahead29/understanding-neural-networks-and-gpt-a-comprehensive-guide-a309734d4409"
isUpdated: true
---

Neural Network란 무엇인가요?

Neural Network은 인간의 두뇌에서 영감을 받은 계산 모델입니다. 이미지를 인식하거나 언어를 이해하는 등의 작업을 수행하기 위해 숫자를 처리합니다. 예를 들어, 이미지를 처리할 때 각 픽셀은 숫자로 표현됩니다: 검정색 픽셀은 0이고, 흰색 픽셀은 28x28 픽셀 그리드 내에서 1입니다. 이 그리드는 0에서 9까지의 숫자를 표현할 수 있습니다.

![Understanding Neural Networks and GPT: A Comprehensive Guide](/assets/img/2024-07-09-UnderstandingNeuralNetworksandGPTAComprehensiveGuide_0.png)

Neural Network의 변형
Neural Network의 종류는 다양하며, 각각이 이미지 인식, 자연어 처리 등의 특정 작업을 위해 설계되어 있습니다.

<div class="content-ad"></div>

숨겨진 층들
신경망은 실제 연산이 이루어지는 숨겨진 층을 포함한 여러 층으로 구성됩니다. 예를 들어, 간단한 신경망은 각각 16개의 뉴런을 가진 두 개의 숨겨진 층을 가질 수 있습니다. 한 층의 뉴런 활성화는 다음 층의 뉴런 활성화를 결정하며, 이를 통해 네트워크가 복잡한 패턴을 학습할 수 있습니다.

![Understanding Neural Networks and GPT: A Comprehensive Guide](/assets/img/2024-07-09-UnderstandingNeuralNetworksandGPTAComprehensiveGuide_1.png)

# GPT 이해하기 (생성적 사전 훈련 변환기)

생성적
GPT는 학습한 패턴을 기반으로 새로운 텍스트를 생성할 수 있습니다.

<div class="content-ad"></div>

미리 훈련된 모델은 광범위한 데이터셋에서 훈련되었으며, "미리 훈련된"이란 특정 작업을 위해 추가로 훈련하거나 세밀하게 조정할 수 있다는 뜻입니다.

트랜스포머는 GPT가 텍스트 생성, 텍스트 음성 변환 및 텍스트 이미지 변환과 같은 다양한 작업을 수행할 수 있는 핵심 기술입니다.

단어 예측 예시
GPT와 같은 모델은 문장에서 다음 단어를 예측합니다. 단어의 분포를 이해하고 정확하고 문맥적으로 관련된 예측을 하는 데 뛰어납니다.

트랜스포머를 통한 데이터 흐름

<div class="content-ad"></div>

1. 암호 해독하기: 토큰화

새로운 언어를 읽고 있는 상상을 해보세요. 이해하기 위해 문장을 개별 단어로 분해할 수 있습니다. 그것이 바로 기계 학습에서 하는 일인 토큰화입니다. 복잡한 데이터인 텍스트, 이미지 또는 소리를 가져와서 이를 토큰(작은 조각)으로 분해합니다. 이 토큰들은 기계가 이해하고 처리할 수 있는 구성 요소가 됩니다.

2. 단어에서 숫자로: 기계 언어 구사하기

안타깝게도, 컴퓨터는 단어나 그림을 직접 이해하지 못합니다. 그들은 수의 언어에 의존합니다. 여기서 숫자로 변환하는 작업이 필요합니다. 토큰화 후 각 토큰(단어, 픽셀 등)은 기계가 처리할 수 있는 고유 코드로 변환됩니다. 이를 새로운 언어를 위한 암호 해독책을 만든다고 생각해보세요. 각 단어는 특정 숫자로 할당되어 기계가 정보를 이해할 수 있게 됩니다.

<div class="content-ad"></div>

3. 실수로부터 배우기: 역전파

우리가 실수로부터 배우듯이, 기계도 역전파라는 기술을 사용하여 개선합니다. 이 과정은 기계 학습 모델이 얼마나 잘 수행되고 있는지에 기반하여 내부 설정을 조정하도록 도와줍니다. 모델이 잘못된 예측을 하면 역전파가 오차를 분석하고 접근 방식을 조정하는 데 도움을 줍니다. 반복적인 훈련과 역전파를 통해 시간이 지남에 따라 모델은 데이터의 패턴을 이해하고 정확한 예측을 하는 데 능숙해집니다.

4. 기능의 힘: 복잡한 패턴 해제하기

기계 학습 모델은 놀랍도록 복잡한 패턴을 배울 수 있는 수퍼 학생들과 같습니다. 이를 달성하기 위해 수학적 함수의 조합을 사용합니다. 이러한 함수는 데이터 내의 관계와 연결을 분석하는 데 도움이 되는 강력한 도구 역할을 합니다. 모델이 더 많은 정보를 처리함에 따라, 이러한 함수를 사용하여 패턴을 식별하고 정보를 이해하는 데 능숙해집니다.

<div class="content-ad"></div>

5. 데이터 테트리스: 성공을 위한 조직

기계가 마법을 부리려면 그들이 배우는 데이터는 특정한 방식으로 조직화되어 있어야 합니다. 이 형식을 텐서라고 부르는데, 이것은 기본적으로 숫자들의 다차원 배열입니다. 그것을 생각해보면 복잡한 퍼즐 조각들을 모두 모아두고 나서 그것을 함께 맞추기 전에 정리하는 것과 비슷합니다. 텐서는 기계가 필요한 모든 정보를 한 번에 효율적으로 처리할 수 있게 해주어 학습 과정을 더 원할하고 효율적으로 만듭니다.

단어 예측 예시
단어 예측 모델에서는 각 단어의 확률을 텐서나 배열로 표시합니다.

![이미지](/assets/img/2024-07-09-UnderstandingNeuralNetworksandGPTAComprehensiveGuide_2.png)

<div class="content-ad"></div>

GPT 안의 가중치

기계 학습에서의 가중치는 마치 제빵사의 경험과 같습니다. 다양한 재료(특성)와 그 양(값)을 가진 레시피를 상상해보세요. 가중치는 최종 제품(예측)에 각 재료의 중요성을 결정합니다. 경험 많은 제빵사(훈련된 모델)는 과거 결과를 바탕으로 가중치(재료 양)를 조절하여 완벽한 케이크(정확한 예측)를 만들어냅니다. 이러한 가중치는 훈련 중에 계속해서 개선되어, 모델이 원하는 결과에 대한 최적의 특성 균형을 학습하게 됩니다.

GPT3는 1750억 개의 매개변수로 구성되어 있으며, 96개의 레이어로 구성되어 있습니다. 각 레이어는 여러 행렬로 이루어져 있습니다.

모든 계산은 행렬 곱셈을 포함합니다.

<div class="content-ad"></div>

돌아가서 모델로

5만 개의 가중치가 있다고 가정해 봅시다. 초기 임베딩은 각 단어에 대한 열을 가질 것입니다.

단어 - 벡터 (임베딩)

"왕"과 "여왕" 같은 단어는 "남자"와 "여자"에 대한 벡터 방향과 유사합니다. 이는 다음 공식으로 나타낼 수 있습니다:

여왕 ≈ 왕 - 남자 + 여자

이는 "여왕"에 대한 벡터는 "왕"에 대한 벡터를 취한 후 "남자"에 대한 벡터를 뺀 다음 "여자"에 대한 벡터를 더한 것을 의미합니다.

비슷하게,

E(초밥) + (E(독일) - E(일본)) ≈ E(브랏바르스트)

벡터간의 내적은 유사한 단어에 대해서는 양수가 되고, 관련 없는 단어에 대해서는 0이 되며, 반대의 경우에는 음수가 될 수 있습니다.

컨텍스트 크기
컨텍스트 크기는 모델이 예측을 위해 살펴보는 텍스트의 양을 결정합니다.

![](/assets/img/2024-07-09-UnderstandingNeuralNetworksandGPTAComprehensiveGuide_3.png)

<div class="content-ad"></div>

주의 메커니즘

주의란 무엇인가요?
주의는 모델이 입력 텍스트의 관련 부분에 집중할 수 있게 도와줍니다. 예를 들어, 단어 "두더지"는 동물이나 피부의 특징과 같이 다양한 맥락에서 다른 의미로 사용될 수 있습니다. 주의는 맥락을 기반으로 임베딩을 조정하는 데 도움을 줍니다.

예시: 주의의 작용
만약 텍스트가 "탑"에 관한 것이라면, 벡터는 이를 반영하도록 업데이트됩니다. "소형 에펠탑"이라면 벡터가 다시 업데이트됩니다.

헤드와 멀티헤드 주의
여러 개의 주의 헤드를 통해 모델은 동시에 입력의 다른 부분에 집중할 수 있습니다.

<div class="content-ad"></div>

쿼리, 키, 및 값
쿼리 벡터: 관계에 관한 질문을 합니다 (예: 명사 앞에 형용사가 있는가?).
키: 질문에 대한 답변을 제공합니다.
값: 쿼리와 키에 기반하여 임베딩을 업데이트하는 데 사용됩니다.

주의 메커니즘은 다음과 같은 함수를 사용합니다:

주의(Q, K, V) = 소프트맥스(QK^T / sqrt(dk)) V

이 함수는 모델이 예측된 단어에 확률을 할당하고 문맥에 따라 조정하는 데 도움을 줍니다.

![Understanding Neural Networks and GPT: A Comprehensive Guide](/assets/img/2024-07-09-UnderstandingNeuralNetworksandGPTAComprehensiveGuide_4.png)

자기 주의와 교차 주의

- 자기 주의: 모델이 동일한 입력의 다른 부분에 초점을 맞춥니다.
- 교차 주의: 모델이 텍스트와 이미지 사이와 같이 서로 다른 입력에 초점을 맞춥니다.

<div class="content-ad"></div>

## 결론

신경망 및 GPT와 같은 트랜스포머는 인공지능 분야에서 강력한 도구로, 자연어 처리, 이미지 인식 등에서 놀라운 성과를 이루도록 도와줍니다. 인간과 유사한 텍스트를 이해하고 생성하는 능력은 주변의 관련 정보에 집중하도록 도와주는 주의 메커니즘과 같은 복잡한 메커니즘에 의해 이루어집니다.
