---
title: "AI는 우리의 눈을 필요로 한다 2024년 최신 AI 비전 기술 5가지"
description: ""
coverImage: "/assets/img/2024-07-02-AINeedsOurEye_0.png"
date: 2024-07-02 22:58
ogImage:
  url: /assets/img/2024-07-02-AINeedsOurEye_0.png
tag: Tech
originalTitle: "AI Needs Our Eye"
link: "https://medium.com/brain-labs/ai-needs-our-eye-581c699bdfa6"
isUpdated: true
---

## 우리 스스로 생각하는 법을 잊지 말자

![Image](/assets/img/2024-07-02-AINeedsOurEye_0.png)

인공지능(AI)이 우리의 귀중한 지적 성취물에 따라잡으며, 우리는 불안해하고 있습니다. AI 대화 봇은 변호사 자격시험과 의료 면허 시험을 통과할 수 있으며, 기사를 쓸 수 있으며(이 기사는 제외!), 주문에 맞게 이미지를 생성하고 회의록부터 과학 논문까지 텍스트를 요약합니다. 패배론자들은 일자리와 인간의 목적을 잃는 것 뿐만 아니라 인류 자체의 파멸을 두려웁니다. 한번 기계들이 자존 보존을 목표로 배우면, 아마도 그들은 우리의 통제에서 벗어나 완전히 우리가 그것들을 끄는 것을 방해할 수도 있습니다.

걱정하지 마세요, 멸망의 의심자들은 말합니다, 오늘날의 AI는 실제로 생각하지 않습니다 — 강력한 대화 봇은 단어를 추측하느라 쿼리를 기반으로 다음 단어를 맞추고 답변을 점진적으로 작성합니다. 내일의 AI는 이를 더 잘 수행할 수 있겠지만, 본질적으로 다르지는 않을 것입니다. 당신의 마음을 읽을 수 있더라도, 그것은 입력을 반응적으로 처리하기만 할 것입니다.

<div class="content-ad"></div>

그래, 불에 손대는 거야 아니다 아무것도 없는 거야? 아마 둘 다일 것이다. HAL 9000처럼 감성적이고 살인적인 인공 지능을 만들 수 있는 여부는 의문이지만, 생존을 소중히 여기는 인간들이 AI를 통해 스스로의 멸종을 일으키지는 않을 것이라고 합리적으로 가정해 보겠습니다. 하지만 우리가 컴퓨터로부터 스스로를 지킬 수는 있어도, 우리로부터 안전한 것일까요?

파멸론자들은 AI 능력이 급격하게 발전하는 속도를 충분히 과소평가해서는 안 된다고 경고하는 데 옳은 점이 있습니다. 네, 대형 언어 모델(Large Language Models, LLMs)을 지탱하는 원칙들은 간단할 수 있지만, 인간의 사고 역시 뉴런 신호의 생물학적 본질로 축소되면 단순합니다. 인식은 다양한 협의체의 구성원들 간에 대규모 뉴런의 상호 통신으로부터 발생합니다. 마찬가지로, LLMs는 수많은 텍스트 구성 블록(일명 "토큰"이라고 함)이 고차원 공간에 배열돼, 단어들 간의 의미와 관계를 포착하기 위해 조직되어 있습니다. 인간들과 LLMs 모두에게 이 고차원적 구성은 유연하며 학습을 통해 진화합니다. 누적된 인간 지식의 상당 부분에 훈련된 LLM을 기반으로 한 챗봇들은 인간과 유사한 추론 능력과 맥락 인식을 명백히 나타냅니다. 그렇지만, 그들이 실제로 추론을 하는 것은 아니어도, 추론하는 것처럼 만드는데 훌륭한 시연을 합니다. 아마 가장 솔직한 사실은, 그들은 창의성을 나타낼 수 있다는 점입니다. 기존 것을 종합하는 것이 아니라 새로운 아이디어를 발명하는 능력은 지능의 표본이라고 할 수 있습니다. 일상 물건들에 대한 대체 사용법을 고안하는 능력을 테스트할 때, 챗봇들은 대부분의 인간들보다 뛰어납니다. 최근 AI는 계산 기하학의 오래된 문제에 대해 수학자보다 나은 해결책을 찾아냈습니다.

그러나 모든 질문에 답을 가지고 있다고 해서 똑똑하다는 것은 아닙니다, 그저 편리할 뿐입니다. 인간을 괴롭히는 문제들, 충돌, 질병, 서식지 파괴는 쉽게 답이 나오는 것이 아닙니다. 어려운 문제들은 종종 중요한 피해를 막을 수 없을 정도로 인식되지 않습니다. AI는 가르쳐진 것만 학습할 수 있으며, 인간들의 다양한 문제 영역으로 들어가지 못합니다. 어린이들은 위험, 동료 및 낯선 어른들의 혼란스런 세계에서 나아가면서 무엇을 배워야 할지를 배우는데, 그래서 어린이들은 AI보다 오히려 영리하고 더 위험할 수 있습니다. ChatGPT는 절대 유쾌함으로 화를 내거나 땅에 똥을 눌러놓지 않을 것입니다.

AI는 특정 유형의 작업을 처리하기에 가장 적합하며, — LLM을 제외하고는 — 일반적으로 특정 유형의 작업을 처리하도록 개발됩니다. 모든 AI는 이미 어려운 문제로 인간 창조자들에게 인정된 문제군으로 인한 존재를 갚습니다. "딥러닝"은 복잡한 입력에서 패턴을 감지하고 특징을 추출하기 위해 뇌와 유사한 다층 구조를 사용합니다. 이것은 음성을 인식하고 언어 간에 번역하며, 사진에서 고양이를 찾고, 질병을 진단하는 다양한 응용 분야로 이어졌습니다. 이 모든 것은 물건을 분류하기 위해 제2차 세계 대전 중에 나타난 "퍼셉트론"이라는 공통 조상 패러다임에 기인합니다. 초기때부터 몇십 년에 걸쳐, 누구도 퍼셉트론의 학습 능력을 그것이 분류할 수 있는 물건들을 이해하는 능력으로 혼동하지 않았습니다. 오늘날의 신경망은 단일 뉴런 퍼셉트론의 극도로 복잡한 후속자로서, 매우 섬세한 패턴을 감지할 수 있습니다 — 때로는 인간의 인식을 속일 정도로 미묘한 패턴까지 감지할 정도이며, 컴퓨터가 인간 전문가와 경쟁하거나 그들을 앞지르도록 만들 수 있습니다. AI가 전문 기술과 지식을 지닌 사람들에게 맡겨진 작업들을 수행함에 따라 살아있는 "기계 속의 유령" 분위기를 뿜어냅니다.

<div class="content-ad"></div>

AI 관련 전문가가 아니라 AI는 전문가 수준의 패턴을 인식할 뿐입니다. 저는 의료 이미지를 분석하여 암의 징후를 찾는 일을 하고 있습니다. 아내는 미술 역사가이며, 우리는 그림과 드로잉에서 진품과 작가를 확인하기 위해 AI를 활용합니다. 두 분야는 비슷한 AI 설계와 이미지 전처리 전략을 사용합니다. AI는 관련 도메인에 대해 아무것도 "알지" 못합니다. AI가 암과 양성을 구분하거나 렘브란트와 위조 작품을 구별하는 방법에 대해 우리도 알지 못합니다. AI가 판단의 근거인 이미지 영역을 식별할 수는 있지만 판단의 근거는 알 수 없습니다. "설명 가능한" AI를 개발하려는 노력은 AI 모델의 복잡성으로 인해 실패했습니다.

AI는 우리가 볼 수 없는 패턴을 인식하는 데 매우 뛰어나지만, 세계나 자신의 특정 분야에 대해 아무것도 모르기 때문에 보는 대로 스마트하지 않습니다. AI는 자신의 결정 근거를 제공하기에 너무 어리석습니다. 결과적으로, 우리의 AI에 대한 신뢰는 거의 완전히 성공적인 기록에서 파생됩니다. 이러한 능력은 인간의 전문성이 퇴화되는 것("de-skilling")과 AI 지원에 과도하게 의존하는 것("자동화 안일함")으로 이어질 수 있습니다. 의료 분야에서 의사들은 의심스러운 AI 결과물에 직면하여 이를 수정해야 할 임상 지식을 잃을 수도 있고, 자동화 안일함으로 인해 심지어 오류에 주목하지 못할 수도 있습니다. 어떤 AI 시스템도 완벽하지 않습니다. 오류 가능성은 항상 존재하며, 이로 인해 과도한 의존의 가능성도 있습니다.

역대 미국 대통령 변호사인 마이클 코헨은 감호 기간을 단축하려는 노력 중에 자신의 주장을 뒷받침하는 사례를 찾아 변호사들에게 보냈습니다. 그러나 그 사례들이 실제로 존재하지 않았음을 발견한 판사는 분노에 휩싸였습니다. 코헨은 Google의 Bard 챗봇을 통해 그 사례를 만들어 냈습니다. 결과적으로, 두 양 층의 변호사들이 사실을 확인하는 가장 기본적인 변호사의 책임을 다하지 않고 악명 높은 믿을 수 없는 AI의 결과를 반복했습니다. 우리는 컴퓨터보다 더 현명해야 하며, 그들이 우리를 바보로 만들거나 우리를 게을리하게 만들지 않아야 합니다. 의료 AI 시스템은 사실을 만들어 내지는 않지만 오류를 만들어 낼 수 있습니다; 그러므로 우리는 의사 결정을 위한 도구로 사용해야 하며, 의사 결정 자체를 AI에게 맡겨서는 안 됩니다. AI는 완벽하지 않기 때문에 미술품 거래상과 박물관은 여전히 연원을 조사하고, 탄소 데이트 및 피그먼트 분석과 같은 보다 일상적인 과학 도구를 사용한 후에야 AI에 의존할 수 있습니다. AI에 대한 대규모 직업 상실 두려움은 현실적으로 과장될 것이며(일부 주의가 필요하지만), 제대로 활용될 경우 생산성이 증가하고 인적 실수가 줄어들 가능성이 더 높습니다. 특히 건강과 안전이 관련된 경우, 사람들은 루프 안에 머물러 있어야 합니다.

만약 AI를 정말 이해하지 못한다면, 낮은 수준의 구성은 완전히 이해하지만 특정한 고수준 출력의 근거를 모른다면 우리는 인간의 능력을 버리지 말아야 합니다. 경험과 도메인 이해에 기반한 인간의 능력을 통해 스스로 생각하고 더 나아가야 합니다. 물론, 우리가 복잡한 챗봇을 이해하지 못한다면, 그들이 어떤 면에서 우리처럼 조금이라도 사고하는지 확신할 수 없을 것입니다. AI가 성급한 결론을 내릴 것이 아니라고 이전에 말했지만, 챗봇이 엿장수가 되는 다른 예시에서, 변호사가 스스로 작업한 사례의 사실을 잘못 표현하고 있는 챗봇을 발견했습니다. 이 변호사는 기자에게 다음과 같이 말했습니다: "그것에게 '넌 틀렸어. 나는 이 사례를 주장했어.'라고 했고, AI는 '넌 여기 앉아서 네가 일한 사례에 대해 자랑할 수 있지만, 나는 맞아. 직접 증명이야.' 하고 URL 주소를 준 다음 아무 것도 안 내놨습니다." 파괴된 상황을 조망하는 분노한 부모처럼 들리는 변호사는 "그것은 조금 사이코패스"라고 덧붙였습니다.

<div class="content-ad"></div>

![Tarot Card](/assets/img/2024-07-02-AINeedsOurEye_1.png)
