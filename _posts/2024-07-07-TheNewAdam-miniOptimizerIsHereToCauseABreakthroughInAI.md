---
title: "새로운 Adam-mini 옵티마이저, AI에 혁신을 가져올 최신 기술"
description: ""
coverImage: "/code-tower.github.io/assets/no-image.jpg"
date: 2024-07-07 14:09
ogImage:
  url: /code-tower.github.io/assets/no-image.jpg
tag: Tech
originalTitle: "The New ‘Adam-mini’ Optimizer Is Here To Cause A Breakthrough In AI"
link: "https://medium.com/gitconnected/the-new-adam-mini-optimizer-is-here-to-cause-a-breakthrough-in-ai-6b0ba252ae36"
isUpdated: true
---

Most modern neural networks are backed by an Optimizer for their training. The Adam Optimizer, introduced in 2017, has emerged as the preferred choice for training Large Language Models (LLMs) in the industry today, thanks to its variations. However, despite its remarkable performance, one aspect has largely gone unnoticed - Memory inefficiency.

<div class="content-ad"></div>

7 억 개의 파라미터를 가진 LLM을 훈련시키려면 Adam은 약 86GB의 메모리가 필요합니다.

Google PaLM과 같은 모델의 경우 5400억 개의 파라미터로 Adam 그 자체를 포함할 수 있도록 50개 이상의 GPU가 필요합니다.

하지만 이제는 그렇지 않을 수도 있습니다. 흥미로운 소식이 있어요!

ML 연구팀이 Adam의 개선된 버전인 Adam-mini를 개발했습니다.

<div class="content-ad"></div>

아담 최적화 기법을 사용하여 10억 개 파라미터를 가진 LLM(언어 모델)을 훈련할 때, Adam-mini 최적화 기법은 메모리 사용 효율이 2배 높아지며 성능도 49.6% 향상된다고 알려져 있습니다.
