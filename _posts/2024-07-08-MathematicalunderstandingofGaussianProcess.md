---
title: "ê°€ìš°ì‹œì•ˆ í”„ë¡œì„¸ìŠ¤ì˜ ìˆ˜í•™ì  ì´í•´"
description: ""
coverImage: "/assets/img/2024-07-08-MathematicalunderstandingofGaussianProcess_0.png"
date: 2024-07-08 00:08
ogImage:
  url: /assets/img/2024-07-08-MathematicalunderstandingofGaussianProcess_0.png
tag: Tech
originalTitle: "Mathematical understanding of Gaussian Process"
link: "https://medium.com/the-quantastic-journal/mathematical-understanding-of-gaussian-process-eaffc9c8a6d6"
isUpdated: true
---

## ê¸°ê³„ í•™ìŠµê³¼ ìˆ˜í•™

![Mathematical understanding of Gaussian Process](/assets/img/2024-07-08-MathematicalunderstandingofGaussianProcess_0.png)

ê°€ìš°ì‹œì•ˆ í”„ë¡œì„¸ìŠ¤ëŠ” ë°ì´í„°ê°€ ì ì„ ë•Œ íŠ¹íˆ ìœ ìš©í•©ë‹ˆë‹¤. ì €ëŠ” ì œì¡° ì‚°ì—…ì—ì„œ ë°ì´í„° ê³¼í•™ìë¡œ ì¼í•  ë•Œ, ìš°ë¦¬ íŒ€ì€ ë‹¤ìŒì— ì–´ë–¤ ì‹¤í—˜ ì¡°ê±´ì„ ì‹œí–‰í•´ì•¼ í•˜ëŠ”ì§€ ë°íˆê¸° ìœ„í•´ ì´ ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì´ ì•Œê³ ë¦¬ì¦˜ì€ ë‹¤ë¥¸ ì•Œê³ ë¦¬ì¦˜ì— ë¹„í•´ ëœ ì¸ê¸°ê°€ ìˆìŠµë‹ˆë‹¤. ì´ ë¸”ë¡œê·¸ì—ì„œëŠ” ì‹œê°í™”ì™€ íŒŒì´ì¬ êµ¬í˜„ì„ í†µí•´ ê°€ìš°ì‹œì•ˆ í”„ë¡œì„¸ìŠ¤ì˜ ìˆ˜í•™ì  ë°°ê²½ì„ ì„¤ëª…í•˜ê² ìŠµë‹ˆë‹¤.

## ëª©ì°¨

<div class="content-ad"></div>

## 1. ì²˜ìŒ ë‹¨ê³„: ë‹¤ë³€ëŸ‰ ê°€ìš°ì‹œì•ˆ ë¶„í¬

ë‹¤ë³€ëŸ‰ ê°€ìš°ì‹œì•ˆ ë¶„í¬ëŠ” ê°€ìš°ì‹œì•ˆ í”„ë¡œì„¸ìŠ¤ë¥¼ ì´í•´í•˜ê¸° ìœ„í•œ í•„ìˆ˜ì ì¸ ê°œë… ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤. ë¹ ë¥´ê²Œ ë³µìŠµí•´ë³´ê² ìŠµë‹ˆë‹¤. ë§Œì•½ ë‹¤ë³€ëŸ‰ ê°€ìš°ì‹œì•ˆ ë¶„í¬ì— ìµìˆ™í•˜ë‹¤ë©´ ì´ ì„¹ì…˜ì„ ê±´ë„ˆë›°ì–´ë„ ë©ë‹ˆë‹¤.

ë‹¤ë³€ëŸ‰ ê°€ìš°ì‹œì•ˆ ë¶„í¬ëŠ” ë‘ ê°œ ì´ìƒì˜ ì°¨ì›ì„ ê°–ëŠ” ê°€ìš°ì‹œì•ˆ ë¶„í¬ì˜ ê²°í•© í™•ë¥ ì…ë‹ˆë‹¤. ë‹¤ë³€ëŸ‰ ê°€ìš°ì‹œì•ˆ ë¶„í¬ì˜ í™•ë¥  ë°€ë„ í•¨ìˆ˜ëŠ” ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.

<div class="content-ad"></div>

![Image](/assets/img/2024-07-08-MathematicalunderstandingofGaussianProcess_1.png)

xì€ D Ã— 1 ì°¨ì›ì„ ê°€ì§„ ì…ë ¥ ë°ì´í„°ì´ë©°, Î¼ëŠ” xì™€ ê°™ì€ ì°¨ì›ì„ ê°€ì§„ í‰ê·  ë²¡í„°ì´ë©°, Î£ëŠ” D Ã— D ì°¨ì›ì„ ê°€ì§„ ê³µë¶„ì‚° í–‰ë ¬ì…ë‹ˆë‹¤.

ë‹¤ë³€ëŸ‰ ê°€ìš°ì‹œì•ˆ ë¶„í¬ëŠ” ì•„ë˜ ì¤‘ìš”í•œ íŠ¹ì§•ì„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤.

- ë‹¤ë³€ëŸ‰ ê°€ìš°ì‹œì•ˆ ë¶„í¬ì˜ ì£¼ë³€ ë¶„í¬ ë˜í•œ ê°€ìš°ì‹œì•ˆ ë¶„í¬ë¥¼ ë”°ë¦„
- ë‹¤ë³€ëŸ‰ ê°€ìš°ì‹œì•ˆ ë¶„í¬ì˜ ì¡°ê±´ë¶€ ë¶„í¬ ë˜í•œ ê°€ìš°ì‹œì•ˆ ë¶„í¬ë¥¼ ë”°ë¦„

<div class="content-ad"></div>

Now, let's delve into visualizing those ideas. Let's assume our data in D dimensions follows a Gaussian distribution.

For feature 1, if we split the D-dimensional input data into the first L dimensions and the remaining D-L=M dimensions, we can represent the Gaussian distribution like this:

![Gaussian Distribution](/assets/img/2024-07-08-MathematicalunderstandingofGaussianProcess_2.png)

Next, when we integrate out the distribution concerning xâ‚‚, we can express the probability distribution of xâ‚ as:

<div class="content-ad"></div>

ì´ ê³µì‹ (1)ì„ ê¸°ë°˜ìœ¼ë¡œ í•  ë•Œ, ë‹¤ë¥¸ ë³€ìˆ˜ë“¤ì„ ì£¼ë³€í™”í•  ë•Œ ì œì™¸ì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê·¸ë˜í”„ëŠ” ì´ì°¨ì› ê°€ìš°ì‹œì•ˆ ë¶„í¬ì˜ ê²½ìš°ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ë³´ì‹œë‹¤ì‹œí”¼, ì£¼ë³€í™”ëœ ë¶„í¬ëŠ” ê° ì¶•ì— ë§¤í•‘ë˜ë©°, í˜•íƒœëŠ” ê°€ìš°ì‹œì•ˆ ë¶„í¬ì…ë‹ˆë‹¤. ì§ê´€ì ìœ¼ë¡œ, ë‹¤ë³€ëŸ‰ ê°€ìš°ì‹œì•ˆ ë¶„í¬ë¥¼ í•œ ì¶•ì— ë”°ë¼ ì˜ë¼ì„œ ë‹¨ë©´ì˜ ë¶„í¬ëŠ” ì—¬ì „íˆ ê°€ìš°ì‹œì•ˆ ë¶„í¬ë¥¼ ë”°ë¦…ë‹ˆë‹¤. ìƒì„¸í•œ ìœ ë„ê³¼ì •ì€ ì°¸ê³ ë¬¸í—Œ [2]ì—ì„œ í™•ì¸í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

íŠ¹ì§• 2ì˜ ê²½ìš°, ë™ì¼í•œ Dì°¨ì› ë‹¤ë³€ëŸ‰ ê°€ìš°ì‹œì•ˆ ë¶„í¬ì™€ ë‘ êµ¬ì—­ìœ¼ë¡œ ë‚˜ëˆˆ ì…ë ¥ ë°ì´í„° xâ‚ê³¼ xâ‚‚ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. ë‹¤ë³€ëŸ‰ ê°€ìš°ì‹œì•ˆ ë¶„í¬ì˜ ì¡°ê±´ë¶€ í™•ë¥ ì€ ë‹¤ìŒê³¼ ê°™ì´ ì“¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤:

![Formula 2](/assets/img/2024-07-08-MathematicalunderstandingofGaussianProcess_4.png)

<div class="content-ad"></div>

ê·¸ë¦¼ì—ì„œëŠ” 2ì°¨ì› ê°€ìš°ì‹œì•ˆ ë¶„í¬(ë“±ê³ ì„ ) ë° ì¡°ê±´ë¶€ ê°€ìš°ì‹œì•ˆ ë¶„í¬(ì ì„ )ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. ê°€ìš°ì‹œì•ˆ ë¶„í¬ì˜ ëª¨ì–‘ì€ ê° ì¡°ê±´ì—ì„œ ë‹¤ë¦…ë‹ˆë‹¤. ìì„¸í•œ ìœ ë„ ë‚´ìš©ì€ ì°¸ì¡° [2]ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## 1. ì„ í˜• íšŒê·€ ëª¨í˜• ë° ì°¨ì›ì˜ ì €ì£¼

ê°€ìš°ì‹œì•ˆ í”„ë¡œì„¸ìŠ¤ë¡œ ë“¤ì–´ê°€ê¸° ì „ì— ì„ í˜• íšŒê·€ ëª¨í˜•ê³¼ ê·¸ ë‹¨ì , ì¦‰ ì°¨ì›ì˜ ì €ì£¼ì— ëŒ€í•´ ëª…í™•íˆ í•´ë³´ê³  ì‹¶ìŠµë‹ˆë‹¤. ê°€ìš°ì‹œì•ˆ í”„ë¡œì„¸ìŠ¤ëŠ” ì´ ê°œë…ê³¼ ë°€ì ‘í•œ ê´€ë ¨ì´ ìˆìœ¼ë©° ê·¸ ë‹¨ì ì„ ê·¹ë³µí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ìµìˆ™í•˜ë‹¤ë©´ ì´ ì±•í„°ë¥¼ ê±´ë„ˆë›°ì…”ë„ ë©ë‹ˆë‹¤.

ì„ í˜• íšŒê·€ ëª¨í˜•ì„ ë‹¤ì‹œ ë³µìŠµí•´ë´…ì‹œë‹¤. ì„ í˜• íšŒê·€ ëª¨í˜•ì€ ê¸°ì € í•¨ìˆ˜ ğ“(x)ë¥¼ ì‚¬ìš©í•˜ì—¬ ë°ì´í„°ë¥¼ ìœ ì—°í•˜ê²Œ í‘œí˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

<div class="content-ad"></div>

ë§ˆë²•ì˜ íƒ€ë¡œê°€ ì—¬ëŸ¬ë¶„ì„ ì´ˆëŒ€í•©ë‹ˆë‹¤!

![ë§ˆë²•ì˜ íƒ€ë¡œ](/assets/img/2024-07-08-MathematicalunderstandingofGaussianProcess_5.png)

ê¸°ì € í•¨ìˆ˜ë¡œëŠ” ë‹¤í•­ì‹ í•­ì´ë‚˜ ì½”ì‚¬ì¸ í•¨ìˆ˜ì™€ ê°™ì€ ë¹„ì„ í˜• í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ ì„ í˜• íšŒê·€ ëª¨ë¸ì€ xì— ë¹„ì„ í˜• ê¸°ì € í•¨ìˆ˜ë¥¼ ì ìš©í•¨ìœ¼ë¡œì¨ ë¹„ì„ í˜• ê´€ê³„ë¥¼ íŒŒì•…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì•„ë˜ ê·¸ë˜í”„ëŠ” ë°©ì‚¬ ê¸°ì € í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ëŠ” ì˜ˆì‹œë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.

![ë§ˆë²•ì˜ íƒ€ë¡œ](/assets/img/2024-07-08-MathematicalunderstandingofGaussianProcess_6.png)

ì•Œ ìˆ˜ ìˆë“¯ì´ ë³µì¡í•œ ë°ì´í„° êµ¬ì¡°ë¥¼ íŒŒì•…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì—¬ì „íˆ ë§¤ê°œë³€ìˆ˜ wì˜ ê´€ì ì—ì„œëŠ” ì„ í˜• ê´€ê³„ë¥¼ ìœ ì§€í•˜ê¸° ë•Œë¬¸ì— ì´ë¥¼ ì„ í˜• íšŒê·€ ëª¨ë¸ì´ë¼ê³  í•©ë‹ˆë‹¤. ë§¤ê°œë³€ìˆ˜ì— ë¹„ì„ í˜• í‘œí˜„ì´ ì—†ëŠ”ì§€ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì ê²€í•´ë³´ì„¸ìš”!

<div class="content-ad"></div>

ì´ëŸ¬í•œ íŒŒë¼ë¯¸í„°ëŠ” ë‹¤ì¤‘ ì„ í˜• íšŒê·€ì™€ ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ ìœ ë„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë‹¤ìŒ ë°©ì •ì‹ë“¤ì€ ì„ í˜• íšŒê·€ ëª¨ë¸ì˜ í–‰ë ¬ ë° ì„ í˜• ëŒ€ìˆ˜ í˜•íƒœì…ë‹ˆë‹¤. ìš°ë¦¬ëŠ” Nê°œì˜ ë°ì´í„° í¬ì¸íŠ¸ì™€ p+1ê°œì˜ íŒŒë¼ë¯¸í„°ë¥¼ ê°€ì •í•©ë‹ˆë‹¤.

![Equation 1](/assets/img/2024-07-08-MathematicalunderstandingofGaussianProcess_7.png)

![Equation 2](/assets/img/2024-07-08-MathematicalunderstandingofGaussianProcess_8.png)

ì…ë ¥ ë°ì´í„°ì— ê¸°ì € í•¨ìˆ˜ë¥¼ ì ìš©í•œ í›„ í–‰ë ¬ ğš½ì˜ ê°’ë“¤ì´ ìƒìˆ˜ê°€ ë˜ëŠ” ê²ƒì„ ì£¼ëª©í•˜ì„¸ìš”. ì´ê²ƒì€ ë‹¤ì¤‘ ì„ í˜• íšŒê·€ì™€ ìœ ì‚¬í•˜ì§€ ì•Šë‚˜ìš”? ì‹¤ì œë¡œ, íŒŒë¼ë¯¸í„°ì˜ í•´ì„ì  ë¯¸ë¶„ì€ ê°™ìŠµë‹ˆë‹¤.

<div class="content-ad"></div>

![MathematicalunderstandingofGaussianProcess_9](/assets/img/2024-07-08-MathematicalunderstandingofGaussianProcess_9.png)

ì•ˆë…•í•˜ì„¸ìš”! ì„ í˜• íšŒê·€ ëª¨ë¸ì€ ê³µì‹(4)ì—ì„œ ë…ë¦½ ë³€ìˆ˜ê°€ í•˜ë‚˜ë¼ê³  ê°€ì •í•˜ëŠ” í•¨ì •ì´ ìˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ ì…ë ¥ ë°ì´í„° ì°¨ì›ì´ ì¦ê°€í• ìˆ˜ë¡ ë§¤ê°œ ë³€ìˆ˜ì˜ ìˆ˜ê°€ ê¸°í•˜ ê¸‰ìˆ˜ì ìœ¼ë¡œ ì¦ê°€í•©ë‹ˆë‹¤. ê¸°ì € í•¨ìˆ˜ì˜ ìˆ˜ë¥¼ ì¶”ê°€í•˜ë©´ ëª¨ë¸ì˜ ìœ ì—°ì„±ì„ ë†’ì¼ ìˆ˜ ìˆì§€ë§Œ, ê³„ì‚°ëŸ‰ì´ í˜„ì‹¤ì ì´ì§€ ì•Šê²Œ ì¦ê°€í•©ë‹ˆë‹¤. ì´ ë¬¸ì œë¥¼ ì°¨ì›ì˜ ì €ì£¼ë¼ê³  í•©ë‹ˆë‹¤. ê³„ì‚°ëŸ‰ì„ ì¦ê°€ì‹œí‚¤ì§€ ì•Šìœ¼ë©´ì„œ ëª¨ë¸ì˜ ìœ ì—°ì„±ì„ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆëŠ” ë°©ë²•ì´ ìˆì„ê¹Œìš”? ë„¤, ê°€ìš°ì‹œì•ˆ í”„ë¡œì„¸ìŠ¤ ì´ë¡ ì„ ì ìš©í•´ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë‹¤ìŒ ì„¹ì…˜ìœ¼ë¡œ ë„˜ì–´ê°€ì„œ ê°€ìš°ì‹œì•ˆ í”„ë¡œì„¸ìŠ¤ì— ëŒ€í•´ ì•Œì•„ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.

## 3. ê°€ìš°ì‹œì•ˆ í”„ë¡œì„¸ìŠ¤ì˜ ìˆ˜í•™ì  ë°°ê²½

ëª¨ë¸ì˜ ë§¤ê°œ ë³€ìˆ˜ ìˆ˜ê°€ ì¦ê°€í•  ë•Œ ì„ í˜• íšŒê·€ ëª¨ë¸ì´ ì§ë©´í•˜ëŠ” ì°¨ì›ì˜ ì €ì£¼ì— ëŒ€í•œ ë¬¸ì œë¥¼ ì‚´í´ë³´ì•˜ìŠµë‹ˆë‹¤. ì´ ë¬¸ì œì˜ í•´ê²°ì±…ì€ ë§¤ê°œ ë³€ìˆ˜ì˜ ê¸°ëŒ€ê°’ì„ ì·¨í•˜ê³  ë§¤ê°œ ë³€ìˆ˜ë¥¼ ê³„ì‚°í•  í•„ìš”ê°€ ì—†ëŠ” ìƒí™©ì„ ë§Œë“œëŠ” ê²ƒì…ë‹ˆë‹¤. ì´ê²ƒì´ ë¬´ì—‡ì„ ì˜ë¯¸í• ê¹Œìš”? ì„ í˜• íšŒê·€ ëª¨ë¸ì˜ ê³µì‹ì„ ê¸°ì–µí•´ë³´ì„¸ìš”.

<div class="content-ad"></div>

![Mathematical understanding of Gaussian Process 10](/assets/img/2024-07-08-MathematicalunderstandingofGaussianProcess_10.png)

Now, if we assume that the parameter w follows a Gaussian distribution, the output y will also follow a Gaussian distribution since the matrix ğš½ is just a constant value matrix. We assume the parameter follows the Gaussian distribution as shown below:

![Mathematical understanding of Gaussian Process 11](/assets/img/2024-07-08-MathematicalunderstandingofGaussianProcess_11.png)

From this assumption, we can calculate the parameters of the output distribution as follows:

<div class="content-ad"></div>

ìœ„ì—ì„œ ë³¼ ìˆ˜ ìˆë“¯ì´, ê¸°ëŒ€ê°’ì„ ì·¨í•¨ìœ¼ë¡œì¨ ë§¤ê°œë³€ìˆ˜ ê³„ì‚°ì„ ì œê±°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ ë¬´í•œí•œ ë§¤ê°œë³€ìˆ˜ê°€ ìˆë”ë¼ë„ ê³„ì‚°ì— ì˜í–¥ì„ ë¯¸ì¹˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì´ xì™€ y ì‚¬ì´ì˜ ê´€ê³„ëŠ” ê°€ìš°ì‹œì•ˆ í”„ë¡œì„¸ìŠ¤ë¥¼ ë”°ë¦…ë‹ˆë‹¤. ê°€ìš°ì‹œì•ˆ í”„ë¡œì„¸ìŠ¤ì˜ ì •ì˜ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

![ê°€ìš°ì‹œì•ˆ í”„ë¡œì„¸ìŠ¤](/assets/img/2024-07-08-MathematicalunderstandingofGaussianProcess_14.png)

<div class="content-ad"></div>

ì§ê´€ì ìœ¼ë¡œ ì´í•´í•˜ë©´, ê°€ìš°ì‹œì•ˆ í”„ë¡œì„¸ìŠ¤ëŠ” ë¬´í•œ ê°œìˆ˜ì˜ ë§¤ê°œë³€ìˆ˜ë¥¼ ê°–ëŠ” ë‹¤ë³€ëŸ‰ ê°€ìš°ì‹œì•ˆ ë¶„í¬ì…ë‹ˆë‹¤. ì—¬ê¸°ì„œ ë‚˜ì˜¤ëŠ” ê³µì‹ (7)ì€ ì£¼ì–´ì§„ ë°ì´í„°ë¡œë¶€í„° ê°€ìš°ì‹œì•ˆ í”„ë¡œì„¸ìŠ¤ì˜ ì£¼ë³€ ê°€ìš°ì‹œì•ˆ ë¶„í¬ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤. ì£¼ë³€ ë‹¤ë³€ëŸ‰ ê°€ìš°ì‹œì•ˆ ë¶„í¬ê°€ ì—¬ì „íˆ ê°€ìš°ì‹œì•ˆ ë¶„í¬ë¥¼ ë”°ë¥¸ë‹¤ëŠ” íŠ¹ì§•ì—ì„œ ë¹„ë¡¯ë©ë‹ˆë‹¤. ë”°ë¼ì„œ ê°€ìš°ìŠ¤ ê³¼ì •ì„ ì˜ í™œìš©í•¨ìœ¼ë¡œì¨ ë¬´í•œ ì°¨ì›ì˜ ë§¤ê°œë³€ìˆ˜ë¥¼ ê³ ë ¤í•˜ì—¬ ëª¨ë¸ì„ êµ¬ì¶•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ë˜ ë‹¤ë¥¸ ë¬¸ì œëŠ” ë§¤íŠ¸ë¦­ìŠ¤ ğš½ë¥¼ ì–´ë–»ê²Œ ì„ íƒí•´ì•¼ í•˜ëŠ”ì§€ì…ë‹ˆë‹¤. ìœ„ì˜ ê³µì‹ì—ì„œ ê³µë¶„ì‚° ë§¤íŠ¸ë¦­ìŠ¤ ë¶€ë¶„ì— ì§‘ì¤‘í•˜ê³  ì´ë¥¼ Kë¡œ ì„¤ì •í•  ë•Œ, ê° ìš”ì†ŒëŠ” ë‹¤ìŒê³¼ ê°™ì´ ì“¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤:

![ìˆ˜ì‹](/assets/img/2024-07-08-MathematicalunderstandingofGaussianProcess_15.png)

ê³µì‹ (9)ì— ë”°ë¥´ë©´, ê³µë¶„ì‚° ë§¤íŠ¸ë¦­ìŠ¤ì˜ ê° ìš”ì†ŒëŠ” ğŸ‡(xáµ¢)ì™€ ğŸ‡(xâ±¼)ì˜ ë‚´ì ì˜ ê³±ìœ¼ë¡œ ë©ë‹ˆë‹¤. ë‚´ì ì€ ì½”ì‚¬ì¸ ìœ ì‚¬ì„±ê³¼ ìœ ì‚¬í•˜ë¯€ë¡œ, ê³µì‹ (9)ì˜ ê°’ì€ xáµ¢ì™€ xâ±¼ê°€ ìœ ì‚¬í• ìˆ˜ë¡ ê°’ì´ ì»¤ì§‘ë‹ˆë‹¤.

<div class="content-ad"></div>

íŠ¹ì´ê°’ ë¶„í•´ì˜ íŠ¹ì„±ì¸ ëŒ€ì¹­ì´ê³  ì–‘ì˜ ì •ë¶€í˜¸ì¸ ê³µë¶„ì‚° í–‰ë ¬ì„ ì¶©ì¡±í•˜ê³  ì—­í–‰ë ¬ì„ ê°€ì§€ë ¤ë©´ ğŸ‡(x)ë¥¼ ì ì ˆíˆ ì„ íƒí•´ì•¼ í•©ë‹ˆë‹¤. ì´ë¥¼ ë‹¬ì„±í•˜ê¸° ìœ„í•´ ìš°ë¦¬ëŠ” ğŸ‡(x)ì— ëŒ€í•´ì„œ ì»¤ë„ í•¨ìˆ˜ë¥¼ í™œìš©í•©ë‹ˆë‹¤.

![image](/assets/img/2024-07-08-MathematicalunderstandingofGaussianProcess_16.png)

ì»¤ë„ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ëŠ” ì¥ì  ì¤‘ í•˜ë‚˜ëŠ” ì»¤ë„ í•¨ìˆ˜ë¥¼ í†µí•´ ğŸ‡(x)ì˜ ë‚´ì ì„ ëª…ì‹œì ìœ¼ë¡œ ê³„ì‚°í•˜ì§€ ì•Šê³  ì–»ì„ ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤. ì´ ê¸°ìˆ ì„ ì»¤ë„ íŠ¸ë¦­ì´ë¼ê³  í•©ë‹ˆë‹¤. ì»¤ë„ íŠ¸ë¦­ì— ëŒ€í•œ ìì„¸í•œ ì„¤ëª…ì€ [3] ë¸”ë¡œê·¸ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê°€ì¥ ë§ì´ ì‚¬ìš©ë˜ëŠ” ì»¤ë„ í•¨ìˆ˜ëŠ” ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.

- ê°€ìš°ì‹œì•ˆ ì»¤ë„

<div class="content-ad"></div>

![Linear kernel](/assets/img/2024-07-08-MathematicalunderstandingofGaussianProcess_17.png)

- Linear kernel

![Periodic kernel](/assets/img/2024-07-08-MathematicalunderstandingofGaussianProcess_18.png)

- Periodic kernel

<div class="content-ad"></div>

![ì´ë¯¸ì§€](/assets/img/2024-07-08-MathematicalunderstandingofGaussianProcess_19.png)

ì‹œê°í™”ëœ ë‚´ìš©ì—ì„œ ê° ì»¤ë„ì„ ì‚¬ìš©í•˜ì—¬ ì¼ì°¨ì› ê°€ìš°ì‹œì•ˆ í”„ë¡œì„¸ìŠ¤ë¥¼ ìƒ˜í”Œë§í•˜ëŠ” ê²ƒì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ì»¤ë„ì˜ íŠ¹ì§•ë“¤ì„ ë³¼ ìˆ˜ ìˆì–´ìš”.

ì´ì œ, ê°€ìš°ì‹œì•ˆ í”„ë¡œì„¸ìŠ¤ë¥¼ ë‹¤ì‹œ ì‚´í´ë´…ì‹œë‹¤. ì»¤ë„ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ì •ì˜ë¥¼ ë‹¤ì‹œ ì‘ì„±í•  ìˆ˜ ìˆì–´ìš”:

![ì´ë¯¸ì§€ ](/assets/img/2024-07-08-MathematicalunderstandingofGaussianProcess_20.png)

<div class="content-ad"></div>

When each element of \(f\) follows a Gaussian process, which represents an infinite Gaussian distribution, the joint probability of \(f\) follows a multivariate Gaussian distribution.

Moving forward, we will explore the practical application of the Gaussian process, specifically the Gaussian process regression model.

## 4. Applying the Gaussian Process: Gaussian Process Regression

In this last section, we will demonstrate how the Gaussian process is applied to regression analysis. Stay tuned for the exciting topics ahead!

<div class="content-ad"></div>

- ê°€ìš°ì‹œì•ˆ í”„ë¡œì„¸ìŠ¤ ëª¨ë¸ì„ í”¼íŒ…í•˜ê³  ì¶”ë¡ í•˜ëŠ” ë°©ë²•
- 1ì°¨ì› ë°ì´í„°ë¥¼ ìœ„í•œ ê°€ìš°ì‹œì•ˆ í”„ë¡œì„¸ìŠ¤ ëª¨ë¸ ì˜ˆì‹œ
- ë‹¤ì°¨ì› ë°ì´í„°ë¥¼ ìœ„í•œ ê°€ìš°ì‹œì•ˆ í”„ë¡œì„¸ìŠ¤ ëª¨ë¸ ì˜ˆì‹œ

## ê°€ìš°ì‹œì•ˆ í”„ë¡œì„¸ìŠ¤ ëª¨ë¸ í”¼íŒ…ê³¼ ì¶”ë¡ í•˜ëŠ” ë°©ë²•

ìš°ë¦¬ì—ê²ŒëŠ” Nê°œì˜ ì…ë ¥ ë°ì´í„° xì™€ í•´ë‹¹í•˜ëŠ” ì¶œë ¥ ë°ì´í„° yê°€ ìˆìŠµë‹ˆë‹¤.

![Mathematical understanding of Gaussian Process_21](/assets/img/2024-07-08-MathematicalunderstandingofGaussianProcess_21.png)

<div class="content-ad"></div>

ê°„í¸í•¨ì„ ìœ„í•´ ì „ì²˜ë¦¬ë¥¼ ìœ„í•´ ì…ë ¥ ë°ì´í„° xì— ì •ê·œí™”ë¥¼ ì ìš©í•©ë‹ˆë‹¤. ì´ëŠ” xì˜ í‰ê· ì´ 0ì´ë¼ëŠ” ê²ƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤. xì™€ y ì‚¬ì´ì— ë‹¤ìŒ ê´€ê³„ê°€ ìˆë‹¤ê³  ê°€ì •í•˜ê³ , fê°€ ê°€ìš°ì‹œì•ˆ í”„ë¡œì„¸ìŠ¤ë¥¼ ë”°ë¥¸ë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤.

![image](/assets/img/2024-07-08-MathematicalunderstandingofGaussianProcess_22.png)

ê·¸ëŸ¬ë¯€ë¡œ ì¶œë ¥ yëŠ” ë‹¤ìŒ ë‹¤ë³€ëŸ‰ ê°€ìš°ì‹œì•ˆ ë¶„í¬ë¥¼ ë”°ë¦…ë‹ˆë‹¤.

![image](/assets/img/2024-07-08-MathematicalunderstandingofGaussianProcess_23.png)

<div class="content-ad"></div>

ì í•©ì„±ì„ ìœ„í•´ ì»¤ë„ í•¨ìˆ˜ë¥¼ í†µí•´ ê³µë¶„ì‚° í–‰ë ¬ì„ ê³„ì‚°í•œ í›„, ì¶œë ¥ y ë¶„í¬ì˜ ë§¤ê°œë³€ìˆ˜ëŠ” ì •í™•íˆ í•˜ë‚˜ë¡œ ê²°ì •ë©ë‹ˆë‹¤. ë”°ë¼ì„œ ê°€ìš°ì‹œì•ˆ í”„ë¡œì„¸ìŠ¤ëŠ” ì»¤ë„ í•¨ìˆ˜ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì´ì™¸ì—ëŠ” í›ˆë ¨ ë‹¨ê³„ê°€ ì—†ìŠµë‹ˆë‹¤.

ì¶”ë¡  ë‹¨ê³„ëŠ” ì–´ë–»ìŠµë‹ˆê¹Œ? ê°€ìš°ì‹œì•ˆ í”„ë¡œì„¸ìŠ¤ëŠ” ì„ í˜• íšŒê·€ ëª¨ë¸ê³¼ ê°™ì´ ê°€ì¤‘ì¹˜ ë§¤ê°œë³€ìˆ˜ê°€ ì—†ê¸° ë•Œë¬¸ì— ìƒˆ ë°ì´í„°ë¥¼ í¬í•¨í•˜ì—¬ ë‹¤ì‹œ ì í•©í•´ì•¼ í•©ë‹ˆë‹¤. í•˜ì§€ë§Œ, ë‹¤ë³€ëŸ‰ ê°€ìš°ìŠ¤ ë¶„í¬ì˜ íŠ¹ì§•ì„ ì´ìš©í•˜ì—¬ ê³„ì‚°ëŸ‰ì„ ì¤„ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ìƒˆ ë°ì´í„° í¬ì¸íŠ¸ mê°œë¥¼ ê°€ì •í•´ë´…ì‹œë‹¤.

![image](/assets/img/2024-07-08-MathematicalunderstandingofGaussianProcess_24.png)

<div class="content-ad"></div>

In this case, the distribution, including new data points, also follows the Gaussian distribution, so we can describe it as follows:

![Gaussian Process](/assets/img/2024-07-08-MathematicalunderstandingofGaussianProcess_25.png)

Don't forget about formula (2), the parameters of the conditional multivariate Gaussian distribution.

![Gaussian Process Parameters](/assets/img/2024-07-08-MathematicalunderstandingofGaussianProcess_26.png)

<div class="content-ad"></div>

ì´ ê³µì‹ì„ ì‹(11)ì— ì ìš©í•˜ë©´ ë§¤ê°œë³€ìˆ˜ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

![MathematicalunderstandingofGaussianProcess_27](/assets/img/2024-07-08-MathematicalunderstandingofGaussianProcess_27.png)

ì´ê²ƒì€ ê°€ìš°ì‹œì•ˆ í”„ë¡œì„¸ìŠ¤ íšŒê·€ ëª¨ë¸ì˜ ì—…ë°ì´íŠ¸ ê³µì‹ì…ë‹ˆë‹¤. ì´ë¥¼ í‘œë³¸ì´ ì·¨í•˜ê³  ì‹¶ì„ ë•ŒëŠ” ì´ë ˆìŠ¤í‚¤ ë¶„í•´ì—ì„œ ìœ ë„ëœ í•˜ì‚¼ê° í–‰ë ¬ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.

![MathematicalunderstandingofGaussianProcess_28](/assets/img/2024-07-08-MathematicalunderstandingofGaussianProcess_28.png)

<div class="content-ad"></div>

[4]ì—ì„œ ë””í…Œì¼í•œ ì´ë¡ ì  ì„¤ëª…ì„ í™•ì¸í•  ìˆ˜ ìˆì–´ìš”. í•˜ì§€ë§Œ ì‹¤ì œ ìƒí™©ì—ì„œëŠ” íŒŒì´ì¬ì— ì´ë¯¸ ì¢‹ì€ ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤ì´ ìˆê¸° ë•Œë¬¸ì— ê°€ìš°ì‹œì•ˆ í”„ë¡œì„¸ìŠ¤ íšŒê·€ë¥¼ ì²˜ìŒë¶€í„° êµ¬í˜„í•  í•„ìš”ê°€ ì—†ì–´ìš”.

ì•„ë˜ ì‘ì€ ì„¹ì…˜ë“¤ì—ì„œ ìš°ë¦¬ëŠ” Gpy ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ê°€ìš°ì‹œì•ˆ í”„ë¡œì„¸ìŠ¤ë¥¼ ì–´ë–»ê²Œ êµ¬í˜„í•˜ëŠ”ì§€ ë°°ìš¸ ê±°ì˜ˆìš”. pipë¥¼ í†µí•´ ì‰½ê²Œ ì„¤ì¹˜í•  ìˆ˜ ìˆì–´ìš”.

## ì˜ˆì‹œ: ì¼ì°¨ì› ë°ì´í„°ìš© ê°€ìš°ì‹œì•ˆ í”„ë¡œì„¸ìŠ¤ ëª¨ë¸

ìš°ë¦¬ëŠ” ê°€ìš°ì‹œì•ˆ ë…¸ì´ì¦ˆì™€ í•¨ê»˜ ì‹  ê¸°ëŠ¥ì—ì„œ ìƒì„±ëœ ì¥ë‚œê° ì˜ˆì œ ë°ì´í„°ë¥¼ ì‚¬ìš©í•  ê±°ì˜ˆìš”.

<div class="content-ad"></div>

# ë¬´ì‘ìœ„ ìƒ˜í”Œ ìƒì„±

X = np.linspace(start=0, stop=10, num=100).reshape(-1, 1)
y = np.squeeze(X \* np.sin(X)) + np.random.randn(X.shape[0])
y = y.reshape(-1, 1)

![Image](/assets/img/2024-07-08-MathematicalunderstandingofGaussianProcess_29.png)

ìš°ë¦¬ëŠ” RBF ì»¤ë„ì„ ì‚¬ìš©í•˜ëŠ” ì´ìœ ëŠ” ë§¤ìš° ìœ ì—°í•˜ê³  ì‚¬ìš©í•˜ê¸° ì‰½ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. GPy ë•ë¶„ì— ëª‡ ì¤„ ë§Œìœ¼ë¡œ RBF ì»¤ë„ê³¼ ê°€ìš°ì‹œì•ˆ í”„ë¡œì„¸ìŠ¤ íšŒê·€ ëª¨ë¸ì„ ì„ ì–¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

# RBF ì»¤ë„

kernel = GPy.kern.RBF(input_dim=1, variance=1., lengthscale=1.)

# RBF ì»¤ë„ì„ ì‚¬ìš©í•œ ê°€ìš°ì‹œì•ˆ í”„ë¡œì„¸ìŠ¤ íšŒê·€

m = GPy.models.GPRegression(X, y, kernel)

<div class="content-ad"></div>

![Gaussian Process](/assets/img/2024-07-08-MathematicalunderstandingofGaussianProcess_30.png)

When we look at this image, we can observe the x point, which represents the input data. The blue curve illustrates the expected values of the Gaussian process regression model at that point. The light blue shaded region indicates the 95% confidence interval.

It's interesting to note that the confidence interval is shallower in areas with many data points and broader in areas with only a few points. For engineers in the manufacturing industry, this can offer insights into where more data is needed, and where the data region (or experimental condition) is likely to have target values.

## Example: Using a Gaussian process model for data in multiple dimensions

<div class="content-ad"></div>

ìŠ¤í‚¥ëŸ° ìƒ˜í”Œ ë°ì´í„°ì…‹ì—ì„œ ë‹¹ë‡¨ë³‘ ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•  ê²ƒì…ë‹ˆë‹¤.

```python
# ë‹¹ë‡¨ë³‘ ë°ì´í„°ì…‹ ë¶ˆëŸ¬ì˜¤ê³  ë°ì´í„°í”„ë ˆì„ ìƒì„±
diabetes = datasets.load_diabetes()
df = pd.DataFrame(diabetes.data, columns=diabetes.feature_names)
```

í•´ë‹¹ ë°ì´í„°ì…‹ì€ ì´ë¯¸ ì „ì²˜ë¦¬(ì •ê·œí™”)ë˜ì–´ ìˆìœ¼ë¯€ë¡œ ê°€ìš°ì‹œì•ˆ í”„ë¡œì„¸ìŠ¤ íšŒê·€ ëª¨ë¸ì„ ì§ì ‘ êµ¬í˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì—¬ëŸ¬ ì…ë ¥ì— ëŒ€í•´ ê°™ì€ í´ë˜ìŠ¤ë¥¼ ì‚¬ìš©í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.

```python
# ë°ì´í„°ì…‹
X = df[['age', 'sex', 'bmi', 'bp']].values
y = df[['target']].values

# ì»¤ë„ ì„¤ì •
kernel = GPy.kern.RBF(input_dim=4, variance=1.0, lengthscale=1.0, ARD=True)

# GPR ëª¨ë¸ ìƒì„±
m = GPy.models.GPRegression(X, y, kernel=kernel)
```

<div class="content-ad"></div>

The result is displayed in the table below.

![The result table](/assets/img/2024-07-08-MathematicalunderstandingofGaussianProcess_31.png)

As you can see, there is plenty of room for improvement. The simplest way to enhance the prediction result is by collecting more data. If that's not feasible, you could consider altering the kernel selection or optimizing the hyperparameters. I'll share the code I utilized later, so feel free to modify and experiment with it!

You can recreate the visualizations and GPy experiments using the code provided below.

<div class="content-ad"></div>

ìš°ë¦¬ëŠ” ê°€ìš°ìŠ¤ ê³¼ì •ì˜ ìˆ˜í•™ì  ì •ë¦¬ì™€ ì‹¤ì œ êµ¬í˜„ì— ëŒ€í•´ ì´ì•¼ê¸°í–ˆìŠµë‹ˆë‹¤. ì´ ê¸°ìˆ ì€ ë°ì´í„°ê°€ ì ì„ ë•Œ ìœ ìš©í•©ë‹ˆë‹¤. í•˜ì§€ë§Œ, ê³„ì‚° ì–‘ì´ ë°ì´í„° ì–‘ì— ë”°ë¼ ë‹¬ë¼ì§€ê¸° ë•Œë¬¸ì— ëŒ€ìš©ëŸ‰ ë°ì´í„°ì— ì í•©í•˜ì§€ ì•Šë‹¤ëŠ” ì ì„ ê¸°ì–µí•´ ì£¼ì„¸ìš”. ì½ì–´ì£¼ì…”ì„œ ê°ì‚¬í•©ë‹ˆë‹¤!

## ì°¸ê³  ë¬¸í—Œ

[1] Mochihashi, D., Oba, S., ã‚¬ã‚¦ã‚¹éç¨‹ã¨æ©Ÿæ¢°å­¦ç¿’, è¬›è«‡ç¤¾

[2]https://www.khoury.northeastern.edu/home/vip/teach/MLcourse/3_generative_models/lecture_notes/Marginal and conditional distributions of multivariate normal distribution.pdf

<div class="content-ad"></div>

There is a fascinating article by Hui in Medium about Kernels for Machine Learning. You can also check out this resource at [rpubs](https://rpubs.com/binhho660/922614). Enjoy exploring these insightful reads!
