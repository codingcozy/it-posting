---
title: "RAG에서 검색 최적화하기 언제, 무엇을, 어떻게 진행해야 할까"
description: ""
coverImage: "/assets/img/2024-07-09-OptimizingRetrievalinRAGWhenWhatandHow_0.png"
date: 2024-07-09 23:43
ogImage:
  url: /assets/img/2024-07-09-OptimizingRetrievalinRAGWhenWhatandHow_0.png
tag: Tech
originalTitle: "Optimizing Retrieval in RAG: When, What and How"
link: "https://medium.com/ai-in-plain-english/optimizing-retrieval-in-rag-when-what-and-how-bff2bb8356bf"
isUpdated: true
---

인공지능 소프트웨어가 사용되어 이 기사의 텍스트 문법, 흐름 및 가독성을 향상시켰습니다.

RAG (Retrieval Augmented Generation)는 대규모 언어 모델(LLMs)을 외부 지식 소스와 연계하는 강력한 패러다임으로 부상했습니다. RAG는 텍스트나 지식 그래프와 같은 말뭉치에서 관련 정보를 검색함으로써 LLMs의 환영 문제를 완화하고 지식 집중적 작업에서의 성능을 향상시킵니다.

그러나 RAG의 효과는 검색 구성 요소에 크게 달려 있습니다. 검색을 트리거할 때, 어떤 맥락을 검색할지, 그리고 그러한 맥락을 효율적으로 검색할지 결정하는 것이 중요합니다. 이 기사에서는 RAG 시스템에서 검색의 이러한 핵심 측면에 대해 고찰합니다.

## 지식 기반 생성의 핵심 과제

<div class="content-ad"></div>

현대적인 LLM들은 자연어 생성에서 놀라운 능력을 보여주고 있지만, 종종 사실상의 모순, 환각, 그리고 이들의 훈련 데이터에 존재하지 않는 다량의 외부 지식에 접근해야 하는 작업에서 실패하는 경향이 있습니다. 이는 질문 응답, 분석, 의사 결정 지원 시스템과 같은 지식 집약적인 영역에서 이들의 유용성을 제한합니다.

RAG 패러다임 (검색 증강 생성)은 이 제약을 해소하기 위해 외부 말뭉치에서 관련 지식을 동적으로 검색하여 LLM의 입력에 통합하는 방식을 시도합니다. 이 외부 지식은 LLM의 생성을 "증강"시키며, 믿을만한 출처의 사실적 정보에 근거한 결과물을 생산할 수 있게 도와주며, 동시에 LLM의 유창성, 일관성 및 언어 이해 능력을 유지합니다.

RAG 시스템은 세 가지 핵심 구성 요소로 구성됩니다:

- 텍스트 말뭉치나 지식 그래프와 같은 지식 출처
- 검색 모듈로 출처에서 관련 지식 단편을 식별하고 검색하는 기능
- 검색된 지식을 소비하는 언어 모델과 입력을 사용하여 지식에 기반한 출력물을 생성합니다.

<div class="content-ad"></div>

## 최적화된 검색의 중요성

RAG는 인상적인 결과를 보여줬지만, 그 성능은 검색 구성요소의 효과적인 실행과 끊임없이 연결돼 있습니다. 최적이 아닌 검색 전략은 LLM의 생성에 부정적인 영향을 미칠 수 있습니다:

- 충분하지 않은 검색은 지식 범위를 불완전하게 만들어 지면된 추론을 방해할 수 있습니다.
- 과도한 검색은 관련 없는 정보로 LLM을 넘치게 만들어, 인지 부하를 늘립니다.
- 비효율적인 검색 메커니즘은 지연 시간을 증가시켜 실시간 응용을 방해합니다.
- 잘못 구성된 쿼리는 품질이 낮은 검색을 유발해, 증대의 이점을 무효화시킬 수 있습니다.

그러므로, 검색을 최적화하기 위해 트리거를 언제 작동시킬지 신중하게 결정하고, 무엇을 검색해야 하는지 정확하게 명시하고, 그 컨텍스트를 검색하는 효과적인 메커니즘을 개발하는 것이 RAG의 전체 잠재력을 실현하는 데 중요합니다.

<div class="content-ad"></div>

이러한 이슈는 LLM(Large Language Models) 및 지식 원본들이 규모와 복잡성이 증가함에 따라 더욱 중요해집니다. 더 긴 맥락을 분석하고 지식 그래프와 같이 더 복잡한 지식 표현을 다루는 대규모 언어 모델들을 고려할 때, 최적화된 검색 전략은 처리 가능하고 고품질의 지식 기반을 만드는 데 필수적입니다.

이 글에서는 RAG 시스템에서 검색 최적화의 세 가지 측면 — 언제, 무엇, 어떻게 — 에 대해 심층적으로 다룹니다. 최신 기술 접근 방식, 개방적인 도전 과제, 그리고 희망적인 미래 방향을 살펴보며, 이 핵심 구성요소에 대한 종합적인 시각을 제공합니다. 이는 지식 기반 AI 능력을 바탕으로 하는 이 중요한 구성요소에 대한 통찰력 있는 것입니다.

![이미지](/assets/img/2024-07-09-OptimizingRetrievalinRAGWhenWhatandHow_0.png)

# 언제 검색해야 하는가: 실시간 정보 요구 감지

<div class="content-ad"></div>

회수 강화 생성 시스템에서 가장 중요한 결정 중 하나는 회수 모듈을 활성화하고 외부 소스에서 추가 지식을 가져오는 최적의 타이밍을 결정하는 것입니다. 이 타이밍을 올바르게 맞추는 것은 섬세한 균형조정 작업입니다. 회수를 너무 자주 트리거하면 지나친 노이즈와 계산 오버헤드가 발생할 수 있고, 필요할 때 회수하지 않으면 언어 모델의 정확하고 지식 기반의 출력 능력에 제약을 줄 수 있습니다.

고정된 회수 타이밍 방법

대부분의 기존 RAG 방법은 회수 시기를 결정하기 위해 비교적 간단한 방법을 사용합니다. 일반적인 전략은 다음과 같이 고정된 간격으로 회수를 트리거하는 것입니다.

Markdown format 변경:

- 토큰 수준 회수: LLM이 생성한 각 n 개의 토큰마다 회수를 활성화합니다.
- 문장 수준 회수: LLM이 각 완전한 문장을 생성한 후 회수를 트리거합니다.

<div class="content-ad"></div>

직관적이지만, 정적이고 간격 기반의 접근법은 언어 모델의 동적 정보 요구에 적응하지 못할 수 있습니다. 입력, 문맥 및 현재 생성 상태에 따라 상당히 다를 수 있기 때문입니다.

다른 기술로는 토큰 불확실성을 신호로 사용하는 것이 있습니다. 특정 토큰에 대한 LLM의 예측 확률이 설정된 임계값 아래로 떨어질 때 검색을 활성화합니다. 그러나 토큰 불확실성만으로는 실제 지식 간격을 결정하기에 불완전할 수 있습니다. 왜냐하면 낮은 신뢰도를 가진 토큰일지라도 더 넓은 문맥에서는 사소할 수 있기 때문입니다.

동적인 문맥에 따른 검색 시기의 필요성

대형 언어 모델은 내부 지식만으로도 유의미한 콘텐츠를 자주 생성할 수 있는 매우 능숙한 시스템입니다. 복잡한 추론 도전이나 지식 간격을 만났을 때에만 외부 검색이 필요합니다. 그들의 정보 요구는 현재 문맥과 밀접하게 연관되어 있으며, 동일한 생성 과정 내에서도 빠르게 변할 수 있습니다.

<div class="content-ad"></div>

"When to retrieve" 전략은 동적이고 맥락에 민감해야 합니다. 실시간 요구 사항을 종합적으로 이해하여 LLM이 지식 증강이 필요한 경우를 정확히 파악할 수 있어야 합니다. 이를 하지 않으면 너무 많은 중복된 검색을 유발하여 계산 부하를 증가시키지만 실질적인 가치를 추가하지 않을 수도 있고, 더 나쁜 경우에는 지식 기초를 놓치게 되어 불완전하거나 부정확한 결과를 초래할 수 있습니다.

RIND: 실시간 정보 요구사항 감지

DRAGIN (LLM의 정보 요구 기반 동적 검색 증강 생성) 프레임워크는 이러한 도전 과제를 해결하기 위해 RIND (실시간 정보 요구사항 감지)라는 혁신적인 접근 방식을 소개했습니다. RIND는 토큰 불확실성과 같은 간단한 신호를 넘어 검색이 정말로 필요한지를 나타낼 수 있는 다양한 요소를 분석합니다:

- 토큰 불확실성: 예측된 확률 분포를 기반으로 한 토큰 불확실성은 결정 요소가 되지 않지만 유용한 신호입니다.
- 토큰의 의미적 중요성: RIND는 각 토큰이 문맥 내에서 얼마나 의미론적으로 중요하거나 중요한지를 이해하여 한정사나 불용어를 가중 평가합니다.
- 후속 문맥에 미치는 영향: LLM의 자기주의 분포를 조사함으로써 RIND는 각 토큰이 후속 문맥 생성에 미치는 영향을 평가할 수 있습니다.
- 생성 복잡성: RIND는 또한 개념의 추상성, 필요한 추론 유형 (예: 다중 점프, 가상 사실) 및 정보 다양성과 같은 속성을 분석하여 생성 과정의 전반적인 복잡성과 지식 요구 사항을 평가합니다."

<div class="content-ad"></div>

모든 요인을 종합적으로 모델링하고 절도 있게 결합함으로써, RIND는 외부 지식 검색이 언어 모델의 생성 능력을 최적으로 보완하고 뿌리내릴 필요가 있는지에 대해 세심한 판단을 할 수 있습니다.

RIND의 맥락 풍부한 방식과 대조적으로, 주로 토큰 불확실성에 의존하는 FLARE과 같은 방법은 결핍될 수 있습니다. 이러한 방법은 중요하지 않은 불확실성 때문에 의미없이 검색을 활성화할 수도 있고, 더 악화된 경우 내부 지식만으로 처리하기 어려운 복합적인 시나리오에 마주할 때 검색을 실패할 수도 있습니다.

대형 언어 모델이 생성 능력을 계속 발전시키는 가운데, 검색할 때의 최적 결정은 점점 더 중요해질 것입니다. 정적이고 휴리스틱 기반의 접근 방식은 충분하지 않을 것으로 예상되며, RIND와 같이 LLM의 실시간 정보 요구를 정확하게 파악하고 지식 검색을 적절히 활성화할 수 있는 지능적인 전략이 필요할 것입니다. 이는 모델이 익숙한 개념을 바탕으로 추론하는 놀라운 능력을 유지하면서 더 복잡하고 지식 집약적인 시나리오에 외부 지식을 보다 효과적으로 추가하는 데 도움이 됩니다.

# 검색할 내용: Self-Attention을 통해 쿼리 작성하기

<div class="content-ad"></div>

결정적인 위치로 돌아가는 것을 활성화하기로 결정되었을 때, 다음 중요한 단계는 외부 지식 원천으로부터 정확히 어떤 문맥 또는 정보를 검색해야 하는지 결정하는 것입니다. 불완전하거나 관련 없는 정보를 검색하는 것은 아예 검색하지 않는 것만큼 해로울 수 있습니다. 왜냐하면 이는 언어 모델을 오도하거나 생성 프로세스에 소음을 도입할 수 있기 때문입니다.

로컬 창 기반 쿼리 형성

대부분의 기존 RAG 방법은 입력 내의 로컬 컨텍스트 창에 기초한 비교적 단순한 방법을 사용하여 쿼리 형성에 적용합니다. 일반적인 전략으로는 다음과 같은 것들이 있습니다:

- 마지막 몇 단어: LLM에 의해 생성된 마지막 몇 개의 토큰만 사용하여 쿼리를 형성합니다.
- 이전 문장: LLM이 이전에 생성한 문장을 쿼리로 사용합니다.

<div class="content-ad"></div>

직관적이지만 협소한 지역 창에 기반한 이러한 접근 방식은 보다 광범위한 글로벌 맥락을 포착하지 못하고 LLM의 실제 정보 요구를 정확히 파악하지 못할 수 있습니다. 이는 입력의 다양한 부분에 걸쳐 있는 정보를 포괄적으로 이해하는 데 중요한 알렉산더 대왕의 전투와 전략에 대한 맥락을 포괄하지 못하고 "청취해야 할 정보를 회색 영역에서 좁게 정의하는 허여운 질의를 만들 수 있습니다.

예를 들어, "알렉산더 대왕의 주요 군사 캠페인은 무엇이었고, 그의 전략이 처음 강나르 철저한 후계자들에게 어떻게 영향을 미쳤는가?"와 같은 질문을 고려해 봅시다. 마지막 몇 단어에만 초점을 맞춘 단순한 방법은 "청사하다"와 같은 빈약한 질의를 만들어 알렉산더의 캠페인과 전략에 대한 컨텍스트를 포함하지 않아서 답변을 포괄적으로 제공하는 데 중요한 정보를 검색하는 것에 실패할 수 있습니다.

글로벌, 컨텍스트 리치한 질의 수립을 위한 자가주의

트랜스포머와 같은 대형 언어 모델은 자가주의 메커니즘을 통해 전체 입력 컨텍스트를 분석하며, 현재 생성 요구에 따라 다른 세그먼트에서 정보를 동적으로 집중하고 결합합니다. 이를 통해 그들의 자가주의 분포는 효과적인 검색 질의를 구성하는 데 가장 관련성 있는 입력의 어떤 측면이 가장 중요한지에 대한 매우 풍부한 정보원으로 나타납니다.

<div class="content-ad"></div>

\*\*DRAGIN 프레임워크의 QFS (자기 주의를 기반으로 하는 쿼리 형성) 구성 요소는 이를 활용하여 LLM의 입력 토큰에 대한 자기 주의 분포를 조사함으로써 이를 이용합니다. 로컬 컨텍스트에만 초점을 맞추는 대신, QFS는 LLM이 현재 생성 상태에 가장 중요하다고 판단하는 토큰 즉, 가장 높은 주의 가중치를 갖는 토큰을 식별합니다.

이러한 높은 주의를 받은 토큰들을 우선적으로 결합하여 쿼리를 구성함으로써, QFS는 LLM의 정보 요구 사항을 전체적인 맥락 내에서 정확하게 포착할 수 있습니다. 이를 통해 부정확하거나 주변 정보 대신 목표로 하는 관련 지식 요소를 검색할 수 있습니다.

예를 들어, 이전 예시에서 QFS는 높은 자기 주의 토큰을 우선순위로 두어 "알렉산더 대왕의 군사 캠페인 전략이 체육 황제같은 후대 정복자에게 영향을 주었다"와 같은 풍부한 쿼리를 형성할 수 있습니다. 이를 통해 알렉산더의 전략, 그 영향 및 다른 정복자들에 대한 관련 컨텍스트에 대한 포관적인 정보를 검색하여, LLM의 실제 정보 요구 사항을 최대한 충족시킬 수 있습니다.\*\*

**구조화된 지식에 대한 쿼리 형성**

<div class="content-ad"></div>

텍스트 말뭉치는 흔히 알려진 정보 원본이지만, 지식 그래프와 같은 보다 구조화된 원본을 활용하는 데 관심이 증가하고 있습니다. 이러한 경우, 효과적인 쿼리 작성이 더욱 중요해지며, 부정확한 쿼리는 지식 원본에서 상세하지 않거나 관련이 없는 하위 그래프를 검색할 수 있습니다.

QFS의 self-attention 기반 접근 방식은 이러한 상황에 특히 적합합니다. QFS는 입력을 단어 모음으로 다루는 대신, 입력 시퀀스 내의 자기-주목하는 중요성과 연결 및 관계에 따라 토큰에 우선순위를 부여할 수 있습니다. 이러한 전역적이고 구조화된 시각을 통해 QFS는 지식 그래프를 검색할 때 만족해야 할 제약 조건, 정보 및 관계를 더 정확하게 지정할 수 있는 쿼리를 작성할 수 있습니다.

예를 들어, QFS는 "알렉산더 대왕과 관련된 (군사 작전)과 그것이 (천조스 칸과 같은 다른 정복자들의 전략에 미친 영향)에 대한 정보를 찾아주세요"와 같이 표현력 있는 구조화된 쿼리를 생성할 수 있습니다.

또는 적절한 개체/관계/컨텍스트 제약 조건의 조합을 지정하여 구조화된 원본에서 정보를 최대로 관련 있는 하위 그래프를 검색할 수도 있습니다.

<div class="content-ad"></div>

지금까지 주로 사용되던 로컬 창에만 초점을 맞춘 이전의 방법론들은 전역적인 맥락 속에서 필요한 정보를 정확하게 찾아내기 어렵습니다.

쿼리 형성의 미래

언어 모델과 지식 소스가 복잡해지면서, self-attention 기반의 쿼리 형성이 높은 정확성의 검색 보강에 반드시 필요해질 것으로 예상됩니다. 단순한 단어 가방이나 창 기반 방법론은 전체 콘텍스트 내에서 필요한 정보를 정확하게 분리하는 데 더 이상 충분치 않을 것입니다.

또한, 정보 필요성 감지 및 서브그래프 추출/요약 기술과 같은 다른 구성 요소와 쿼리 형성을 밀접하게 통합하는 데 추가 연구가 필요합니다 (“검색 방법” 섹션에 다루어짐). RIND가 검색이 필요한 시점을 이해하는 능력을 QFS의 필요한 사항을 정확하게 명시하는 기능과 결합하고, 해당 정보를 실제로 검색하고 처리하여 더 나은 응답으로 구성하는 최적화를 통해, 지식 기반 언어 생성의 품질과 유틸리티를 극대화할 수 있습니다.

<div class="content-ad"></div>

Tarot expert로서, 자주 묻는 질문 중 하나가 "정확하고 목표 지향적인 쿼리를 작성하고 LLM이 필요로 하는 정보를 정확히 검색하는 데 있어 핵심이 되는 자기 주의적 컨텍스트 모델링은 믿을 만하고 신뢰할 수 있는 지식 중심 AI 시스템의 길을 열어줍니다.

# 검색하는 방법: 구조화된 지식 소스와 그래프 검색

지식 그래프와 기타 구조화된 데이터 표현의 부상은 검색 증강 생성 시스템에 대한 새로운 지평을 연 확실히 했습니다. 텍스트 코퍼스는 자연 언어를 통해 암시적이고 구조화되지 않은 방식으로 정보를 포착하는 반면, 지식 그래프는 그래프 기반 형식을 통해 내재적 엔티티, 관계, 의미를 명시적으로 만듭니다.

이러한 명시성과 구조화된 특성은 LLM의 추론, 다중 홉 처리 및 연결된 정보 흐름 처리를 향상시킬 수 있는 더 풍부한 표현을 가능하게 합니다. 그러나 이는 지식 소스에서 관련 있는 서브그래프를 탐색하고 추출할 수 있는 보다 고급 검색 패러다임을 필요로 합니다.

<div class="content-ad"></div>

From Text Retrieval to Graph Retrieval

전통적인 텍스트 검색 방법인 희소 벡터 공간 모델 (예: BM25) 또는 밀집 검색 (예: DPRs)은 대규모 단어 뭉치 텍스트 말뭉치에서 작동하도록 설계되었습니다. 이러한 방법들은 쿼리를 상대적으로 구조화되지 않은 방식으로 관련 단락이나 문서와 일치시킵니다.

반면, 지식 그래프에서 정보를 검색하려면 명시적이고 구조화된 표현을 이해하고 탐색해야 합니다. 지식 그래프를 텍스트 말뭉치로 취급하고 전통적인 검색을 적용하는 단순한 방법은 부족합니다. 이 방법은 그래프 구조에 인코딩된 풍부한 관계, 의미 및 제약 조건 정보를 캡쳐하지 못합니다.

지식 그래프 검색 파이프라인

**Knowledge Graph Retrieval Pipeline**

<div class="content-ad"></div>

지식 그래프를 효과적으로 활용하기 위해 RAG 프레임워크는 특화된 그래프 검색 파이프라인을 포함해야 합니다. 다음과 같은 기술을 활용해야 합니다:

1. 초기 의미 검색: 텍스트 검색과 마찬가지로, 첫 번째 단계는 입력 쿼리를 기반으로 관련 entity 노드를 식별하기 위해 벡터 인덱스(예: 노드 텍스트/설명)를 사용하는 것입니다. 이 의미 검색은 정보 검색 사전 필터링 단계로 작용합니다.

2. 서브그래프 검색: 초기 노드로부터, 표현력 있는 그래프 쿼리(Cypher for Neo4j와 같은)가 생성되어 관련 정보를 포함하는 이웃 노드나 연결된 서브그래프를 검색합니다. 이는 그래프에서 명시적 관계와 연결성을 기반으로 추가적인 컨텍스트를 제공합니다.

3. 그래프 분석 강화: 핵심 서브그래프는 구조화된 표현을 활용하는 그래프 분석 알고리즘을 통해 더욱 세밀하게 개선되고 확장될 수 있습니다.

<div class="content-ad"></div>

링크 예측: 노드 사이의 잠재적 관계를 매핑하지 않은 상태에서 탐색해보세요.  
노드 임베딩: 직접적인 링크 이상으로 관련 엔티티를 추천합니다.  
요약: 대규모 그래프를 중요한 썸네일 요약 하위 그래프로 요약합니다.  
군집화: 커뮤니티/군집 소속을 기반으로 맥락을 분할합니다.

마지막으로, 입력 쿼리에서 파싱한 제약 조건 (예: 날짜, 위치, 조건)은 검색된 맥락을 필터링하고 하위 그래프로 제한하여 정보 요구사항에 밀접한 관련성을 제공합니다.

이 그래프 검색 파이프라인은 전통적인 텍스트 검색과는 크게 대조됩니다. 초기 의미적 일치는 전반적인 정보 범위를 제공하며, 이후 단계에서는 반복적으로 그 지역적, 구조화된 맥락을 검색, 보완 및 정제하여 그래프의 명시적 표현을 활용합니다.

그라운딩된 추론을 위한 구조화된 지식

<div class="content-ad"></div>

RAG 시스템은 지식 그래프에서 가져온 풍부하고 상호 연결된 맥락을 통해 텍스트만으로는 어려운 심층적 추론 능력을 가능하게 할 수 있습니다.

- 다중 점프 추론: 관련있는 다단계 추론 체인을 검색하여 다중 점프 질문에 대한 작업을 수행할 수 있습니다.
- 시간적 추론: 시간적 제약 조건은 사건 추론을 위해 정보 흐름을 시기에 따라 필터링하고 정렬할 수 있습니다.
- 관계 추론: 의미론적 관계는 개념적 유사성을 드러내어 유추 추론을 지원할 수 있습니다.
- 추론 추론: 인과 부분 그래프를 통해 설명이나 근본 원인에 대해 추론할 수 있습니다.

이러한 추론 유형이 제공하는 조합 계산 능력은 지식 그래프가 큰 규모의 언어 모델(Large Language Models, LLMs)에 대한 지식 소스로 점점 더 널리 채택되는 주요 이유입니다.

건강 관련 예시

<div class="content-ad"></div>

그래프 검색의 힘을 설명하기 위해, 지식 그래프로 표현된 FHIR (Fast Healthcare Interoperability Resources) 데이터 표준 상의 의료 질문 응답 시스템을 고려해 보죠:

"1월 15일에 이상한 검사 결과 후 환자 X에게 수행된 절차는 무엇인가요?"와 같은 초기 질의는 벡터 검색을 사용하여 관련 환자 개체 노드를 검색할 수 있습니다.

그 후에 Cypher 질의를 통한 하위 그래프 검색을 통해 연결된 수술, 상태, 관찰 및 만남 노드를 가져와서 환자의 치료 궤적에 대한 맥락을 제공할 수 있습니다.

링크 예측은 진단과 처방된 새 약물과 같은 매핑되지 않은 관계를 제안하여 이를 더욱 풍부하게 만들 수 있습니다.

<div class="content-ad"></div>

노드 임베딩은 직접적인 링크 이상의 관련 조건들을 검색할 수 있습니다.

마지막으로, 시간 제약은 그래프를 1월 15일 이후의 관련 기간만 포함하도록 필터링할 것입니다.

FHIR 지식 그래프에서 검색된 이 상호 연결된 다면적 맥락은 LLM이 텍스트만으로는 매우 어려운 방식으로 케어 여정에 대한 다양하고 근거 있는 다음 컴포넌트 응답 추론을 생성할 수 있습니다.

앞으로의 길

<div class="content-ad"></div>

지식 그래프 및 다른 구조화된 표현물이 다양한 분야에서 더욱 널리 사용됨에 따라, 발전된 그래프 검색 능력을 개발하는 것이 차세대 RAG 시스템에 중요하게 작용할 것입니다. 벡터 검색, 표현력 있는 그래프 쿼리, 구조화된 표현물 상에서 추론, 그리고 맥락에 기반한 제약 조건 결합은 LLMs가 필요로 하는 관련 지식 서브그래프를 정확하게 검색할 수 있도록 할 것입니다.

게다가, 그래프 검색을 셀프 어텐션 주도 쿼리 형성 (이전 섹션에서 다룸)과 같은 다른 혁신적인 검색 구성 요소들과 통합하는 것이 경제성 있는 이점을 가져올 것으로 예상됩니다.

하지만, 중요한 특별한 과제들은 아직도 남아 있습니다. 확장성, 동적 그래프 다루기, 다중 모달 및 이기반 지식 출처 상에서 추론, 그리고 생성 피드백에 따라 자체 개선 및 조정할 수 있는 검색 프레임워크 개발과 같은 측면에서입니다.

하지만 분명한 것은 검색 보강 생성의 미래가 풍부한 구조화된 표현물로부터 관련 지식을 원활하게 탐색하고 추출할 수 있는 전체적인 파이프라인을 개발하는 데에 있을 것이라는 것입니다. 그래프 검색의 "어떻게"를 정복하는 것이 지식 기반 언어 생성의 전체 잠재력을 발휘하는 열쇠가 될 것입니다.

<div class="content-ad"></div>

# 유망한 방향과 앞으로의 길

대형 언어 모델 및 지식 소스의 지속적인 발전으로, RAG 시스템에서 "언제", "무엇", "어떻게"라는 차원에서 검색을 최적화하는 것은 중요한 연구 과제로 남을 것입니다:

- LLM(대형 언어 모델) 생성 상태, 주의 패턴 및 기타 내부 표현을 이용하여 추론할 수 있는 더 정교한 정보 요구 검출 방법 개발하기.
- 그래프 분석 알고리즘과 셀프 어텐션 신호를 엄격하게 결합하여 지식 그래프에서 극대화된 관련 컨텍스트 검색 기법 탐구하기.
- 추론 능력(연역법, 귀납법, 추론법 등)을 지식 그래프의 구조화된 표현과 관계를 활용하여 검색 파이프라인 내에 통합하기.
- 차세대 지식 소스의 증가된 복잡성, 다중성(텍스트, 그래프, 테이블 등) 및 동적 업데이트를 다루는 매커니즘 조사하기.
- 대규모 LLM(대형 언어 모델) 및 지식 저장소에 대해 정확도나 대기 시간을 희생하지 않고 규모 조정할 수 있는 효율적인 검색 패러다임 설계하기.

신뢰할 수 있는 AI 시스템의 미래가 바로 다음 세대 지식 소스에서 정립된 풍부하고 구조화된 지식과 대형 언어 모델의 추론 능력을 조화롭게 결합하는 RAG 프레임워크 개발에 있을 것입니다. 검색하는 방법, 시점, 그리고 방식을 최적화하는 것이 이 잠재력을 발휘하는 데 중요한 역할을 할 것입니다.

<div class="content-ad"></div>

# In Plain English 🚀

Thank you for being a part of the In Plain English community! Before you go:

- Be sure to clap and follow the writer 👏️️
- Follow us: [X](#) | [LinkedIn](#) | [YouTube](#) | [Discord](#) | [Newsletter](#)
- Visit our other platforms: [Stackademic](#) | [CoFeed](#) | [Venture](#) | [Cubed](#)
- More content at [PlainEnglish.io](#)
