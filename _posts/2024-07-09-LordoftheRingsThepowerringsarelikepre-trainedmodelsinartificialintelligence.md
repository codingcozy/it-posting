---
title: "반지의 제왕 힘의 반지가 인공지능의 사전 훈련된 모델과 같은 이유"
description: ""
coverImage: "/assets/img/2024-07-09-LordoftheRingsThepowerringsarelikepre-trainedmodelsinartificialintelligence_0.png"
date: 2024-07-09 23:33
ogImage:
  url: /assets/img/2024-07-09-LordoftheRingsThepowerringsarelikepre-trainedmodelsinartificialintelligence_0.png
tag: Tech
originalTitle: "Lord of the Rings! The power rings are like pre-trained models in artificial intelligence!"
link: "https://medium.com/ai-advances/lord-of-the-rings-the-power-rings-are-the-pre-trained-models-a3de5b3a77e7"
isUpdated: true
---

## # Part I: 텍스트의 가장 중요한 사전 훈련 모델을 검토하고 비교해보겠습니다.

![Lord of the Rings](/assets/img/2024-07-09-LordoftheRingsThepowerringsarelikepre-trainedmodelsinartificialintelligence_0.png)

# 반지의 제왕!

이 삼부작에서, 우리는 영화 "반지의 제왕"의 기억에 남는 힘의 반지와 유사한 사전 훈련 모델에 대해 이야기할 것입니다. 영화가 삼부작이었던 것처럼, 이 기사도 세 부분으로 나눠서 준비했습니다. 영화에서 힘의 반지, 엘프의 반지, 그리고 드워프의 반지가 존재했던 것을 기억한다면, 이 삼부작에서는 "반지의 제왕" 영화의 힘의 반지와 유사한 사전 훈련 모델들에 대해 이야기할 것입니다. 기사는 영화처럼 세 부분으로 나누어져 있습니다. 영화에서 힘의 반지, 엘프의 반지, 그리고 드워프의 반지가 있었던 것처럼 말이죠. 비슷하게, 사전 훈련 모델은 인공지능 모델을 구축하는 데 사용할 수 있는 힘의 반지와 같습니다. 이러한 모델들을 결합함으로써, AGI 또는 심지어 초지능을 개발할 수 있는 Multimodal 모델을 만들 수 있습니다. 그러나 영화처럼, 이 모델들이 잘못된 손에 넘어가면 심각한 위협이 될 수 있습니다. 그래서 이 기사의 제목을 "반지의 제왕"로 지었습니다. 이 흥미진진한 여정에 함께해주세요.

<div class="content-ad"></div>

이 글은 세 가지 파트로 나뉠 것입니다.

## 파트 I: 텍스트에 가장 중요한 사전 훈련 모델을 검토하고 비교하는 것으로 시작하겠습니다.

## 파트 II: 이미지 분야에서의 가장 중요한 사전 훈련 모델(AlexNet, VGG, GoogLeNet, ResNet, MobileNet 및 MobileNetV2)을 오늘까지 시작부터 검토하고 비교할 것입니다.

## 파트 III: 로봇 공학 분야의 가장 중요한 사전 훈련 모델을 검토하고 비교할 것입니다.

<div class="content-ad"></div>

이번 트릴로지에서는 세 가지 파트로 나눠 사전 훈련된 모델에 대해 탐구할 것입니다: 텍스트, 이미지, 그리고 로보틱스. 이들 개념, 발생 과정, 그리고 이 모델들이 작동하는 방식을 알아볼 것이고, 다양한 아키텍처와 종류의 사전 훈련된 모델을 살펴볼 것입니다. 그 중에서 가장 강력한 모델들은 무엇이며, 사전 훈련된 모델과 트랜스포머가 LLMs의 주요 장점인지에 대해서도 알아볼 것입니다. 게다가, 만약 사전 훈련된 모델과 트랜스포머가 존재하지 않았다면 우리가 오늘날처럼 ChatGPT와 같이 강력한 챗봇을 보유하고 있을 수 있었을지에 대해서도 논의할 것입니다. 요즘에는 챗봇이 NLP 분야에만 국한되지 않았다는 것이죠. 예를 들어, GPT-4는 텍스트 생성뿐만 아니라 여러 측면에서 다재다능하게 활용됩니다. 또한, 그들이 강화 학습과 유사하게 작동하는지도 살펴볼 것입니다. 로보틱스 분야에서는 어떤 사전 훈련된 모델이 있는지, 그 중에서 어떤 것이 멀티모달 모델에 더 나은지 알아볼 것입니다. 사전 훈련된 모델이 요즘에는 NLP에만 유용한 것인지에 대해서도 토론할 것입니다. 이미지 분야에서는 어떤 모델이 가장 강력한지, 그리고 이러한 모델을 어떻게 세부 조정하거나 하이퍼파라미터 튜닝할 수 있는지 알아볼 것입니다. 이 삼부작에서 함께 이러한 질문을 탐구하고 답을 찾아봅시다.

# 개요

사전 훈련된 모델의 가장 중요한 장점 중 하나는 이러한 모델을 세밀하게 조정하고 특정 작업에 대한 전이 학습을 적용할 수 있는 능력입니다. 전이 학습은 딥러닝의 중요한 부분이며, 그 지속적인 혁명의 기초적인 요소 중 하나입니다. 인간이나 다른 생물의 학습에도 전이 학습이 근본적인 측면입니다. 지금 이 텍스트를 읽는 동안, 저는 사전 훈련된 모델에 대한 지식을 여러분에게 전달하고 있습니다!

이 주제를 더 잘 이해하기 위해 한 가지 예시를 생각해봅시다. 큰 집을 짓고 싶다고 상상해보세요. 집을 짓는 데에는 두 가지 방법이 있습니다:

<div class="content-ad"></div>

### 처음 방법은 기초부터 시작하여 완전한 지진 저항성을 갖춘 건물을 만들 때까지 주택을 걸어가는 것입니다. 이를 위해서는 재료와 도구가 필요하며, 건축을 위해 좀 더 복잡한 공학이 필요합니다. 이는 더 높은 비용으로 이어질 수 있으며, 결과물이 완전히 만족스럽지 않을 수도 있습니다.

### 두 번째 방법은 주택 건설 분야에서 수년간의 경험과 전문 지식을 지닌 회사의 노하우를 활용하는 것입니다. 이 회사가 건설하는 주택은 사전에 제작된 모듈식 구성품을 사용합니다. 이 퍼즐 조각 같은 부품들은 맞춤 설정이 가능하며, 건물의 내부와 외부 디자인을 우리 취향에 맞게 조정할 수 있습니다. 건설 측면에서 엔지니어링이 되어 있는 방식으로 건물이 완전한 지진 저항성을 갖도록 설계되었습니다! 결과적으로 프로젝트를 디자인하고 실행하는 데 필요한 비용과 시간이 크게 줄어들었으며, 수년간의 전문 경험의 결실로 완전한 지진 저항성을 갖춘 주택을 보유할 수 있습니다!

### 어떤 방법을 선택하시겠습니까?

### 사전 훈련된 모델의 역사와 어떻게 발전해 왔는지

<div class="content-ad"></div>

프리 트레인 모델들은 인공지능 모델을 훈련하는 데 따른 중대한 어려움과 고민들로부터 등장했어요. 특히 딥러닝 분야에서는 높은 비용과 대규모 데이터를 활용하고 수집하는 어려움과 같은 문제들이 있었어요. 대규모 데이터셋은 강력하고 비싼 하드웨어가 필요했어요. 과거에는 매우 큰 데이터셋을 쉽게 구할 수 없었어요. 프리 트레인 모델은 2010년대 초반에 처음 소개되었고, 처음에는 자연어 처리를 위해 만들어졌어요. 그리고 이후에도 대규모 언어 모델의 성장에서 중요한 역할을 하고 있어요.

프리 트레인 모델의 가장 중요한 특징 중 하나는 초기 가중치와 편향값을 가지고 있다는 것이에요. 이는 특정 작업에 대해 효과적으로 세밀하게 조정할 수 있는 가능성을 제공해 줘요. 어떤 모델이든 100% 정확도를 달성할 수는 없지만, 프리 트레인 모델들은 보통 전문가들에 의해 더 정확하고 효율적인 도구와 더 강력한 로직으로 만들어져 있기 때문에 처음부터 만들어진 모델보다 더 정확하고 효과적일 수 있어요.

다양한 중요한 작업을 위해 가장 유명하고 널리 사용되는 프리 트레인 모델들은 다음과 같아요:

- 이미지 처리: VGG (2014) / ResNet (2015)
- 기계 번역: Transformer (2017) / BERT (2018)
- 음악 생성: OpenAI의 MuseNet (2019) / Magenta의 Music Transformer (2018)
- 로봇 공학: Dactyl (2018) / DeepLoco (2018)
- 감정 분석: BERT (2018) / RoBERTa (2019)
- 가짜 뉴스 탐지: BERT (2018) / GPT-3 (2020)

<div class="content-ad"></div>

# 사전 훈련 모델에 대한 컨셉이 정확히 무엇인가요?

딥 신경망 (DNNs) 개발 과정에서 처음 소개된 사전 훈련 모델의 개념은 Word2Vec 및 GloVe와 같은 알고리즘의 발전과 함께 더 많은 관심을 받게 되었습니다. 이 맥락에서 사전 훈련 모델은 기존의 모델을 새로운 모델을 훈련하는 출발점으로 사용하는 것을 의미합니다. 특히 문자용 사전 훈련 모델은 특히 유용할 수 있습니다. 단어 임베딩과 GloVe 알고리즘에 대해 간단히 살펴보고 작동 방식을 보겠습니다. 시작해볼까요?

# 단어 임베딩이란 무엇인가요?

컴퓨터는 숫자로 통신하기 때문에 문제를 전달하기 위해 숫자를 사용해야 합니다. 텍스트 데이터를 처리하려면 Word Embedding과 같은 다양한 기술을 사용하여 숫자로 변환해야 합니다. 기본적으로 Word Embedding은 데이터셋에서 단어 특징의 분포로부터 학습하는 딥 모델 또는 신경망입니다.

<div class="content-ad"></div>

Word Embedding의 개념은 한 레터에서 시작된 것이 아닙니다. 시간이 지남에 따라 몇몇 연구자들이 제안한 아이디어들의 총결정입니다. 1980년대에 Geoffrey Hinton은 Word Embedding과 "Feature Representation" 개념을 소개했습니다. 이는 단어를 밀집된 벡터로 변환하는 것을 포함합니다. Word Embedding에서 우리는 컴퓨터가 이를 처리할 수 있도록 문장 속의 단어를 벡터로 인코딩합니다.

Word Embedding은 텍스트 데이터셋 속 단어의 의미를 활용하여 그것들을 밀집된 공간에서 표현합니다. 이 공간은 밀집된 거리를 통해 단어 사이의 의미론적 관계를 나타내며, 유사하거나 관련된 단어들은 서로 가까이 있게 됩니다. 다음 섹션에서는 텍스트 처리에 Word Embedding을 사용하는 사전 훈련된 모델을 탐색해 보겠습니다.

# Word2Vec

구글 연구진인 Tomas Mikolov, Kai Chen, Greg Corrado, Jeffrey Dean을 포함한 연구자들이 2013년 논문에서 Word2Vec 모델을 발표했습니다.

<div class="content-ad"></div>

Word2vec 알고리즘은 단어의 벡터 표현을 생성하는 자연어 처리(NLP) 기술입니다. Gensim 라이브러리의 `gensim.models` 하위 모듈을 사용하여 Word2Vec 모델을 인스턴스화하려면 아래 코드 스니펫을 참고할 수 있습니다.

```python
from gensim.models import Word2Vec

# 예시 데이터
sentences = [["this", "is", "a", "sentence"], ["another", "sentence"]]

# Word2Vec 모델 생성
model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)

# 단어 벡터 표시
word_vectors = model.wv
word_vectors_dict = {}
for word in word_vectors.key_to_index:
    word_vectors_dict[word] = word_vectors[word]

print(word_vectors_dict)
```

Word2Vec 모델은 단어 임베딩을 사용하여 단어의 벡터 표현을 만듭니다. 각 단어는 텍스트 데이터를 사용하여 훈련된 Word2Vec 모델로 얻은 특정 차원의 실수 벡터에 연결됩니다. 이 매핑은 유사한 의미를 가진 단어들이 벡터 공간에서 가까이 위치하도록 하여 그들 사이의 의미적 관계를 보존합니다.

문장에서 개별 구성 요소가 함께 모여 의미를 전달합니다. 특정 단어가 개별적으로 존재하지 않더라도, 문장 내에서 설정된 관계에 기반하여 여전히 의미와 맥락을 전달할 수 있습니다. 단어 임베딩은 이 과정을 컴퓨터에게 보다 쉽게 만들기 위해 노력합니다. 예를 들어, Word2Vec 벡터 공간에서 "bird"와 "fly"와 같은 의미가 유사한 단어들이 서로 가까이 위치합니다. 두 단어의 벡터가 벡터 공간에서 가까이 있을 경우, 그들 사이에 의미적 관계가 있음을 나타냅니다.

<div class="content-ad"></div>

이 능력을 통해 언어 지식이 한 작업에서 다른 작업으로 전달될 수 있습니다. 특정 데이터셋에 대한 학습된 벡터는 다른 데이터셋에 적용될 수 있습니다. 예를 들어, 기계 번역을 위해 훈련된 벡터는 감성 분석에도 유용할 수 있습니다. 왜냐하면 단어 사이의 유사한 의미가 이러한 작업에서 보존되기 때문입니다. 따라서 Word2Vec 모델은 이 아이디어를 활용한 첫 번째 모델 중 하나였습니다.

# Word2Vec 방법론

Word2Vec 방법은 일반적으로 skip-gram 및 Continuous Bag of Words (CBOW) 모델과 같은 심층 신경망(DNN) 모델에 의존합니다. 왜냐하면 단어 순서가 중요하지 않기 때문입니다.

## CBOW 모델

<div class="content-ad"></div>

CBOW (Continuous Bag-of-Words) 모델은 자연어 처리에 사용되는 방법입니다. 문장에서 중심 또는 대상 단어를 예측하는 방식으로 작동합니다. 대상 단어는 일반적으로 문장에서 임의로 선택됩니다. 이 모델은 대상 단어 앞뒤에 오는 단어들을 분석하여 예측하려고 합니다. 예를 들어, “사랑, 그녀의 비가 내리는 눈을 푸른 하늘로 향해”라는 문장에서 대상 단어인 “사랑”을 주변 단어인 “그녀의 비가 내리는 눈을 푸른”와 같은 단어들을 분석하여 예측하는 것이 목표입니다. 다시 말해, 이 알고리즘은 대상 단어 주변의 단어들을 사용하여 예측을 시도합니다. 이 문장에서 “하늘”을 예측하기 위해 알고리즘은 “사랑”, “그녀”, “비가 내리는”, “눈을”, “푸른”, “하늘”과 같은 입력을 사용할 수 있습니다.

## Skip-gram 모델

Skip-gram 모델은 CBOW와 다르게 작동하며, 주어진 대상 단어 주변의 단어들을 예측하는 방식입니다. 예를 들어, 특정 문장에서 대상 단어가 “하늘”이라면, 모델은 “사랑”, “눈”, “비가 내리는”, “그녀/그”, 그리고 대상 단어 앞뒤에 나타날 수 있는 다른 단어들을 예측하려고 시도할 것입니다. 본질적으로, 이 모델은 각 단어에 대한 벡터를 생성하여 주변 단어들을 정확하게 예측할 수 있도록 합니다.

![image](/assets/img/2024-07-09-LordoftheRingsThepowerringsarelikepre-trainedmodelsinartificialintelligence_1.png)

<div class="content-ad"></div>

# CBOW 대 Skip-gram

CBOW 모델은 더 긴 텍스트와 다양한 텍스트를 처리할 때 가장 잘 작동합니다. 그에 반해 skip-gram 모델은 단어 간의 의미 관계 패턴을 식별하는 것이 목표인 짧은 텍스트에 더 적합합니다. 보통 skip-gram이 단어 벡터 학습에서 높은 정확도를 보유한 강력한 모델로 간주됩니다. Word2Vec은 GPT-3와 BERT를 포함한 많은 언어 모델들을 위한 기본 도구입니다. 그러나 현대의 언어 모델은 Transformer와 같은 고급 방법을 이용하여 단어 벡터를 학습합니다. 그럼에도 불구하고, Word2Vec은 여전히 강력한 도구로서 LLMs를 훈련하는 데 사용됩니다.

# GloVe 모델

Global Vectors for Word Representation의 약자인 GloVe 모델은 전역 벡터 개념에 기반한 분산 단어 표현 방법으로 개발되었습니다. 이 방법은 Word2Vec이라는 다른 모델과 유사하지만, 이 두 모델 간에는 이 벡터를 계산하는 방식이 다릅니다. Stanford 대학은 2014년에 오픈 소스 프로젝트로 GloVe를 소개했습니다. 이는 텍스트 내 단어 분포에 대한 통계적 정보를 활용하여 벡터 표현을 유도하기 위해 사용되는 비지도 학습 알고리즘입니다. 이 정보에는 다양한 텍스트에서의 단어 분포의 확률, 단어 간 의미적 관계, 텍스트에서 추출된 기타 특징 등이 포함됩니다. 이 모든 데이터는 단어 벡터를 구성하는 데 사용됩니다.

<div class="content-ad"></div>

GloVe에서는 텍스트에서 두 단어가 함께 나타날 상대 확률을 단어-단어 공기 행렬과 같은 언어 통계를 사용하여 계산합니다. 이 확률에 따라 단어 벡터가 업데이트되며 단어 사이의 의미 관계가 유지됩니다.

# Word2Vec 대 GloVe

Word2Vec와 GloVe 알고리즘은 일반적으로 동의어나 "회사-제품", "우편번호-도시"와 같은 관련성과 같은 단어 사이의 의미적 유사성을 식별하는 데 사용됩니다. 그러나 이러한 알고리즘은 동의어 (같은 철자를 가졌지만 다른 의미를 가진 단어)을 식별하는 데 효과적이지 않을 수 있습니다. 왜냐하면 동의어의 통계적 패턴은 보통 다릅니다. 예를 들어, "은행"이라는 단어는 일반적으로 돈을 보관하는 장소(금융 기관)와 연결되지만 데이터를 저장하는 장소(데이터베이스)를 가리킬 수도 있습니다. 이 두 가지 "은행" 사례는 동의어이며 같은 철자를 가지고 다른 의미를 갖습니다.

하지만 단어 간 의미적 관계를 기반으로 하는 Word2Vec 모델은 동의어를 식별하는 데 더 효과적일 수 있습니다. 그럼에도 불구하고, 동일한 철자의 단어의 다양한 의미를 이해할 수 있는 동의어를 감지하는 하이브리드 접근법이나 보다 고급 모델도 있습니다.

<div class="content-ad"></div>

또 다른 고려할 점은 단어의 대소문자 구분이며, GloVe와 Word2Vec 모델 모두 대소문자를 구분하고 각각의 경우에 다른 단어 엔티티로 취급하여 각각 다른 벡터를 만듭니다. 따라서 이 모델들을 올바르게 활용하기 위해 모든 단어의 대소문자를 표준화하는 것이 좋습니다. 그러나 이러한 표준화는 의미 정보의 손실로 이어질 수 있습니다. 예를 들어, 대문자로 쓰인 "US"는 미국을 의미하지만 소문자로 쓰인 경우 대명사 "us"를 의미합니다.

## 텍스트 전처리 기법

모든 단어를 특정 형식으로 변환하면 모델이 의미적 패턴을 학습하고 올바르게 작동할 수 있습니다. 그러나 GloVe나 Word2Vec과 같은 모델에서 대소문자 구분은 문제가 될 수 있습니다. 이 문제에 대처하기 위해 몇 가지 텍스트 전처리 방법이 사용됩니다. 그 중 하나는 어간 추출인데, 단어의 접두사와 접미사를 제거하여 단어의 뿌리 형태에 도달하고 비슷한 단어들을 표준화시킵니다. 예를 들어, "running," "runs," "ran," 그리고 "runner"라는 단어들은 모두 뿌리 혹은 기본 형태인 "run"에서 유래합니다. Porter 알고리즘은 여러 접미사를 제거하고 단어를 의미 있는 뿌리 형태로 변환하기 위한 규칙 세트를 활용하는 잘 알려진 어간 추출 기술입니다. 예를 들어, "cats"는 "cat"으로, "running"은 "run"으로 변환됩니다.

어간 추출 기술을 적용하려면 NLTK를 설치하고 필요한 데이터를 다운로드해야 합니다. 그럼 아래의 Python 코드를 사용할 수 있습니다:

<div class="content-ad"></div>

```python
import nltk
from nltk.stem import PorterStemmer

# stemming에 필요한 데이터 다운로드
nltk.download('punkt')

# PorterStemmer 클래스의 인스턴스 생성
stemmer = PorterStemmer()

# 어간화할 단어들
words = ["running", "runs", "ran", "runner", "cats", "cat"]

# 단어 어간화 후 결과 출력
for word in words:
    stemmed_word = stemmer.stem(word)
    print(f"{word} -> {stemmed_word}")
```

위 코드는 "running", "runs", "ran", "runner", "cats", 그리고 "cat"을 그들 각각의 어원으로 변환하고 결과를 출력합니다.

```python
running -> run
runs -> run
ran -> ran
runner -> runner
cats -> cat
cat -> cat
[nltk_data] Downloading package punkt to /root/nltk_data...
[nltk_data]   Package punkt is already up-to-date!
```

자연어 처리에서 사용되는 두 번째 기술은 Lemmatization입니다. 이 방법은 단어를 해당 기본 형태나 어간 형태로 변환합니다. Lemmatization을 수행하기 위해, stemming 메서드와 유사하게 NLTK 라이브러리를 사용합니다. 먼저 `nltk.download('wordnet')` 명령을 사용하여 필요한 데이터를 다운로드해야 합니다. 그런 다음, Lemmatization 작업을 수행하는 데 사용되는 `WordNetLemmatizer` 클래스의 인스턴스를 만듭니다. 단어를 Lemmatization하기 위해 `words` 라는 리스트에 단어를 넣습니다. `for` 루프를 사용하여 `words` 리스트에서 각 단어를 가져와 `WordNetLemmatizer` 클래스의 `lemmatize` 메서드를 사용하여 Lemmatization합니다.

<div class="content-ad"></div>

위의 코드를 실행한 후, 각 단어에 대한 원본 단어와 함께 표제어 추출 결과가 문장으로 출력됩니다. 예를 들어, "running"이라는 단어가 입력으로 주어진다면, 해당 작업의 출력은 "running"이 될 것입니다. 왜냐하면 "running"이라는 단어는 "run"이라는 뿌리에서 파생된 동사로 간주되기 때문입니다.

[nltk_data] 패키지 'wordnet'을 /root/nltk_data에 다운로드 중...
running -> running
cats -> cat
ate -> ate
flying -> flying
better -> better

위의 방법들은 모든 단어의 변형을 표준화하여 대소문자에 덜 민감하게 만듭니다. 이러한 기술을 활용하여 모든 단어가 표준 형식으로 변환되므로 대소문자가 텍스트 분석 결과에 영향을 미치지 않습니다. 이 두 가지 방법은 단어를 표준화하기 위해 서로 다른 방식을 취하며, 문제의 성격과 목표에 따라 양쪽 기술을 동시에 사용할 수 있습니다. 그러나 이는 텍스트 처리의 복잡성이 증가하고 효율이 감소할 수 있음을 유의해야 합니다.

<div class="content-ad"></div>

GloVe 및 Word2Vec과 같은 알고리즘은 단어 임베딩 모델로, BERT 및 GPT와 같은 모델의 단어를 표현하는 전처리 단계 중 하나로 사용됩니다. 하지만 이러한 모델의 주요 부분은 아닙니다. BERT 및 GPT와 같은 모델은 사전 훈련된 언어 모델로, 기계 번역, 텍스트 생성, 질의응답 및 자연어 처리 작업에서 Transformer 네트워크를 사용하여 좋은 성능을 발휘합니다. 앞으로 이어질 내용에서 이들을 자세히 살펴보겠습니다. 계속해서 참고하세요.

# 강력함을 발휘하는 LLM의 비밀은 어디에?

GloVe와 Word2Vec은 자연어 처리를 위한 최초의 모델이었습니다. 이후 최근에는 BERT, GPT-3, RoBERTa와 같은 사전 훈련된 모델이 소개되었는데, 이들은 어텐션을 기반으로 한 변환 기법을 사용합니다. 이러한 모델들은 더 효율적이고 인기가 많습니다. 2017년에 NLP를 혁신시킨 논문이 발표되었습니다. 그 논문인 "Attention is All You Need"가 NLP에 미칠 영향을 예상하지 못했을 것입니다.

어텐션 메커니즘은 강력한 Language Model Machines (LLM)의 중요한 구성 요소로 간주됩니다. 특히, 딥 러닝과 자연어 처리에서 어텐션 개념은 순환 신경망(RNN)을 개선하고 보다 긴 시퀀스나 문장을 더 잘 관리하는 데 사용됩니다. BERT와 GPT-3와 같은 사전 훈련된 모델은 이러한 기본적인 개념 위에 구축되었으며 상당한 발전을 거쳤습니다. BERT는 어텐션 아이디어를 활용하고, GPT-3는 Self-Attention 모델을 사용합니다. 게다가 GPT-3 모델은 원래 Self-Attention 모델을 위해 고안된 Transformer 아키텍처를 활용합니다.

<div class="content-ad"></div>

# Self-Attention Mechanism

셀프 어텐션 메커니즘은 RNN과는 다르게 입력 데이터를 순차적으로 처리하는 것이 아닙니다. (즉, 문장에서 단어와 같은 각 입력은 특정 가중치를 가진 네트워크 노드로 순차적으로 전달되며, 단어 간의 관계는 시간적(또는 순차적)으로 고려되며, 시간에 걸쳐 의존성이 순차적으로 조사됩니다). CNN에서는 치환 필터를 사용하여 입력 데이터의 다른 기능을 독립적으로 추출하고 시간 의존성을 고려하지 않고 처리를 하게 됩니다. 즉, 입력 데이터의 다양한 부분을 처리하고 데이터의 위치와 관련된 정보를 고려합니다.

셀프 어텐션 메커니즘은 문장에서 단어 간의 장기 의존성과 관계를 효과적으로 포착할 수 있습니다. 셀프 어텐션 모델의 핵심 요소는 각 단어가 순차열에서 모든 다른 단어에 동시에 주목하고 특정 작업에 중요한 단어에 따라 가중치가 부여된 표현을 만들 수 있는 셀프 어텐션 메커니즘입니다. 다시 말해, 문장의 각 단어는 다른 모든 단어를 고려하여 자신의 역할과 중요성을 결정합니다. 이 접근 방식을 통해 모델은 특정 단어 순서에 의존하지 않고 문장의 전체 길이에 걸쳐 장기적으로 단어 간의 관계를 고려하게 됩니다. 요약하면, 셀프 어텐션 모델은 복잡한 텍스트 데이터의 패턴을 포착하기 위한 강력한 도구를 제공함으로써 NLP를 크게 변화시켰습니다.

# Transformer Mechanism

<div class="content-ad"></div>

트랜스포머 모델은 텍스트의 구조를 이해하고 정확한 예측을 내리기 위해 주의 메커니즘을 사용합니다. 과거에는 한 언어에서 다른 언어로 문장을 단어별 번역하는 방법이 효과적이지 않았습니다. 주의 메커니즘이 소개되면서 각 시간 단계마다 시퀀스의 모든 요소에서 중요한 정보를 수집하고 그들 사이의 관계를 인식할 수 있게 되었습니다. 그러나 이는 모델이 한꺼번에 모든 문장을 읽는다는 뜻은 아닙니다.

트랜스포머 아키텍처에서 모델은 각 단어마다 문장의 다른 모든 단어에 주의를 기울여 정보를 수집하고 이를 기반으로 다양한 예측을 수행합니다. GPT-3는 1750억 개 이상의 파라미터를 가진 대규모 트랜스포머 아키텍처를 사용합니다. 이 큰 파라미터 수는 모델이 스스로 학습하고 방대한 양의 데이터에서 학습할 수 있게 해주며, 더 복잡한 패턴을 인식할 수 있게 합니다. 따라서 GPT-3는 문장의 다양한 요소에 주의를 기울여 복잡한 텍스트를 생성하고 의미 분석을 수행합니다.

이러한 모델들은 공간 주의와 계층적 주의를 활용합니다. 공간 주의에서는 모델이 출력을 생성하기 위해 입력이나 출력의 특정 부분에만 주의를 기울입니다. 계층적 주의에서는 모델이 입력이나 출력의 서로 다른 수준에서 주의를 조절하여 단어나 문장과 같은 텍스트 요소에 서로 다른 기회와 중요성을 할당합니다. 이 두 유형의 주의를 사용하여 모델은 텍스트에서 중요한 정보를 식별하고 최상의 및 가장 포괄적인 출력을 생성할 수 있습니다.

# 사전 훈련된 모델로 LLM(언어 모델) 구축하기

<div class="content-ad"></div>

BERT과 GPT는 자연어 처리를 위한 두 가지 매우 성공적인 사전 훈련 모델입니다. 이러한 모델은 LLM(언어모델) 구축을 포함한 여러 작업에 활용할 수 있습니다.

이 모델들은 대량의 텍스트 데이터를 기반으로 훈련될 수 있으며, 특정 자연어 처리 작업을 위한 특정 기능을 갖춘 사용자 정의 LLM을 생성할 수 있습니다.

일반적으로 우리는 해당 모델의 아키텍처 및 가중치와 같은 원본 모델에 액세스할 수 있지만, 특정 작업에 효과적으로 활용하기 위해 자체 데이터로 이들을 세밀하게 조정할 수 있습니다. 이러한 프로세스는 특정 작업에 대한 모델의 성능을 향상시키며, LLM을 구축하는 비용 효율적인 방법입니다.

예를 들어, 자연어 처리를 위한 사전 훈련 모델로 GPT 모델을 사용하려면 다음 코드를 실행하여 해당 모델을 얻을 수 있습니다. Hugging Face transformers 라이브러리를 사용하면 DistilGPT-2, BERT, RoBERTa, DistilBERT, XLNet과 같은 대부분의 사전 훈련된 모델을 동일한 코드를 사용하여 자연어 처리 작업에 활용할 수 있습니다.

아래는 Hugging Face 웹사이트에서 발췌한 두 개의 코드 조각으로, DistilGPT-2 모델을 다양한 방식으로 텍스트 생성에 사용하는 방법을 보여줍니다.

<div class="content-ad"></div>

# 파이프라인을 사용하면 Hub에서 작업과 모델 ID만 지정하면 됩니다.

from transformers import pipeline
pipe = pipeline("text-generation", model="distilgpt2")

첫 번째 방법은 transformers 라이브러리의 `pipeline` 클래스를 사용하는 것입니다. 이 클래스를 사용하면 미리 학습된 모델을 텍스트 분류, 텍스트 번역, 텍스트 생성 등과 같은 다양한 작업에 대해 정의하거나 구조화하지 않아도 쉽게 사용할 수 있습니다. 이 방법을 사용하려면 작업 유형을 "text-generation"으로 지정하고 사용할 모델을 "distilgpt2"로 지정하면 됩니다. 반면 두 번째 방법은 아래 코드로 설명되어 있습니다.

# 더 많은 제어를 원하는 경우 토크나이저 및 모델을 정의해야 합니다.

from transformers import AutoTokenizer, AutoModelForCausalLM
tokenizer = AutoTokenizer.from_pretrained("distilgpt2")
model = AutoModelForCausalLM.from_pretrained("distilgpt2")

두 번째 방법은 `AutoTokenizer` 클래스를 사용하여 토크나이저를 로드한 다음 `from_pretrained` 메서드를 통해 DistilGPT-2 모델과 연결된 토크나이저를 로드하는 것을 포함합니다. 모델을 로드하기 위해 `AutoModelForCausalLM` 클래스를 사용합니다. 이 방법은 모델과 토크나이저에 대해 더 많은 제어를 필요로 하는 사용자들에게 권장됩니다.

<div class="content-ad"></div>

한번 이 코드들을 실행하면, 사전 학습된 모델을 사용할 수 있게 됩니다. 그러면 모델에 텍스트를 입력하고 다음 코드를 사용하여 텍스트 특성의 표현을 분석할 수 있습니다.

```python
import torch
# 입력 텍스트를 정의합니다
input_text = "사랑, 파란 하늘을 향한 그녀의 눈 속의 비."

# 입력 텍스트를 토큰화합니다
input_ids = tokenizer.encode(input_text, return_tensors="pt")

# 모델의 출력 logits을 받아옵니다
with torch.no_grad():
    outputs = model(input_ids)
    logits = outputs.logits

# 출력 logits의 형태를 출력합니다
print("출력 logits의 형태:", logits.shape)
```

위에서 언급된 코드 조각을 실행한 후, 결과 출력은 다음과 같습니다:

```python
출력 logits의 형태: torch.Size([1, 12, 50257])
```

<div class="content-ad"></div>

DistilGPT-2 모델은 (batch_size, sequence_length, vocab_size) 형식의 출력 logits를 생성합니다. 예를 들어, [1, 12, 50257]는 생성된 출력물이 길이가 12이고 어휘 크기가 50257인 것을 나타냅니다.

PyTorch와 TensorFlow는 사전 훈련된 모델을 구축하는 데 사용되는 두 가지 인기있는 라이브러리입니다. 두 라이브러리 모두 동일한 아키텍처를 갖고 있지만 모델이 어떻게 정의되고 활용되는 점에서 차이가 있습니다. TensorFlow 모델은 계산 그래프로 정의되며, 모델을 훈련하고 사용하는 데 특정 함수가 있습니다. 일반적으로 `Keras`나 `tf.keras`와 같은 고수준 API가 모델을 구축하고 훈련하는 데 사용됩니다. 반면, PyTorch 모델은 모듈로 정의되며 Python 함수를 사용하여 직접 관리할 수 있습니다. 게다가 PyTorch는 모델을 구축하고 훈련하는 데 주요 API입니다.

# 미세 조정

이 글을 통해 텍스트와 관련된 다양한 유형의 사전 훈련된 모델을 탐색하고 그 아키텍처를 살펴보았습니다. 이러한 모델을 우리 자신의 데이터셋에서 사용하고 수행하려는 특정 작업에 대해 미세 조정해야 합니다. 미세 조정은 트랜스퍼 러닝을 사용하는 깊은 모델을 훈련하는 데 중요한 개념입니다.

<div class="content-ad"></div>

조정 작업은 대규모 데이터셋으로 훈련된 사전 훈련 모델을 적용하여 주어진 작업에 맞게 조정하는 과정을 의미합니다. 일반적으로 모델의 상위 레이어를 동결하고 하위 레이어의 가중치만을 최적화할 수 있습니다. 이 프로세스를 통해 모델은 작은 데이터셋에서 특정 패턴을 학습하면서 대규모 사전 훈련 데이터셋에서 학습한 일반적인 패턴을 이용할 수 있습니다.

사전 훈련된 모델에는 다양한 아키텍처와 구성이 있습니다. 먼저 적합한 사전 훈련 모델을 선택하고 위에서 언급한 모든 단계를 수행해야 합니다. 이는 새로운 데이터에 맞게 매개변수를 조정하면서 훈련 중에 학습된 지식을 보존하는 것을 포함합니다. 적절한 아키텍처를 선택하고 과제별 목적 함수를 정의하며 데이터셋을 준비하고, 사전 훈련된 가중치로 모델 초기화하고 이를 반복적인 최적화를 통해 매개변수를 세밀조정하는 것입니다. 이 과정에서 전이 학습, 하이퍼파라미터 튜닝 및 PEFT(정밀 효율적 세밀조정)과 같은 기술이 중요합니다. 정규화 방법과 하이퍼파라미터 튜닝은 성능 최적화에 도움이 됩니다.

위 단계를 완료한 후 성능 메트릭을 선택하여 세밀조정된 모델을 평가합니다. 정확도, 재현율 또는 해당 문제에 적합한 기타 메트릭과 같은 평가 메트릭을 사용합니다. 준비된 데이터셋에서 모델을 세밀조정하는 과정은 일반적으로 epoch-by-epoch 또는 batch-by-batch 방식을 사용하여 목적 함수(예: 손실 함수)의 값을 계산하고 모델 매개변수를 업데이트합니다. 그런 다음 평가 데이터에서 결과를 평가하고 평가 메트릭을 계산하여 최종 모델 성능을 평가합니다.

# 결론

<div class="content-ad"></div>

이 기사의 첫 부분에서는 주요 사전 훈련 텍스트 모델들, 그들의 소개 및 유용성에 대해 논의했습니다. 우리는 또한 대형 언어 모델 (LLM)이 이제 우리의 일상생활의 중요한 부분이 되고 지속적으로 발전하고 있다는 것을 보았습니다. 이러한 모델들은 가까운 미래에 우리의 개인 비서가 되고, 검색 엔진의 중요성이 천천히 줄어들 것이라는 것도 알았습니다. 또한 이러한 모델들을 세밀하게 조정하는 것이 그 성능을 최적화하는 데 중요하다는 것을 배웠습니다.

제2부에서는 이미지를 위한 필수 사전 훈련 모델을 탐험하고 그들을 처음부터 현재까지 비교할 것입니다. 이 분석은 우리에게 이러한 모델들의 중요성을 강조하기 위한 것입니다. 제2부는 곧 발행될 예정이며, 여러분이 읽고 피드백을 공유해 주시면 감사하겠습니다.

이 여정에 함께해 주셔서 감사합니다.

딥러닝의 세계로의 이 여정에 함께해 주셔서 감사합니다.

<div class="content-ad"></div>

**카드 전문가로서**, virgool.io에 이 블로그 게시물 링크가 있어요.

# Part I: 텍스트를 위한 가장 중요한 미리 학습된 모델을 비교하고 검토할 거예요.

# Part II: 오늘 날까지 이미지 분야에서 가장 중요한 미리 학습된 모델 (AlexNet, VGG, GoogLeNet, ResNet, MobileNet 및 MobileNetV2)을 시작부터 비교하고 검토할 거예요.

# Part III: 로봇공학을 위한 가장 중요한 미리 학습된 모델을 비교하고 검토할 거예요.

<div class="content-ad"></div>

만약 관심이 있다면, 제 다른 글 "MNIST와 CNN 아키텍처로 인한 과적합 관찰을 통한 인공 신경망 구축의 단계별 가이드" 를 Medium에서 확인해보세요.

이란의 인공지능 미래: 철저한 리뷰

콘텐츠가 마음에 들었기를 바랍니다.
