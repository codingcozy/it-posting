---
title: "날씨 예보로 설명하는 대형 언어 모델LLM 작동 원리"
description: ""
coverImage: "/assets/img/2024-07-13-ExplainingHowLLMsWorkThroughWeatherForecastingAnalogies_0.png"
date: 2024-07-13 03:43
ogImage:
  url: /assets/img/2024-07-13-ExplainingHowLLMsWorkThroughWeatherForecastingAnalogies_0.png
tag: Tech
originalTitle: "Explaining How LLMs Work Through Weather Forecasting Analogies"
link: "https://medium.com/ai-mind-labs/explaining-how-llms-work-through-weather-forecasting-analogies-b8f49e32b357"
isUpdated: true
---

![Image](/assets/img/2024-07-13-ExplainingHowLLMsWorkThroughWeatherForecastingAnalogies_0.png)

# 주의력이 중요합니다

“Attention is All you Need”은 인공 지능 분야에서 패러다임 전환을 이끈 연구 논문입니다. 이 논문은 세계에 트랜스포머 아키텍처를 소개한 것으로, 이 아키텍처는 챗-GPT와 같은 응용 프로그램을 구동하는 여러 LLM의 기반이 됩니다.

## AI 엔지니어를 향한 꿈꾸는 이들에게

<div class="content-ad"></div>

AI 엔지니어, 연구원 또는 취미로 공부하는 사람들은 LLMs와 기술적 관점에서 소통하는 데 필수적인 트랜스포머에 대해 알고 있어야 합니다.

이미 트랜스포머 아키텍처가 어떻게 작동하는지에 대한 다양한 설명이 많이 있지만, 이들은 매우 추상적일 수 있습니다. 이를 이해하기 어려운 엔지니어들도 있습니다. 제가 제공하는 트랜스포머 아키텍처에 대한 좀 더 구체적인 설명은 모델에 대한 직관력을 키울 수 있도록 돕기 위한 것입니다.

원래 "Attention is All you Need" 논문에서 제안된 인코더-디코더 트랜스포머 모델을 설명할 것입니다. 이를 숙지하면 다른 더 독특한 아키텍처를 이해하는 데 도움이 될 것입니다.

이 기사를 마치면, 아래 항목들을 알게 될 것입니다:

<div class="content-ad"></div>

- 변형기(Transformer)가 어떻게 작용하는지에 대해 더 확신 있게 표현하는 방법을 배웁니다.
- Llama 및 Mistral 모델에서 보여주는 파생 LLM(Large Language Model) 아키텍처 몇 가지를 이해합니다.
- 변형기 아키텍처에서 인코더(Encoder)와 디코더(Decoder)의 목적을 이해합니다.
- 주목 메커니즘(Attention mechanisms)이 변형기가 데이터에서 복잡한 패턴을 식별하도록 하는 방법을 파악합니다.
- 디코더가 자기 회귀적 방식으로 예측을 생성하는 방법을 인지합니다.
- 변형기가 RNN과 같은 선배 모델에 비해 더 효율적이고 확장 가능한 이유를 이해합니다.

자, 함께 알아봅시다.

# 변형기(Transformer)란?

먼저 추상적인 설명으로 시작해보겠습니다.

<div class="content-ad"></div>

트랜스포머는 자가 주의 메커니즘으로 구분되는 딥 뉴럴 네트워크 유형입니다. 자연어 처리(때로는 비전)에서 뛰어나며 다음과 같은 다양한 작업에 능통합니다:

- 텍스트 생성 및 대화
- 기계 번역
- 감성 분석
- 혐오 발언 탐지
- 명명된 엔티티 인식
- 비구조적 데이터 구축
- 텍스트 요약
- 이미지 인식

여러분은 보시다시피, 목록이 꽤 방대하며 응용 분야가 다양합니다. 오늘날의 대형 언어 모델(LLM) 중 많은 모델이 트랜스포머 아키텍처 위에 구축되어 있어 인공지능 언어 이해의 발전에 꼭 필요합니다.

이제 트랜스포머 아키텍처에 대해 좀 더 구체적으로 알아봅시다.

<div class="content-ad"></div>

# Transformer City에 오신 것을 환영합니다

우리는 Transformer City라는 이름의 도시를 상상할 수 있습니다. 이 도시 안에서, Transformer Broadcasting Corp (TBC)는 날씨 예보를 전달하는 책임이 있습니다. 이들은 두 개의 탑에서 날씨 데이터를 관측, 기록 및 처리함으로써 이를 수행합니다. 첫 번째인 인코더 타워는 Transformer City의 서쪽에 위치합니다. 두 번째인 디코더 타워는 도시 중심에 위치해 있습니다.

이 세상의 날씨 패턴은 서쪽에서 동쪽으로 움직이기 때문에 서쪽의 날씨가 어느 시점에는 동쪽에 영향을 미친다는 점입니다. 이 사실은 Transformer City의 날씨가 도시 서쪽의 날씨를 관찰하여 예측 가능하다는 것을 의미합니다. 아래의 GIF는 이를 설명하고 있습니다.

배경이 명확해지면, 이제 인코더와 디코더 타워가 어떻게 협력하여 날씨 예보를 제공하는지 살펴봅시다.

<div class="content-ad"></div>

# 인코더 타워

인코더 타워의 주요 역할은 도시 서쪽의 날씨 패턴을 디코더 타워와 공유할 수 있는 보고서로 인코딩하는 것입니다. 이 보고서는 디코더 타워가 날씨 예보를 생성하는 데 사용됩니다.

## 타워 층은 트랜스포머의 층으로서 쌓여있습니다

탑 층을 제외하고, 타워의 각 층은 트랜스포머의 인코더 블록의 층을 나타냅니다.

<div class="content-ad"></div>

## 탑의 맨 꼭대기에서 입력 데이터를 받았어요

맨 꼭대기 층은 날씨 데이터를 직접 관측하고 측정하여 기록하는 곳이에요. 이것은 트랜스포머 네트워크로의 입력 데이터와 유사해요.

## 포지셔널 인코딩으로 360도 관측

인코더 탑의 맨 꼭대기는 360도 관측이 가능해 날씨 데이터를 모든 방향에서 캡처할 수 있어요. 이는 문장에서 단어의 상대적 위치나 순서열에서 항목들의 상대적 위치를 잡아내는 트랜스포머 네트워크의 포지셔널 인코딩과 일치해요.

<div class="content-ad"></div>

## 날씨 과학자들이 멀티헤드 어텐션을 수행합니다

탑 안에는 날씨 데이터의 패턴을 식별하고 탑 내 다른 과학자들과 공유할 보고서를 작성하는 일을 하는 날씨 과학자들이 있습니다. 과학자들은 팀으로 작동하며, 각 층에는 여러 팀이 있습니다.

## 어텐션은 쿼리, 키 및 값으로 설명됩니다

어텐션은 수학적 연산입니다. 수학적 용어로는 우리는 단순히 쿼리와 키 값 쌍을 출력으로 매핑하는 것입니다. 구체적으로 말하면, 이는 수석 과학자가 그들의 팀에게 데이터에서 특정 날씨 패턴을 조사하도록 지시하는 것 (쿼리)인 것입니다. 과학자들은 관련 데이터 포인트를 찾습니다 (키), 해석합니다 (값), 그리고 데이터에 대해 추가 분석을 수행하고 보고서를 작성합니다 (출력). 이 모든 것의 목적은 미래를 예상하기 위해 어떤 날씨 패턴이 관련될 수 있는지 식별하는 것입니다. 문장을 조작하는 트랜스포머의 맥락에서, 어텐션 메커니즘은 예측 작업에 관련될 수 있는 단어를 식별하며, 일반적으로 문장에서 다음 단어를 예측합니다.

<div class="content-ad"></div>

## 다층적으로 운영되는 시스템

최상층에서는 날씨 데이터가 직접 관측, 측정되고 기록됩니다. 이 데이터는 아래층으로 공유되어 과학자 팀들에게 분배됩니다. 이 팀들은 이전에 설명한 관심 작업을 수행하고 결과 보고서를 다음 아래층으로 공유합니다. 이 과정은 탑의 최하층에 이르기까지 반복됩니다.

층간 처리는 병렬로 이루어지며 여러 팀의 연구원이 동시에 동일한 보고서에 주의 작업을 진행할 수 있습니다.

그러나 층간 처리는 순차적으로 이루어집니다. 아래층의 과학자들은 그들 앞층의 과학자들이 보고서를 처리하고 공유한 후에 그들의 처리를 시작할 수 있어야 합니다.

<div class="content-ad"></div>

## 복잡한 패턴 발견을 위한 자기 주의

타워의 한 층에서 나오는 보고서들이 다음 층의 보고서로 이어진다는 것을 이해하는 것이 중요합니다. 이로 인해 두 가지 특성이 나타납니다. 첫 번째는 타워의 각 과학자들이 다른 모든 과학자의 작업을 인식하는 것인데, 이를 자기 주의라고 합니다. 두 번째로는 타워를 내려갈수록 보고서가 더 복잡해진다는 것입니다.

각 층에서 생산된 보고서를 읽을 수 있다고 상상해보세요. 맨 위 층에서는 보고서를 해석하기가 상당히 쉬울 것입니다. 아마도 온도, 압력 등과 같은 날씨 차원의 측정값들일 것입니다. 그 다음 층에서는 보고서가 더 복잡해질 수 있으며, 측정된 값들이 모델에 입력된 추가적인 모델링이 될 수 있습니다. 바닥 층에 도착하면, 보고서는 지쳤던 많은 과학자팀들의 손을 거쳐 매우 알아보기 힘든 형태가 될 것입니다.

이것이 바로 트랜스포머 네트워크에서 일어나는 일입니다. 층을 깊이 들어갈수록 표현이 더 복잡하고 추상적해집니다! 이 특성 때문에 트랜스포머 및 인코더 타워는 데이터의 복잡한 관계를 포착할 수 있게 되지만, 우리가 그들이 무엇을 하는지 이해하지 못하는 것 역시 의미합니다.

<div class="content-ad"></div>

## 디코더 타워와 지식을 공유하기

최종 보고서가 작성되면, 디코더 타워와 공유됩니다. 이제 인코더 타워의 과학자들이 수행한 모든 작업에 대해 디코더 타워의 과학자들이 인식하게 됩니다.

# 디코더 타워

디코더 타워의 주요 역할은 날씨 예보를 작성하는 것입니다. 이를 위해 인코더 타워의 보고서와 이전 예보를 활용합니다.

<div class="content-ad"></div>

## 층을 쌓아가는 과정

디코더 타워는 인코더 타워와 마찬가지로, 각 층은 트랜스포머 모델에서의 레이어를 나타냅니다. 두 타워 간에는 비슷한 연산이 많지만, 디코더 타워가 동작하는 방식에 대해 강조해야 할 중요한 차이점이 있습니다.

## 멀티헤드 어텐션 (새로운 접근)

각 층에서 디코더 타워는 한 가지가 아닌 두 개의 어텐션 연산을 수행합니다. 이는 디코더 타워가 자신의 예측과 인코더 타워에서 공유한 복잡한 보고서를 모두 고려하여 예측을 생성하기 때문입니다.

<div class="content-ad"></div>

**엔코더 타워와 달리 디코더 타워는 자신만의 기상 데이터를 수집하는 대신, 엔코더 타워로부터의 복잡한 보고서에 의존하여 예보를 생성합니다. 그 보고서는 디코더 타워의 과학자들에게 도시 서쪽에서의 날씨 영향에 대한 더 넓은 시각을 제공해줍니다. 이를 효과적으로 중앙 지역 날씨를 예보하기 위해 고려해야 할 것입니다. 더불어 디코더 타워는 서쪽 지역을 위한 예보를 동부 지역을 위한 예보로 이어지게 생성합니다. 이 과정은 서쪽부터 동쪽까지 체계적으로 실행되며, 도시의 서단 가장자리로부터의 위치 이동을 고려한 타워의 예보가 수행됩니다.**

**AutoRegression의 구체적인 사례로 Chat-GPT가 있습니다. 이 모델은 한 단어씩 생성하면서 자신에게 다시 입력하여 다음 단어를 예측합니다.**

**## 디코더 타워의 셀프-어텐션**

**디코더 타워는 셀프-어텐션 속성을 갖고 있습니다. 주요 차이점은 이 타워의 모든 과학자가 도시 서쪽의 날씨 패턴 뿐만 아니라 이전에 생성된 날씨 예보에 대해 인식하고 있다는 것입니다. 본질적으로 여기서의 셀프-어텐션은 각 과학자에게 날씨 패턴에 대한 전역과 지역적 관점을 부여해주며, 이후 예보에 영향을 줄 수 있습니다. 이 프로세스는 두 개의 별도 어텐션 작업을 통해 구현됩니다: 첫 번째는 예보를 포착하며, 두 번째는 초기 어텐션 레이어의 출력과 엔코더로부터의 보고서를 흡수합니다.**

<div class="content-ad"></div>

## 예측 생성하기

디코더 타워의 첫 번째 층에서 시작된 주의 메커니즘은 나머지 층을 통해 전파되어 마지막 예측이 아래에서 생성됩니다.

# 예측을 위한 과학자 교육

트랜스포머 방송사(TBC)의 기상 예보 시스템은 처음에 정확도에서 일부 어려움을 겪었습니다. 패턴 인식과 정밀도를 강화하기 위해 TBC 경영진은 역사적 날씨 데이터를 활용하여 모든 과학자들을 위한 체계적인 교육 프로그램을 시행했습니다.

<div class="content-ad"></div>

TBC 과학자들은 미래 사건을 미리 알지 못한 채 날씨를 예측하는 도전에 직면하고 있습니다. 이 현실은 그들의 훈련 체제에서 반영되었습니다. 여기서 과학자들은 역사적인 날씨 기록인 방대한 양의 과거 기상 사건에 접근하여 기술을 개발했습니다.

예보자들에게는 특정 지점까지의 역사 데이터 조각만 제공되었으며, 그 이후의 날씨 상황에 대한 통찰은 제공되지 않았습니다. 이 방법을 통해 그들은 과거 날씨 패턴에 대한 깊은 이해를 확보하면서도, 이러한 조각들 이후의 즉각적인 날씨 사건에 대해 알지 못하도록 유지되었습니다. 이러한 접근 방식은 미래가 보이지 않는 실시간 예보의 조건을 모방하기 위한 것이었습니다.

이 문맥에서 '미래 사건'은 훈련 중 공개되지 않은 역사 데이터 부분을 가리킵니다. 이 정보를 숨기는 것은 미래 지식 없이 예측하는 실제 세계 시나리오를 모방하는 데 중요했습니다. 따라서, 예보자들은 알려진 과거와 현재 데이터를 기반으로 예측을 수립하는 데 훈련을 받았으며, transformer 모델의 가리기 기술과 일치하였습니다 — 이 기술은 일련의 예측이 뒤 이어지는 부분에 대한 접근이 없이 이뤄지는 것입니다.

훈련 중 이루어진 예측은 예약된 데이터에서 실제 결과와 비교되었습니다. 관측된 불일치 사항은 엄격하게 분석되었으며, 결과들은 인코더와 디코더 타워 전체에 공유되었습니다. 이 반복적인 피드백 루프는 예보자들의 방법론을 정제하는 데 중요했으며, 점차적으로 예보 오차를 줄이는 데 기여했습니다. 한 번 예보자들이 예측에서 최소한의 오차를 보이면, TBC는 Transformer City 주민들에게 신뢰할 수 있는 날씨 예보를 제공하는 것에 대비하였습니다.

이 프로세스는 대규모 텍스트 데이터에 대한 transformer의 사전 훈련 단계와 유사하며, 디코더가 미리 예측해야 할 텍스트를 아직 보지 않도록 가리기를 활용하여 확보합니다. 이는 미래를 알고 있는 상태에서 날씨를 예측하는 예보자들에 상당한 도전으로, 현실을 무시하는 위업입니다.

<div class="content-ad"></div>

# 결론

이 비유를 통해 트랜스포머 아키텍처의 복잡한 작동 방식을 명확히 해 드린 것 같습니다. 그 안에 내재된 복잡성을 실감하실 수 있을 것입니다. 가상의 기상 대형기에서 데이터가 겪는 철저한 처리를 생각해 보세요. 각 전문가팀에 의해 해부되고 재조합되는, 각자의 세밀한 이해를 기여하는 과정들. 이 다차원적인 접근, 비록 복잡하더라도, 최종적으로 일원된 예보를 낳아냅니다. 마찬가지로, 트랜스포머 아키텍처와 그 주의 메커니즘의 층들은 자연어 처리를 혁신적으로 발전시켜 왔습니다. 이는 지속적으로 우리의 상상력을 사로잡는 생성 모델들에 대한 기초를 다졌습니다.

원래의 인코더-디코더 아키텍처에서, 우리는 외연 모델인 가장 대규모의 언어 모델 (LLM)로의 진화를 목격했습니다. 이 모델들은 방대한 양의 텍스트로 훈련되어, 원래의 인코더-디코더 모델들의 성과를 크게 능가하는 인상적인 결과를 도출했습니다.

만약 인공지능 기술을 향상시키고 싶다면, 제 코스의 대기 리스트에 가입해 보세요.

<div class="content-ad"></div>

AI-전환을 희망하는 비즈니스를 위한 자문 상담을 신청하세요.

인공지능, 데이터 과학 및 대규모 언어 모델에 대한 더 많은 통찰력을 얻고 싶다면 YouTube 채널을 구독해보세요.

## AI 마인드로부터의 메시지

![AI Mind](https://miro.medium.com/v2/resize:fit:500/0*5Wm7sOfTpe5DEbhg.gif)

<div class="content-ad"></div>

우리 커뮤니티에 참여해 주셔서 감사합니다! 떠나시기 전에:

- 👏 이야기에 박수를 보내고 작가를 팔로우해 주세요 👉
- 📰 AI Mind 출판물에서 더 많은 콘텐츠를 확인해 보세요
- 🧠 AI 프롬프트를 손쉽게 무료로 향상시켜 보세요
- 🧰 직관적인 AI 도구들을 발견해 보세요
